{"version":3,"file":"endpoints.js","sourceRoot":"","sources":["../../../../../libs/client/src/types/endpoints.ts"],"names":[],"mappings":"","sourcesContent":["export type Audio = {\n  /**\n   * Type of media (always 'audio') Default value: `\"audio\"`\n   */\n  media_type?: \"audio\";\n  /**\n   * URL where the media file can be accessed\n   */\n  url: string;\n  /**\n   * MIME type of the media file\n   */\n  content_type: string;\n  /**\n   * Original filename of the media\n   */\n  file_name: string;\n  /**\n   * Size of the file in bytes\n   */\n  file_size: number;\n  /**\n   * Duration of the media in seconds\n   */\n  duration: number;\n  /**\n   * Overall bitrate of the media in bits per second\n   */\n  bitrate: number;\n  /**\n   * Codec used to encode the media\n   */\n  codec: string;\n  /**\n   * Container format of the media file (e.g., 'mp4', 'mov')\n   */\n  container: string;\n  /**\n   * Number of audio channels\n   */\n  channels: number;\n  /**\n   * Audio sample rate in Hz\n   */\n  sample_rate: number;\n};\nexport type AudioFile = {\n  /**\n   * The URL where the file can be downloaded from.\n   */\n  url: string;\n  /**\n   * The mime type of the file.\n   */\n  content_type?: string;\n  /**\n   * The name of the file. It will be auto-generated if not provided.\n   */\n  file_name?: string;\n  /**\n   * The size of the file in bytes.\n   */\n  file_size?: number;\n  /**\n   * File data\n   */\n  file_data?: string;\n  /**\n   * The duration of the audio file in seconds.\n   */\n  duration: number;\n};\nexport type AudioTrack = {\n  /**\n   * Audio codec used (e.g., 'aac', 'mp3')\n   */\n  codec: string;\n  /**\n   * Number of audio channels\n   */\n  channels: number;\n  /**\n   * Audio sample rate in Hz\n   */\n  sample_rate: number;\n  /**\n   * Audio bitrate in bits per second\n   */\n  bitrate: number;\n};\nexport type BoundingBox = {\n  /**\n   * X-coordinate of the top-left corner\n   */\n  x: number;\n  /**\n   * Y-coordinate of the top-left corner\n   */\n  y: number;\n  /**\n   * Width of the bounding box\n   */\n  w: number;\n  /**\n   * Height of the bounding box\n   */\n  h: number;\n  /**\n   * Label of the bounding box\n   */\n  label: string;\n};\nexport type BoundingBoxes = {\n  /**\n   * List of bounding boxes\n   */\n  bboxes: Array<BoundingBox>;\n};\nexport type BoxPrompt = {\n  /**\n   * X Min Coordinate of the box\n   */\n  x_min?: number;\n  /**\n   * Y Min Coordinate of the box\n   */\n  y_min?: number;\n  /**\n   * X Max Coordinate of the prompt\n   */\n  x_max?: number;\n  /**\n   * Y Max Coordinate of the prompt\n   */\n  y_max?: number;\n  /**\n   * The frame index to interact with.\n   */\n  frame_index?: number;\n};\nexport type CameraControl = {\n  /**\n   * The type of camera movement\n   */\n  movement_type: \"horizontal\" | \"vertical\" | \"pan\" | \"tilt\" | \"roll\" | \"zoom\";\n  /**\n   * The value of the camera movement\n   */\n  movement_value: number;\n};\nexport type Color = {\n  /**\n   * Red value Default value: `128`\n   */\n  r?: number;\n  /**\n   * Green value Default value: `128`\n   */\n  g?: number;\n  /**\n   * Blue value Default value: `128`\n   */\n  b?: number;\n};\nexport type ControlLoraWeight = {\n  /**\n   * URL or the path to the LoRA weights.\n   */\n  path: string;\n  /**\n   * The scale of the LoRA weight. This is used to scale the LoRA weight\n   * before merging it with the base model. Providing a dictionary as {\"layer_name\":layer_scale} allows per-layer lora scale settings. Layers with no scale provided will have scale 1.0. Default value: `1`\n   */\n  scale?: any | number;\n  /**\n   * URL of the image to be used as the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * Type of preprocessing to apply to the input image. Default value: `\"None\"`\n   */\n  preprocess?: \"canny\" | \"depth\" | \"None\";\n};\nexport type ControlNet = {\n  /**\n   * URL or the path to the control net weights.\n   */\n  path: string;\n  /**\n   * optional URL to the controlnet config.json file.\n   */\n  config_url?: string | Blob | File;\n  /**\n   * The optional variant if a Hugging Face repo key is used.\n   */\n  variant?: string;\n  /**\n   * URL of the image to be used as the control net.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask to use for the controlnet. When using a mask, the control image size and the mask size must be the same and divisible by 32.\n   */\n  mask_url?: string | Blob | File;\n  /**\n   * The scale of the control net weight. This is used to scale the control net weight\n   * before merging it with the base model. Default value: `1`\n   */\n  conditioning_scale?: number;\n  /**\n   * The percentage of the image to start applying the controlnet in terms of the total timesteps.\n   */\n  start_percentage?: number;\n  /**\n   * The percentage of the image to end applying the controlnet in terms of the total timesteps. Default value: `1`\n   */\n  end_percentage?: number;\n  /**\n   * The index of the IP adapter to be applied to the controlnet. This is only needed for InstantID ControlNets.\n   */\n  ip_adapter_index?: number;\n};\nexport type ControlNetUnion = {\n  /**\n   * URL or the path to the control net weights.\n   */\n  path: string;\n  /**\n   * optional URL to the controlnet config.json file.\n   */\n  config_url?: string | Blob | File;\n  /**\n   * The optional variant if a Hugging Face repo key is used.\n   */\n  variant?: string;\n  /**\n   * The control images and modes to use for the control net.\n   */\n  controls: Array<ControlNetUnionInput>;\n};\nexport type DiarizationSegment = {\n  /**\n   * Start and end timestamp of the segment\n   */\n  timestamp: Array<void>;\n  /**\n   * Speaker ID of the segment\n   */\n  speaker: string;\n};\nexport type DynamicMask = {\n  /**\n   * URL of the image for Dynamic Brush Application Area (Mask image created by users using the motion brush)\n   */\n  mask_url: string | Blob | File;\n  /**\n   * List of trajectories\n   */\n  trajectories?: Array<Trajectory>;\n};\nexport type Embedding = {\n  /**\n   * URL or the path to the embedding weights.\n   */\n  path: string;\n  /**\n   * The list of tokens to use for the embedding. Default value: `<s0>,<s1>`\n   */\n  tokens?: Array<string>;\n};\nexport type FaceDetection = {\n  /**\n   * Bounding box of the face.\n   */\n  bbox: Array<number>;\n  /**\n   * Keypoints of the face.\n   */\n  kps?: Array<Array<number>>;\n  /**\n   * Keypoints of the face on the image.\n   */\n  kps_image: Image;\n  /**\n   * Confidence score of the detection.\n   */\n  det_score: number;\n  /**\n   * Embedding of the face.\n   */\n  embedding_file: File;\n  /**\n   * Either M or F if available.\n   */\n  sex?: string;\n};\nexport type File = {\n  /**\n   * The URL where the file can be downloaded from.\n   */\n  url: string;\n  /**\n   * The mime type of the file.\n   */\n  content_type?: string;\n  /**\n   * The name of the file. It will be auto-generated if not provided.\n   */\n  file_name?: string;\n  /**\n   * The size of the file in bytes.\n   */\n  file_size?: number;\n  /**\n   * File data\n   */\n  file_data?: string;\n};\nexport type Frame = {\n  /**\n   * URL of the frame\n   */\n  url: string;\n};\nexport type Image = {\n  /**\n   * The URL where the file can be downloaded from.\n   */\n  url: string;\n  /**\n   * The mime type of the file.\n   */\n  content_type?: string;\n  /**\n   * The name of the file. It will be auto-generated if not provided.\n   */\n  file_name?: string;\n  /**\n   * The size of the file in bytes.\n   */\n  file_size?: number;\n  /**\n   * File data\n   */\n  file_data?: string;\n  /**\n   * The width of the image in pixels.\n   */\n  width?: number;\n  /**\n   * The height of the image in pixels.\n   */\n  height?: number;\n};\nexport type ImagePrompt = {\n  /**\n   *  Default value: `\"ImagePrompt\"`\n   */\n  type?: \"ImagePrompt\" | \"PyraCanny\" | \"CPDS\" | \"FaceSwap\";\n  /**\n   *\n   */\n  image_url?: string | Blob | File;\n  /**\n   *  Default value: `0.5`\n   */\n  stop_at?: number;\n  /**\n   *  Default value: `1`\n   */\n  weight?: number;\n};\nexport type ImageSize = {\n  /**\n   * The width of the generated image. Default value: `512`\n   */\n  width?: number;\n  /**\n   * The height of the generated image. Default value: `512`\n   */\n  height?: number;\n};\nexport type IPAdapter = {\n  /**\n   * URL of the image to be used as the IP adapter.\n   */\n  ip_adapter_image_url: string | Blob | File | Array<string | Blob | File>;\n  /**\n   * The mask to use for the IP adapter. When using a mask, the ip-adapter image size and the mask size must be the same\n   */\n  ip_adapter_mask_url?: string | Blob | File;\n  /**\n   * URL or the path to the IP adapter weights.\n   */\n  path: string;\n  /**\n   * Subfolder in the model directory where the IP adapter weights are stored.\n   */\n  model_subfolder?: string;\n  /**\n   * Name of the weight file.\n   */\n  weight_name?: string;\n  /**\n   * URL or the path to the InsightFace model weights.\n   */\n  insight_face_model_path?: string;\n  /**\n   * The scale of the IP adapter weight. This is used to scale the IP adapter weight\n   * before merging it with the base model. Default value: `1`\n   */\n  scale?: number;\n  /**\n   * The scale of the IP adapter weight. This is used to scale the IP adapter weight\n   * before merging it with the base model.\n   */\n  scale_json?: Record<string, any>;\n  /**\n   * The factor to apply to the unconditional noising of the IP adapter.\n   */\n  unconditional_noising_factor?: number;\n  /**\n   * The value to set the image projection shortcut to. For FaceID plus V1 models,\n   * this should be set to False. For FaceID plus V2 models, this should be set to True.\n   * Default is True. Default value: `true`\n   */\n  image_projection_shortcut?: boolean;\n};\nexport type Keyframe = {\n  /**\n   * The timestamp in milliseconds where this keyframe starts\n   */\n  timestamp: number;\n  /**\n   * The duration in milliseconds of this keyframe\n   */\n  duration: number;\n  /**\n   * The URL where this keyframe's media file can be accessed\n   */\n  url: string;\n};\nexport type LoraWeight = {\n  /**\n   * URL or the path to the LoRA weights. Or HF model name.\n   */\n  path: string;\n  /**\n   * The scale of the LoRA weight. This is used to scale the LoRA weight\n   * before merging it with the base model. Default value: `1`\n   */\n  scale?: number;\n  /**\n   * If set to true, the embedding will be forced to be used.\n   */\n  force?: boolean;\n};\nexport type MoondreamInputParam = {\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Prompt to be used for the image Default value: `\"Describe this image.\"`\n   */\n  prompt?: string;\n};\nexport type OCRBoundingBox = {\n  /**\n   * List of quadrilateral boxes\n   */\n  quad_boxes: Array<OCRBoundingBoxSingle>;\n};\nexport type OCRBoundingBoxSingle = {\n  /**\n   * X-coordinate of the top-left corner\n   */\n  x: number;\n  /**\n   * Y-coordinate of the top-left corner\n   */\n  y: number;\n  /**\n   * Width of the bounding box\n   */\n  w: number;\n  /**\n   * Height of the bounding box\n   */\n  h: number;\n  /**\n   * Label of the bounding box\n   */\n  label: string;\n};\nexport type PikaImage = {\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n};\nexport type PointPrompt = {\n  /**\n   * X Coordinate of the prompt Default value: `305`\n   */\n  x?: number;\n  /**\n   * Y Coordinate of the prompt Default value: `350`\n   */\n  y?: number;\n  /**\n   * Label of the prompt. 1 for foreground, 0 for background Default value: `\"1\"`\n   */\n  label?: \"0\" | \"1\";\n  /**\n   * The frame index to interact with.\n   */\n  frame_index?: number;\n};\nexport type Polygon = {\n  /**\n   * List of points\n   */\n  points: Array<any>;\n  /**\n   * Label of the polygon\n   */\n  label: string;\n};\nexport type ReferenceFace = {\n  /**\n   * URL of the reference face image\n   */\n  image_url: string | Blob | File;\n};\nexport type ReferenceImage = {\n  /**\n   * URL to the reference image file (PNG format recommended)\n   */\n  image_url: string | Blob | File;\n};\nexport type Region = {\n  /**\n   * X-coordinate of the top-left corner\n   */\n  x1: number;\n  /**\n   * Y-coordinate of the top-left corner\n   */\n  y1: number;\n  /**\n   * X-coordinate of the bottom-right corner\n   */\n  x2: number;\n  /**\n   * Y-coordinate of the bottom-right corner\n   */\n  y2: number;\n};\nexport type Resolution = {\n  /**\n   * Display aspect ratio (e.g., '16:9')\n   */\n  aspect_ratio: string;\n  /**\n   * Width of the video in pixels\n   */\n  width: number;\n  /**\n   * Height of the video in pixels\n   */\n  height: number;\n};\nexport type RGBColor = {\n  /**\n   * Red color value\n   */\n  r?: number;\n  /**\n   * Green color value\n   */\n  g?: number;\n  /**\n   * Blue color value\n   */\n  b?: number;\n};\nexport type Speaker = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  speaker_id: number;\n  /**\n   *\n   */\n  audio_url: string | Blob | File;\n};\nexport type Track = {\n  /**\n   * Unique identifier for the track\n   */\n  id: string;\n  /**\n   * Type of track ('video' or 'audio')\n   */\n  type: string;\n  /**\n   * List of keyframes that make up this track\n   */\n  keyframes: Array<Keyframe>;\n};\nexport type Trajectory = {\n  /**\n   * X coordinate of the motion trajectory\n   */\n  x: number;\n  /**\n   * Y coordinate of the motion trajectory\n   */\n  y: number;\n};\nexport type TranscriptionWord = {\n  /**\n   * The transcribed word or audio event\n   */\n  text: string;\n  /**\n   * Start time in seconds\n   */\n  start: number;\n  /**\n   * End time in seconds\n   */\n  end: number;\n  /**\n   * Type of element (word, spacing, or audio_event)\n   */\n  type: string;\n  /**\n   * Speaker identifier if diarization was enabled\n   */\n  speaker_id?: string;\n};\nexport type Turn = {\n  /**\n   *\n   */\n  speaker_id: number;\n  /**\n   *\n   */\n  text: string;\n};\nexport type Video = {\n  /**\n   * Type of media (always 'video') Default value: `\"video\"`\n   */\n  media_type?: \"video\";\n  /**\n   * URL where the media file can be accessed\n   */\n  url: string;\n  /**\n   * MIME type of the media file\n   */\n  content_type: string;\n  /**\n   * Original filename of the media\n   */\n  file_name: string;\n  /**\n   * Size of the file in bytes\n   */\n  file_size: number;\n  /**\n   * Duration of the media in seconds\n   */\n  duration: number;\n  /**\n   * Overall bitrate of the media in bits per second\n   */\n  bitrate: number;\n  /**\n   * Codec used to encode the media\n   */\n  codec: string;\n  /**\n   * Container format of the media file (e.g., 'mp4', 'mov')\n   */\n  container: string;\n  /**\n   * Frames per second\n   */\n  fps: number;\n  /**\n   * Total number of frames in the video\n   */\n  frame_count: number;\n  /**\n   * Time base used for frame timestamps\n   */\n  timebase: string;\n  /**\n   * Video resolution information\n   */\n  resolution: Resolution;\n  /**\n   * Detailed video format information\n   */\n  format: VideoFormat;\n  /**\n   * Audio track information if video has audio\n   */\n  audio?: AudioTrack;\n  /**\n   * URL of the extracted first frame\n   */\n  start_frame_url?: string | Blob | File;\n  /**\n   * URL of the extracted last frame\n   */\n  end_frame_url?: string | Blob | File;\n};\nexport type VideoFormat = {\n  /**\n   * Container format of the video\n   */\n  container: string;\n  /**\n   * Video codec used (e.g., 'h264')\n   */\n  video_codec: string;\n  /**\n   * Codec profile (e.g., 'main', 'high')\n   */\n  profile: string;\n  /**\n   * Codec level (e.g., 4.1)\n   */\n  level: number;\n  /**\n   * Pixel format used (e.g., 'yuv420p')\n   */\n  pixel_format: string;\n  /**\n   * Video bitrate in bits per second\n   */\n  bitrate: number;\n};\nexport type WhisperChunk = {\n  /**\n   * Start and end timestamp of the chunk\n   */\n  timestamp: Array<void>;\n  /**\n   * Transcription of the chunk\n   */\n  text: string;\n};\nexport type AdvancedFaceSwapInput = {\n  /**\n   * User's face image to face swap FROM\n   */\n  face_image_0: Image;\n  /**\n   * The gender of the person in the face image.\n   */\n  gender_0: \"\" | \"male\" | \"female\" | \"non-binary\";\n  /**\n   * (Optional) The Second face image to face swap FROM\n   */\n  face_image_1?: Image;\n  /**\n   * The gender of the person in the second face image.\n   */\n  gender_1?: \"\" | \"male\" | \"female\" | \"non-binary\";\n  /**\n   * The target image to face swap TO\n   */\n  target_image: Image;\n  /**\n   * The type of face swap workflow. target_hair = preserve target's hair. user_hair = preserve user's hair.\n   */\n  workflow_type: \"user_hair\" | \"target_hair\";\n  /**\n   * Apply 2x upscale and boost quality. Upscaling will refine the image and make the subjects brighter. Default value: `true`\n   */\n  upscale?: boolean;\n};\nexport type AdvancedFaceSwapOutput = {\n  /**\n   * The mirrored image.\n   */\n  image: Image;\n};\nexport type AmEngOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type AMTFrameInterpolationInput = {\n  /**\n   * Frames to interpolate\n   */\n  frames: Array<Frame>;\n  /**\n   * Output frames per second Default value: `24`\n   */\n  output_fps?: number;\n  /**\n   * Number of recursive interpolation passes Default value: `4`\n   */\n  recursive_interpolation_passes?: number;\n};\nexport type AmtInterpolationFrameInterpolationInput = {\n  /**\n   * Frames to interpolate\n   */\n  frames: Array<Frame>;\n  /**\n   * Output frames per second Default value: `24`\n   */\n  output_fps?: number;\n  /**\n   * Number of recursive interpolation passes Default value: `4`\n   */\n  recursive_interpolation_passes?: number;\n};\nexport type AmtInterpolationFrameInterpolationOutput = {\n  /**\n   * Generated video\n   */\n  video: File;\n};\nexport type AmtInterpolationInput = {\n  /**\n   * URL of the video to be processed\n   */\n  video_url: string | Blob | File;\n  /**\n   * Output frames per second Default value: `24`\n   */\n  output_fps?: number;\n  /**\n   * Number of recursive interpolation passes Default value: `2`\n   */\n  recursive_interpolation_passes?: number;\n};\nexport type AMTInterpolationInput = {\n  /**\n   * URL of the video to be processed\n   */\n  video_url: string | Blob | File;\n  /**\n   * Output frames per second Default value: `24`\n   */\n  output_fps?: number;\n  /**\n   * Number of recursive interpolation passes Default value: `2`\n   */\n  recursive_interpolation_passes?: number;\n};\nexport type AmtInterpolationOutput = {\n  /**\n   * Generated video\n   */\n  video: File;\n};\nexport type AnimatediffSparsectrlLcmInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to specify what you don't want. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The type of controlnet to use for generating the video. The controlnet determines how the video will be animated. Default value: `\"scribble\"`\n   */\n  controlnet_type?: \"scribble\" | \"rgb\";\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable\n   * Diffusion will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The URL of the first keyframe to use for the generation.\n   */\n  keyframe_0_image_url?: string | Blob | File;\n  /**\n   * The frame index of the first keyframe to use for the generation.\n   */\n  keyframe_0_index?: number;\n  /**\n   * The URL of the second keyframe to use for the generation.\n   */\n  keyframe_1_image_url?: string | Blob | File;\n  /**\n   * The frame index of the second keyframe to use for the generation.\n   */\n  keyframe_1_index?: number;\n  /**\n   * The URL of the third keyframe to use for the generation.\n   */\n  keyframe_2_image_url?: string | Blob | File;\n  /**\n   * The frame index of the third keyframe to use for the generation.\n   */\n  keyframe_2_index?: number;\n};\nexport type AnimatediffSparsectrlLcmOutput = {\n  /**\n   * Generated video file.\n   */\n  video: File;\n  /**\n   * The seed used to generate the video.\n   */\n  seed: number;\n};\nexport type AnimateDiffT2VInput = {\n  /**\n   * The prompt to use for generating the video. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"(bad quality, worst quality:1.2), ugly faces, bad anime\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of frames to generate for the video. Default value: `16`\n   */\n  num_frames?: number;\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of frames per second to extract from the video. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * The motions to apply to the video.\n   */\n  motions?: Array<\n    \"zoom-out\" | \"zoom-in\" | \"pan-left\" | \"pan-right\" | \"tilt-up\" | \"tilt-down\"\n  >;\n  /**\n   * The size of the video to generate. Default value: `square`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type AnimateDiffT2VOutput = {\n  /**\n   * Generated video file.\n   */\n  video: File;\n  /**\n   * Seed used for generating the video.\n   */\n  seed: number;\n};\nexport type AnimateDiffT2VTurboInput = {\n  /**\n   * The prompt to use for generating the video. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"(bad quality, worst quality:1.2), ugly faces, bad anime\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of frames to generate for the video. Default value: `16`\n   */\n  num_frames?: number;\n  /**\n   * The number of inference steps to perform. 4-12 is recommended for turbo mode. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of frames per second to extract from the video. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * The motions to apply to the video.\n   */\n  motions?: Array<\n    \"zoom-out\" | \"zoom-in\" | \"pan-left\" | \"pan-right\" | \"tilt-up\" | \"tilt-down\"\n  >;\n  /**\n   * The size of the video to generate. Default value: `square`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type AnimatediffV2vInput = {\n  /**\n   * URL of the video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"(bad quality, worst quality:1.2), ugly faces, bad anime\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * Base model to use for animation generation. Default value: `\"cardosAnimev20\"`\n   */\n  base_model?: \"darkSushiMixMix_colorful\" | \"cardosAnimev20\";\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * Select every Nth frame from the video.\n   * This can be used to reduce the number of frames to process, which can reduce the time and the cost.\n   * However, it can also reduce the quality of the final video. Default value: `2`\n   */\n  select_every_nth_frame?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n};\nexport type AnimateDiffV2VInput = {\n  /**\n   * URL of the video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The first N number of seconds of video to animate. Default value: `3`\n   */\n  first_n_seconds?: number;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"(bad quality, worst quality:1.2), ugly faces, bad anime\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The strength of the input video in the final output. Default value: `0.7`\n   */\n  strength?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of frames per second to extract from the video. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * The motions to apply to the video.\n   */\n  motions?: Array<\n    \"zoom-out\" | \"zoom-in\" | \"pan-left\" | \"pan-right\" | \"tilt-up\" | \"tilt-down\"\n  >;\n};\nexport type AnimatediffV2vOutput = {\n  /**\n   * Generated video file.\n   */\n  video: File;\n  /**\n   * Seed used for generating the video.\n   */\n  seed: number;\n  /**\n   *\n   */\n  timings: any;\n};\nexport type AnimateDiffV2VOutput = {\n  /**\n   * Generated video file.\n   */\n  video: File;\n  /**\n   * Seed used for generating the video.\n   */\n  seed: number;\n};\nexport type AnimatediffV2vTurboInput = {\n  /**\n   * URL of the video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"(bad quality, worst quality:1.2), ugly faces, bad anime\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `8`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.2`\n   */\n  guidance_scale?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Select every Nth frame from the video.\n   * This can be used to reduce the number of frames to process, which can reduce the time and the cost.\n   * However, it can also reduce the quality of the final video. Default value: `2`\n   */\n  select_every_nth_frame?: number;\n};\nexport type AnimateDiffV2VTurboInput = {\n  /**\n   * URL of the video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The first N number of seconds of video to animate. Default value: `3`\n   */\n  first_n_seconds?: number;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"(bad quality, worst quality:1.2), ugly faces, bad anime\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. 4-12 is recommended for turbo mode. Default value: `8`\n   */\n  num_inference_steps?: number;\n  /**\n   * The strength of the input video in the final output. Default value: `0.7`\n   */\n  strength?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of frames per second to extract from the video. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * The motions to apply to the video.\n   */\n  motions?: Array<\n    \"zoom-out\" | \"zoom-in\" | \"pan-left\" | \"pan-right\" | \"tilt-up\" | \"tilt-down\"\n  >;\n};\nexport type AnimatediffV2vTurboOutput = {\n  /**\n   * Generated video file.\n   */\n  video: File;\n  /**\n   * Seed used for generating the video.\n   */\n  seed: number;\n  /**\n   *\n   */\n  timings: any;\n};\nexport type AnimateDiffV2VTurboOutput = {\n  /**\n   * Generated video file.\n   */\n  video: File;\n  /**\n   * Seed used for generating the video.\n   */\n  seed: number;\n  /**\n   *\n   */\n  timings: any;\n};\nexport type AnyLlmInput = {\n  /**\n   * Name of the model to use. Premium models are charged at 10x the rate of standard models, they include: meta-llama/llama-3.2-90b-vision-instruct, openai/gpt-4o, anthropic/claude-3-5-haiku, google/gemini-pro-1.5, anthropic/claude-3.5-sonnet. Default value: `\"google/gemini-flash-1.5\"`\n   */\n  model?:\n    | \"anthropic/claude-3.5-sonnet\"\n    | \"anthropic/claude-3-5-haiku\"\n    | \"anthropic/claude-3-haiku\"\n    | \"google/gemini-pro-1.5\"\n    | \"google/gemini-flash-1.5\"\n    | \"google/gemini-flash-1.5-8b\"\n    | \"meta-llama/llama-3.2-1b-instruct\"\n    | \"meta-llama/llama-3.2-3b-instruct\"\n    | \"meta-llama/llama-3.1-8b-instruct\"\n    | \"meta-llama/llama-3.1-70b-instruct\"\n    | \"openai/gpt-4o-mini\"\n    | \"openai/gpt-4o\"\n    | \"deepseek/deepseek-r1\";\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * System prompt to provide context or instructions to the model\n   */\n  system_prompt?: string;\n  /**\n   * Should reasoning be the part of the final answer.\n   */\n  reasoning?: boolean;\n};\nexport type AnyLlmOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Generated reasoning for the final answer\n   */\n  reasoning?: string;\n  /**\n   * Whether the output is partial\n   */\n  partial?: boolean;\n  /**\n   * Error message if an error occurred\n   */\n  error?: string;\n};\nexport type AnyLlmVisionInput = {\n  /**\n   * Name of the model to use. Premium models are charged at 3x the rate of standard models, they include: meta-llama/llama-3.2-90b-vision-instruct, openai/gpt-4o, anthropic/claude-3-5-haiku, google/gemini-pro-1.5, anthropic/claude-3.5-sonnet. Default value: `\"google/gemini-flash-1.5\"`\n   */\n  model?:\n    | \"anthropic/claude-3.5-sonnet\"\n    | \"anthropic/claude-3-haiku\"\n    | \"google/gemini-pro-1.5\"\n    | \"google/gemini-flash-1.5\"\n    | \"google/gemini-flash-1.5-8b\"\n    | \"openai/gpt-4o\"\n    | \"meta-llama/llama-3.2-90b-vision-instruct\";\n  /**\n   * Prompt to be used for the image\n   */\n  prompt: string;\n  /**\n   * System prompt to provide context or instructions to the model\n   */\n  system_prompt?: string;\n  /**\n   * Should reasoning be the part of the final answer.\n   */\n  reasoning?: boolean;\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n};\nexport type AnyLlmVisionOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Generated reasoning for the final answer\n   */\n  reasoning?: string;\n  /**\n   * Whether the output is partial\n   */\n  partial?: boolean;\n  /**\n   * Error message if an error occurred\n   */\n  error?: string;\n};\nexport type AudioInput = {\n  /**\n   * The URL of the audio file.\n   */\n  audio_url: string | Blob | File;\n};\nexport type AudioOutput = {\n  /**\n   * The generated audio.\n   */\n  audio: File;\n};\nexport type AuraFlowInput = {\n  /**\n   * The prompt to generate images from\n   */\n  prompt: string;\n  /**\n   * The number of images to generate Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The seed to use for generating images\n   */\n  seed?: number;\n  /**\n   * Classifier free guidance scale Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to take Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to perform prompt expansion (recommended) Default value: `true`\n   */\n  expand_prompt?: boolean;\n};\nexport type AuraFlowOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * The seed used to generate the images\n   */\n  seed: number;\n  /**\n   * The expanded prompt\n   */\n  prompt: string;\n};\nexport type AuraSrInput = {\n  /**\n   * URL of the image to upscale.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Upscaling factor. More coming soon. Default value: `\"4\"`\n   */\n  upscaling_factor?: \"4\";\n  /**\n   * Whether to use overlapping tiles for upscaling. Setting this to true helps remove seams but doubles the inference time.\n   */\n  overlapping_tiles?: boolean;\n  /**\n   * Checkpoint to use for upscaling. More coming soon. Default value: `\"v1\"`\n   */\n  checkpoint?: \"v1\" | \"v2\";\n};\nexport type AuraSrOutput = {\n  /**\n   * Upscaled image\n   */\n  image: Image;\n  /**\n   * Timings for each step in the pipeline.\n   */\n  timings: any;\n};\nexport type AutoCaptionInput = {\n  /**\n   * URL to the .mp4 video with audio. Only videos of size <100MB are allowed.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Colour of the text. Can be a RGB tuple, a color name, or an hexadecimal notation. Default value: `\"white\"`\n   */\n  txt_color?: string;\n  /**\n   * Font for generated captions. Choose one in 'Arial','Standard','Garamond', 'Times New Roman','Georgia', or pass a url to a .ttf file Default value: `\"Standard\"`\n   */\n  txt_font?: string;\n  /**\n   * Size of text in generated captions. Default value: `24`\n   */\n  font_size?: number;\n  /**\n   * Width of the text strokes in pixels Default value: `1`\n   */\n  stroke_width?: number;\n  /**\n   * Left-to-right alignment of the text. Can be a string ('left', 'center', 'right') or a float (0.0-1.0) Default value: `center`\n   */\n  left_align?: string | number;\n  /**\n   * Top-to-bottom alignment of the text. Can be a string ('top', 'center', 'bottom') or a float (0.0-1.0) Default value: `center`\n   */\n  top_align?: string | number;\n  /**\n   * Number of seconds the captions should stay on screen. A higher number will also result in more text being displayed at once. Default value: `1.5`\n   */\n  refresh_interval?: number;\n};\nexport type AutoCaptionOutput = {\n  /**\n   * URL to the caption .mp4 video.\n   */\n  video_url: string | Blob | File;\n};\nexport type BaseInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated video. Default value: `[object Object]`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to generate video from Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRAs to use for the image generation. We currently support one lora. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * Use RIFE for video interpolation Default value: `true`\n   */\n  use_rife?: boolean;\n  /**\n   * The target FPS of the video Default value: `16`\n   */\n  export_fps?: number;\n};\nexport type BatchMoonDreamOutput = {\n  /**\n   * URL to the generated captions JSON file containing filename-caption pairs.\n   */\n  captions_file: File;\n  /**\n   * List of generated captions\n   */\n  outputs: Array<string>;\n};\nexport type BatchQueryInput = {\n  /**\n   * List of image URLs to be processed (maximum 32 images)\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Single prompt to apply to all images\n   */\n  prompt: string;\n  /**\n   * Maximum number of tokens to generate Default value: `64`\n   */\n  max_tokens?: number;\n};\nexport type BenV2ImageInput = {\n  /**\n   * URL of image to be used for background removal\n   */\n  image_url: string | Blob | File;\n  /**\n   * Random seed for reproducible generation.\n   */\n  seed?: number;\n};\nexport type BenV2ImageOutput = {\n  /**\n   * The output image after background removal.\n   */\n  image: Image;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type BenV2VideoInput = {\n  /**\n   * URL of video to be used for background removal.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Random seed for reproducible generation.\n   */\n  seed?: number;\n};\nexport type BenV2VideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type BGRemoveInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BGRemoveOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n};\nexport type BGReplaceInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the reference image to be used for generating the new background. Use \"\" to leave empty. Either ref_image_url or bg_prompt has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp. Default value: `\"\"`\n   */\n  ref_image_url?: string | Blob | File;\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt?: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Whether to refine prompt Default value: `true`\n   */\n  refine_prompt?: boolean;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Whether to use the fast model Default value: `true`\n   */\n  fast?: boolean;\n  /**\n   * Number of Images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BGReplaceOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BirefnetInput = {\n  /**\n   * URL of the image to remove background from\n   */\n  image_url: string | Blob | File;\n  /**\n   * Model to use for background removal.\n   * The 'General Use (Light)' model is the original model used in the BiRefNet repository.\n   * The 'General Use (Heavy)' model is a slower but more accurate model.\n   * The 'Portrait' model is a model trained specifically for portrait images.\n   * The 'General Use (Light)' model is recommended for most use cases.\n   *\n   * The corresponding models are as follows:\n   * - 'General Use (Light)': BiRefNet-DIS_ep580.pth\n   * - 'General Use (Heavy)': BiRefNet-massive-epoch_240.pth\n   * - 'Portrait': BiRefNet-portrait-TR_P3M_10k-epoch_120.pth Default value: `\"General Use (Light)\"`\n   */\n  model?: \"General Use (Light)\" | \"General Use (Heavy)\" | \"Portrait\";\n  /**\n   * The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. Default value: `\"1024x1024\"`\n   */\n  operating_resolution?: \"1024x1024\" | \"2048x2048\";\n  /**\n   * The format of the output image Default value: `\"png\"`\n   */\n  output_format?: \"webp\" | \"png\";\n  /**\n   * Whether to output the mask used to remove the background\n   */\n  output_mask?: boolean;\n  /**\n   * Whether to refine the foreground using the estimated mask Default value: `true`\n   */\n  refine_foreground?: boolean;\n};\nexport type BirefnetOutput = {\n  /**\n   * Image with background removed\n   */\n  image: Image;\n  /**\n   * Mask used to remove the background\n   */\n  mask_image?: Image;\n};\nexport type BirefnetV2Input = {\n  /**\n   * URL of the image to remove background from\n   */\n  image_url: string | Blob | File;\n  /**\n   * Model to use for background removal.\n   * The 'General Use (Light)' model is the original model used in the BiRefNet repository.\n   * The 'General Use (Light)' model is the original model used in the BiRefNet repository but trained with 2K images.\n   * The 'General Use (Heavy)' model is a slower but more accurate model.\n   * The 'Matting' model is a model trained specifically for matting images.\n   * The 'Portrait' model is a model trained specifically for portrait images.\n   * The 'General Use (Light)' model is recommended for most use cases.\n   *\n   * The corresponding models are as follows:\n   * - 'General Use (Light)': BiRefNet-DIS_ep580.pth\n   * - 'General Use (Heavy)': BiRefNet-massive-epoch_240.pth\n   * - 'Portrait': BiRefNet-portrait-TR_P3M_10k-epoch_120.pth Default value: `\"General Use (Light)\"`\n   */\n  model?:\n    | \"General Use (Light)\"\n    | \"General Use (Light 2K)\"\n    | \"General Use (Heavy)\"\n    | \"Matting\"\n    | \"Portrait\"\n    | \"High Resolutions\";\n  /**\n   * The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. Default value: `\"1024x1024\"`\n   */\n  operating_resolution?: \"1024x1024\" | \"2048x2048\";\n  /**\n   * The format of the output image Default value: `\"png\"`\n   */\n  output_format?: \"webp\" | \"png\";\n  /**\n   * Whether to output the mask used to remove the background\n   */\n  output_mask?: boolean;\n  /**\n   * Whether to refine the foreground using the estimated mask Default value: `true`\n   */\n  refine_foreground?: boolean;\n};\nexport type BirefnetV2Output = {\n  /**\n   * Image with background removed\n   */\n  image: Image;\n  /**\n   * Mask used to remove the background\n   */\n  mask_image?: Image;\n};\nexport type BlurMaskInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The radius of the Gaussian blur. Default value: `5`\n   */\n  radius?: number;\n};\nexport type BlurMaskOutput = {\n  /**\n   * The mask\n   */\n  image: Image;\n};\nexport type BrEngOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type BriaBackgroundRemoveInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaBackgroundRemoveOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n};\nexport type BriaBackgroundReplaceInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the reference image to be used for generating the new background. Use \"\" to leave empty. Either ref_image_url or bg_prompt has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp. Default value: `\"\"`\n   */\n  ref_image_url?: string | Blob | File;\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt?: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Whether to refine prompt Default value: `true`\n   */\n  refine_prompt?: boolean;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Whether to use the fast model Default value: `true`\n   */\n  fast?: boolean;\n  /**\n   * Number of Images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaBackgroundReplaceOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BriaEraserInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the binary mask image that represents the area that will be cleaned.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * You can use this parameter to specify the type of the input mask from the list. 'manual' opttion should be used in cases in which the mask had been generated by a user (e.g. with a brush tool), and 'automatic' mask type should be used when mask had been generated by an algorithm like 'SAM'. Default value: `\"manual\"`\n   */\n  mask_type?: \"manual\" | \"automatic\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, attempts to preserve the alpha channel of the input image.\n   */\n  preserve_alpha?: boolean;\n};\nexport type BriaEraserOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n};\nexport type BriaExpandInput = {\n  /**\n   * The URL of the input image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The desired size of the final image, after the expansion. should have an area of less than 5000x5000 pixels.\n   */\n  canvas_size: Array<number>;\n  /**\n   * The desired size of the original image, inside the full canvas. Ensure that the ratio of input image foreground or main subject to the canvas area is greater than 15% to achieve optimal results.\n   */\n  original_image_size: Array<number>;\n  /**\n   * The desired location of the original image, inside the full canvas. Provide the location of the upper left corner of the original image. The location can also be outside the canvas (the original image will be cropped).\n   */\n  original_image_location: Array<number>;\n  /**\n   * Text on which you wish to base the image expansion. This parameter is optional. Bria currently supports prompts in English only, excluding special characters. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * You can choose whether you want your generated expension to be random or predictable. You can recreate the same result in the future by using the seed value of a result from the response. You can exclude this parameter if you are not interested in recreating your results. This parameter is optional.\n   */\n  seed?: number;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of Images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaExpandOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BriaGenfillInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the binary mask image that represents the area that will be cleaned.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of Images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaGenfillOutput = {\n  /**\n   * Generated Images\n   */\n  images: Array<Image>;\n};\nexport type BriaProductShotInput = {\n  /**\n   * The URL of the product shot to be placed in a lifestyle shot. If both image_url and image_file are provided, image_url will be used. Accepted formats are jpeg, jpg, png, webp. Maximum file size 12MB.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text description of the new scene or background for the provided product shot. Bria currently supports prompts in English only, excluding special characters.\n   */\n  scene_description?: string;\n  /**\n   * The URL of the reference image to be used for generating the new scene or background for the product shot. Use \"\" to leave empty.Either ref_image_url or scene_description has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp. Default value: `\"\"`\n   */\n  ref_image_url?: string | Blob | File;\n  /**\n   * Whether to optimize the scene description Default value: `true`\n   */\n  optimize_description?: boolean;\n  /**\n   * The number of lifestyle product shots you would like to generate. You will get num_results x 10 results when placement_type=automatic and according to the number of required placements x num_results if placement_type=manual_placement. Default value: `1`\n   */\n  num_results?: number;\n  /**\n   * Whether to use the fast model Default value: `true`\n   */\n  fast?: boolean;\n  /**\n   * This parameter allows you to control the positioning of the product in the image. Choosing 'original' will preserve the original position of the product in the image. Choosing 'automatic' will generate results with the 10 recommended positions for the product. Choosing 'manual_placement' will allow you to select predefined positions (using the parameter 'manual_placement_selection'). Selecting 'manual_padding' will allow you to control the position and size of the image by defining the desired padding in pixels around the product. Default value: `\"manual_placement\"`\n   */\n  placement_type?:\n    | \"original\"\n    | \"automatic\"\n    | \"manual_placement\"\n    | \"manual_padding\";\n  /**\n   * This flag is only relevant when placement_type=original. If true, the output image retains the original input image's size; otherwise, the image is scaled to 1 megapixel (1MP) while preserving its aspect ratio.\n   */\n  original_quality?: boolean;\n  /**\n   * The desired size of the final product shot. For optimal results, the total number of pixels should be around 1,000,000. This parameter is only relevant when placement_type=automatic or placement_type=manual_placement. Default value: `1000,1000`\n   */\n  shot_size?: Array<number>;\n  /**\n   * If you've selected placement_type=manual_placement, you should use this parameter to specify which placements/positions you would like to use from the list. You can select more than one placement in one request. Default value: `\"bottom_center\"`\n   */\n  manual_placement_selection?:\n    | \"upper_left\"\n    | \"upper_right\"\n    | \"bottom_left\"\n    | \"bottom_right\"\n    | \"right_center\"\n    | \"left_center\"\n    | \"upper_center\"\n    | \"bottom_center\"\n    | \"center_vertical\"\n    | \"center_horizontal\";\n  /**\n   * The desired padding in pixels around the product, when using placement_type=manual_padding. The order of the values is [left, right, top, bottom]. For optimal results, the total number of pixels, including padding, should be around 1,000,000. It is recommended to first use the product cutout API, get the cutout and understand the size of the result, and then define the required padding and use the cutout as an input for this API.\n   */\n  padding_values?: Array<number>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaProductShotOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n};\nexport type BriaTextToImageBaseInput = {\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * How many images you would like to generate. When using any Guidance Method, Value is set to 1. Default value: `4`\n   */\n  num_images?: number;\n  /**\n   * The aspect ratio of the image. When a guidance method is being used, the aspect ratio is defined by the guidance image and this parameter is ignored. Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:2\"\n    | \"3:4\"\n    | \"4:3\"\n    | \"4:5\"\n    | \"5:4\"\n    | \"9:16\"\n    | \"16:9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of iterations the model goes through to refine the generated image. This parameter is optional. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * When set to true, enhances the provided prompt by generating additional, more descriptive variations, resulting in more diverse and creative output images.\n   */\n  prompt_enhancement?: boolean;\n  /**\n   * Which medium should be included in your generated images. This parameter is optional.\n   */\n  medium?: \"photography\" | \"art\";\n  /**\n   * Guidance images to use for the generation. Up to 4 guidance methods can be combined during a single inference. Default value: ``\n   */\n  guidance?: Array<GuidanceInput>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaTextToImageBaseOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BriaTextToImageFastInput = {\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * How many images you would like to generate. When using any Guidance Method, Value is set to 1. Default value: `4`\n   */\n  num_images?: number;\n  /**\n   * The aspect ratio of the image. When a guidance method is being used, the aspect ratio is defined by the guidance image and this parameter is ignored. Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:2\"\n    | \"3:4\"\n    | \"4:3\"\n    | \"4:5\"\n    | \"5:4\"\n    | \"9:16\"\n    | \"16:9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of iterations the model goes through to refine the generated image. This parameter is optional. Default value: `8`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * When set to true, enhances the provided prompt by generating additional, more descriptive variations, resulting in more diverse and creative output images.\n   */\n  prompt_enhancement?: boolean;\n  /**\n   * Which medium should be included in your generated images. This parameter is optional.\n   */\n  medium?: \"photography\" | \"art\";\n  /**\n   * Guidance images to use for the generation. Up to 4 guidance methods can be combined during a single inference. Default value: ``\n   */\n  guidance?: Array<GuidanceInput>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaTextToImageFastOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BriaTextToImageHdInput = {\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * How many images you would like to generate. When using any Guidance Method, Value is set to 1. Default value: `4`\n   */\n  num_images?: number;\n  /**\n   * The aspect ratio of the image. When a guidance method is being used, the aspect ratio is defined by the guidance image and this parameter is ignored. Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:2\"\n    | \"3:4\"\n    | \"4:3\"\n    | \"4:5\"\n    | \"5:4\"\n    | \"9:16\"\n    | \"16:9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of iterations the model goes through to refine the generated image. This parameter is optional. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * When set to true, enhances the provided prompt by generating additional, more descriptive variations, resulting in more diverse and creative output images.\n   */\n  prompt_enhancement?: boolean;\n  /**\n   * Which medium should be included in your generated images. This parameter is optional.\n   */\n  medium?: \"photography\" | \"art\";\n  /**\n   * Guidance images to use for the generation. Up to 4 guidance methods can be combined during a single inference. Default value: ``\n   */\n  guidance?: Array<GuidanceInput>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaTextToImageHdOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BrPortugeseOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type CannyInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Low threshold for the hysteresis procedure. Edges with a strength higher than the low threshold will appear in the output image, if there are strong edges nearby. Default value: `100`\n   */\n  low_threshold?: number;\n  /**\n   * High threshold for the hysteresis procedure. Edges with a strength higher than the high threshold will always appear as edges in the output image. Default value: `200`\n   */\n  high_threshold?: number;\n};\nexport type CannyOutput = {\n  /**\n   * Image with edges detected using the Canny algorithm\n   */\n  image: Image;\n};\nexport type CatVtonInput = {\n  /**\n   * Url for the human image.\n   */\n  human_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  garment_image_url: string | Blob | File;\n  /**\n   * Type of the Cloth to be tried on.\n   *\n   * Options:\n   * upper: Upper body cloth\n   * lower: Lower body cloth\n   * overall: Full body cloth\n   * inner: Inner cloth, like T-shirt inside a jacket\n   * outer: Outer cloth, like a jacket over a T-shirt\n   */\n  cloth_type: \"upper\" | \"lower\" | \"overall\" | \"inner\" | \"outer\";\n  /**\n   * The size of the generated image. Default value: `portrait_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same input given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n};\nexport type CatVtonOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n};\nexport type CcsrInput = {\n  /**\n   * The text prompt you would like to convert to speech.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The scale of the output image. The higher the scale, the bigger the output image will be. Default value: `2`\n   */\n  scale?: number;\n  /**\n   * If specified, a patch-based sampling strategy will be used for sampling. Default value: `\"none\"`\n   */\n  tile_diffusion?: \"none\" | \"mix\" | \"gaussian\";\n  /**\n   * Size of patch. Default value: `1024`\n   */\n  tile_diffusion_size?: number;\n  /**\n   * Stride of sliding patch. Default value: `512`\n   */\n  tile_diffusion_stride?: number;\n  /**\n   * If specified, a patch-based sampling strategy will be used for VAE decoding.\n   */\n  tile_vae?: boolean;\n  /**\n   * Size of VAE patch. Default value: `226`\n   */\n  tile_vae_decoder_size?: number;\n  /**\n   * Size of latent image Default value: `1024`\n   */\n  tile_vae_encoder_size?: number;\n  /**\n   * The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate. Default value: `50`\n   */\n  steps?: number;\n  /**\n   * The ending point of uniform sampling strategy. Default value: `0.6667`\n   */\n  t_max?: number;\n  /**\n   * The starting point of uniform sampling strategy. Default value: `0.3333`\n   */\n  t_min?: number;\n  /**\n   * Type of color correction for samples. Default value: `\"adain\"`\n   */\n  color_fix_type?: \"none\" | \"wavelet\" | \"adain\";\n  /**\n   * Seed for reproducibility. Different seeds will make slightly different results.\n   */\n  seed?: number;\n};\nexport type CcsrOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n  /**\n   * The seed used for the generation.\n   */\n  seed: number;\n};\nexport type ChatInput = {\n  /**\n   * Name of the model to use. Premium models are charged at 10x the rate of standard models, they include: meta-llama/llama-3.2-90b-vision-instruct, openai/gpt-4o, anthropic/claude-3-5-haiku, google/gemini-pro-1.5, anthropic/claude-3.5-sonnet. Default value: `\"google/gemini-flash-1.5\"`\n   */\n  model?:\n    | \"anthropic/claude-3.5-sonnet\"\n    | \"anthropic/claude-3-5-haiku\"\n    | \"anthropic/claude-3-haiku\"\n    | \"google/gemini-pro-1.5\"\n    | \"google/gemini-flash-1.5\"\n    | \"google/gemini-flash-1.5-8b\"\n    | \"meta-llama/llama-3.2-1b-instruct\"\n    | \"meta-llama/llama-3.2-3b-instruct\"\n    | \"meta-llama/llama-3.1-8b-instruct\"\n    | \"meta-llama/llama-3.1-70b-instruct\"\n    | \"openai/gpt-4o-mini\"\n    | \"openai/gpt-4o\"\n    | \"deepseek/deepseek-r1\";\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * System prompt to provide context or instructions to the model\n   */\n  system_prompt?: string;\n  /**\n   * Should reasoning be the part of the final answer.\n   */\n  reasoning?: boolean;\n};\nexport type ClarityUpscalerInput = {\n  /**\n   * The URL of the image to upscale.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results. Default value: `\"masterpiece, best quality, highres\"`\n   */\n  prompt?: string;\n  /**\n   * The upscale factor Default value: `2`\n   */\n  upscale_factor?: number;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want in the image. Default value: `\"(worst quality, low quality, normal quality:2)\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The creativity of the model. The higher the creativity, the more the model will deviate from the prompt.\n   * Refers to the denoise strength of the sampling. Default value: `0.35`\n   */\n  creativity?: number;\n  /**\n   * The resemblance of the upscaled image to the original image. The higher the resemblance, the more the model will try to keep the original image.\n   * Refers to the strength of the ControlNet. Default value: `0.6`\n   */\n  resemblance?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `18`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type ClarityUpscalerOutput = {\n  /**\n   * The URL of the generated image.\n   */\n  image: Image;\n  /**\n   * The seed used to generate the image.\n   */\n  seed: number;\n  /**\n   * The timings of the different steps in the workflow.\n   */\n  timings: any;\n};\nexport type CodeformerInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Weight of the fidelity factor. Default value: `0.5`\n   */\n  fidelity?: number;\n  /**\n   * Upscaling factor Default value: `2`\n   */\n  upscaling?: number;\n  /**\n   * Should faces etc should be aligned.\n   */\n  aligned?: boolean;\n  /**\n   * Should only center face be restored\n   */\n  only_center_face?: boolean;\n  /**\n   * Should faces be upscaled Default value: `true`\n   */\n  face_upscale?: boolean;\n  /**\n   * Random seed for reproducible generation.\n   */\n  seed?: number;\n};\nexport type CodeformerOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type Cogvideox5bImageToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated video. Default value: `[object Object]`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to generate video from Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRAs to use for the image generation. We currently support one lora. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * Use RIFE for video interpolation Default value: `true`\n   */\n  use_rife?: boolean;\n  /**\n   * The target FPS of the video Default value: `16`\n   */\n  export_fps?: number;\n  /**\n   * The URL to the image to generate the video from.\n   */\n  image_url: string | Blob | File;\n};\nexport type Cogvideox5bImageToVideoOutput = {\n  /**\n   * The URL to the generated video\n   */\n  video: File;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated video. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * The prompt used for generating the video.\n   */\n  prompt: string;\n};\nexport type Cogvideox5bInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated video. Default value: `[object Object]`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to generate video from Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRAs to use for the image generation. We currently support one lora. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * Use RIFE for video interpolation Default value: `true`\n   */\n  use_rife?: boolean;\n  /**\n   * The target FPS of the video Default value: `16`\n   */\n  export_fps?: number;\n};\nexport type Cogvideox5bOutput = {\n  /**\n   * The URL to the generated video\n   */\n  video: File;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated video. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * The prompt used for generating the video.\n   */\n  prompt: string;\n};\nexport type Cogvideox5bVideoToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated video. Default value: `[object Object]`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to generate video from Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRAs to use for the image generation. We currently support one lora. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * Use RIFE for video interpolation Default value: `true`\n   */\n  use_rife?: boolean;\n  /**\n   * The target FPS of the video Default value: `16`\n   */\n  export_fps?: number;\n  /**\n   * The video to generate the video from.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The strength to use for Video to Video.  1.0 completely remakes the video while 0.0 preserves the original. Default value: `0.8`\n   */\n  strength?: number;\n};\nexport type Cogvideox5bVideoToVideoOutput = {\n  /**\n   * The URL to the generated video\n   */\n  video: File;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated video. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * The prompt used for generating the video.\n   */\n  prompt: string;\n};\nexport type Cogview4Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type Cogview4Output = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type CollectionToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ComfyInput = {\n  /**\n   *\n   */\n  prompt: any;\n  /**\n   *\n   */\n  extra_data?: any;\n  /**\n   * Disable saving prompt metadata in files.\n   */\n  disable_metadata?: boolean;\n};\nexport type CompareTextInput = {\n  /**\n   * Input text\n   */\n  text: string;\n  /**\n   * Text to compare against\n   */\n  compare_text: string;\n  /**\n   * Text to return if the input text matches the compare text\n   */\n  return_text: string;\n  /**\n   * Text to return if the input text does not match the compare text\n   */\n  fail_text: string;\n};\nexport type CompositeImageInput = {\n  /**\n   * Input image url.\n   */\n  background_image_url: string | Blob | File;\n  /**\n   * Overlay image url.\n   */\n  overlay_image_url: string | Blob | File;\n  /**\n   * Optional mask image url.\n   */\n  mask_image_url?: string | Blob | File;\n};\nexport type ControlnetsdxlInput = {\n  /**\n   * Url to input image\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The scale of the ControlNet. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n};\nexport type ControlnetsdxlOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type ControlNetUnionInput = {\n  /**\n   * URL of the image to be used as the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * URL of the mask for the control image.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * Control Mode for Flux Controlnet Union. Supported values are:\n   * - canny: Uses the edges for guided generation.\n   * - tile: Uses the tiles for guided generation.\n   * - depth: Utilizes a grayscale depth map for guided generation.\n   * - blur: Adds a blur to the image.\n   * - pose: Uses the pose of the image for guided generation.\n   * - gray: Converts the image to grayscale.\n   * - low-quality: Converts the image to a low-quality image.\n   */\n  control_mode:\n    | \"canny\"\n    | \"tile\"\n    | \"depth\"\n    | \"blur\"\n    | \"pose\"\n    | \"gray\"\n    | \"low-quality\";\n  /**\n   * The scale of the control net weight. This is used to scale the control net weight\n   * before merging it with the base model. Default value: `1`\n   */\n  conditioning_scale?: number;\n  /**\n   * Threshold for mask. Default value: `0.5`\n   */\n  mask_threshold?: number;\n  /**\n   * The percentage of the image to start applying the controlnet in terms of the total timesteps.\n   */\n  start_percentage?: number;\n  /**\n   * The percentage of the image to end applying the controlnet in terms of the total timesteps. Default value: `1`\n   */\n  end_percentage?: number;\n};\nexport type ControlnextInput = {\n  /**\n   * URL of the reference image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * URL of the input video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Height of the output video. Default value: `1024`\n   */\n  height?: number;\n  /**\n   * Width of the output video. Default value: `576`\n   */\n  width?: number;\n  /**\n   * Guidance scale for the diffusion process. Default value: `3`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * Maximum number of frames to process. Default value: `240`\n   */\n  max_frame_num?: number;\n  /**\n   * Number of frames to process in each batch. Default value: `24`\n   */\n  batch_frames?: number;\n  /**\n   * Number of overlapping frames between batches. Default value: `6`\n   */\n  overlap?: number;\n  /**\n   * Stride for sampling frames from the input video. Default value: `2`\n   */\n  sample_stride?: number;\n  /**\n   * Chunk size for decoding frames. Default value: `2`\n   */\n  decode_chunk_size?: number;\n  /**\n   * Motion bucket ID for the pipeline. Default value: `127`\n   */\n  motion_bucket_id?: number;\n  /**\n   * Frames per second for the output video. Default value: `7`\n   */\n  fps?: number;\n  /**\n   * Condition scale for ControlNeXt. Default value: `1`\n   */\n  controlnext_cond_scale?: number;\n};\nexport type ControlnextOutput = {\n  /**\n   * The generated video.\n   */\n  video: File;\n};\nexport type CreateVoiceInput = {\n  /**\n   * Voice name (required, max 255 characters).\n   */\n  name: string;\n  /**\n   * A list of audio URLs used for cloning (must be between 1 and 5 URLs).\n   */\n  samples: Array<AudioInput>;\n};\nexport type CreateVoiceOutput = {\n  /**\n   * The S3 URI of the cloned voice.\n   */\n  voice: string;\n};\nexport type CreativeUpscalerInput = {\n  /**\n   * The type of model to use for the upscaling. Default is SD_1_5 Default value: `\"SD_1_5\"`\n   */\n  model_type?: \"SD_1_5\" | \"SDXL\";\n  /**\n   * The image to upscale.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results. If no prompt is provide BLIP2 will be used to generate a prompt.\n   */\n  prompt?: string;\n  /**\n   * The scale of the output image. The higher the scale, the bigger the output image will be. Default value: `2`\n   */\n  scale?: number;\n  /**\n   * How much the output can deviate from the original Default value: `0.5`\n   */\n  creativity?: number;\n  /**\n   * How much detail to add Default value: `1`\n   */\n  detail?: number;\n  /**\n   * How much to preserve the shape of the original image Default value: `0.25`\n   */\n  shape_preservation?: number;\n  /**\n   * The suffix to add to the prompt. This is useful to add a common ending to all prompts such as 'high quality' etc or embedding tokens. Default value: `\" high quality, highly detailed, high resolution, sharp\"`\n   */\n  prompt_suffix?: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"blurry, low resolution, bad, ugly, low quality, pixelated, interpolated, compression artifacts, noisey, grainy\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to use for generating the image. The more steps\n   * the better the image will be but it will also take longer to generate. Default value: `20`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, the resulting image will be checked whether it includes any\n   * potentially unsafe content. If it does, it will be replaced with a black\n   * image. Default value: `true`\n   */\n  enable_safety_checks?: boolean;\n  /**\n   * If set to true, the image will not be processed by the CCSR model before\n   * being processed by the creativity model.\n   */\n  skip_ccsr?: boolean;\n  /**\n   * Allow for large uploads that could take a very long time.\n   */\n  override_size_limits?: boolean;\n  /**\n   * The URL to the base model to use for the upscaling\n   */\n  base_model_url?: string | Blob | File;\n  /**\n   * The URL to the additional LORA model to use for the upscaling. Default is None\n   */\n  additional_lora_url?: string | Blob | File;\n  /**\n   * The scale of the additional LORA model to use for the upscaling. Default is 1.0 Default value: `1`\n   */\n  additional_lora_scale?: number;\n  /**\n   * The URL to the additional embeddings to use for the upscaling. Default is None\n   */\n  additional_embedding_url?: string | Blob | File;\n};\nexport type CreativeUpscalerOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type Csm1bInput = {\n  /**\n   * The text to generate an audio from.\n   */\n  scene: Array<Turn>;\n  /**\n   * The context to generate an audio from.\n   */\n  context?: Array<Speaker>;\n};\nexport type Csm1bOutput = {\n  /**\n   * The generated audio.\n   */\n  audio: File | string;\n};\nexport type DdcolorInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n};\nexport type DdcolorOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type DepthAnythingV2Input = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n};\nexport type DepthAnythingV2Output = {\n  /**\n   * Image with depth map\n   */\n  image: Image;\n};\nexport type DepthMapInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * a Default value: `6.283185307179586`\n   */\n  a?: number;\n  /**\n   * bg_th Default value: `0.1`\n   */\n  bg_th?: number;\n  /**\n   * depth_and_normal\n   */\n  depth_and_normal?: boolean;\n};\nexport type DepthMapOutput = {\n  /**\n   * The depth map.\n   */\n  image: Image;\n};\nexport type DetectionInput = {\n  /**\n   * Image URL to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of detection to perform\n   */\n  task_type: \"bbox_detection\" | \"point_detection\" | \"gaze_detection\";\n  /**\n   * Text description of what to detect\n   */\n  detection_prompt: string;\n  /**\n   * Whether to use ensemble for gaze detection\n   */\n  use_ensemble?: boolean;\n};\nexport type DetectionOutput = {\n  /**\n   * Output image with detection visualization\n   */\n  image: Image;\n  /**\n   * Detection results as text\n   */\n  text_output: string;\n};\nexport type DevImageToImageInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type DevReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type DevTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type DifferentialDiffusionInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  control_loras?: Array<ControlLoraWeight>;\n  /**\n   * The controlnets to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnet_unions?: Array<ControlNetUnion>;\n  /**\n   * IP-Adapter to use for image generation. Default value: ``\n   */\n  ip_adapters?: Array<IPAdapter>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  real_cfg_scale?: number;\n  /**\n   * Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.\n   * If using XLabs IP-Adapter v1, this will be turned on!.\n   */\n  use_real_cfg?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of Image for Reference-Only\n   */\n  reference_image_url?: string | Blob | File;\n  /**\n   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`\n   */\n  reference_strength?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to bestarted.\n   */\n  reference_start?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`\n   */\n  reference_end?: number;\n  /**\n   * Base shift for the scheduled timesteps Default value: `0.5`\n   */\n  base_shift?: number;\n  /**\n   * Max shift for the scheduled timesteps Default value: `1.15`\n   */\n  max_shift?: number;\n  /**\n   * Specifies whether beta sigmas ought to be used.\n   */\n  use_beta_schedule?: boolean;\n  /**\n   * URL of image to use as initial image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * URL of change map.\n   */\n  change_map_image_url: string | Blob | File;\n  /**\n   * The strength to use for differential diffusion. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type DiffrhythmInput = {\n  /**\n   * The prompt to generate the song from. Must have two sections. Sections start with either [chorus] or a [verse].\n   */\n  lyrics: string;\n  /**\n   * The URL of the reference audio to use for the music generation.\n   */\n  reference_audio_url: string | Blob | File;\n  /**\n   * The number of inference steps to use for the music generation. Default value: `32`\n   */\n  num_inference_steps?: number;\n  /**\n   * The maximum number of frames to use for the music generation. Default value: `2048`\n   */\n  max_frames?: number;\n};\nexport type DiffrhythmOutput = {\n  /**\n   * Generated music file.\n   */\n  audio: File;\n};\nexport type DiffusionEdgeInput = {\n  /**\n   * The text prompt you would like to convert to speech.\n   */\n  image_url: string | Blob | File;\n};\nexport type DiffusionEdgeOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type DocresDewarpInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n};\nexport type DocresDewarpOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type DocresInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Task to perform\n   */\n  task: \"deshadowing\" | \"appearance\" | \"deblurring\" | \"binarization\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n};\nexport type DocResInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Task to perform\n   */\n  task: \"deshadowing\" | \"appearance\" | \"deblurring\" | \"binarization\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n};\nexport type DocresOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type DrctSuperResolutionInput = {\n  /**\n   * URL of the image to upscale.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Upscaling factor. Default value: `\"4\"`\n   */\n  upscaling_factor?: \"4\";\n};\nexport type DrctSuperResolutionOutput = {\n  /**\n   * Upscaled image\n   */\n  image: Image;\n};\nexport type DreamshaperImageToImageInput = {\n  /**\n   * The Dreamshaper model to use.\n   */\n  model_name?:\n    | \"Lykon/dreamshaper-xl-1-0\"\n    | \"Lykon/dreamshaper-xl-v2-turbo\"\n    | \"Lykon/dreamshaper-8\";\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   *  Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n};\nexport type DreamshaperInpaintingInput = {\n  /**\n   * The Dreamshaper model to use.\n   */\n  model_name?:\n    | \"Lykon/dreamshaper-xl-1-0\"\n    | \"Lykon/dreamshaper-xl-v2-turbo\"\n    | \"Lykon/dreamshaper-8\";\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   *  Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n};\nexport type DreamshaperInput = {\n  /**\n   * The Dreamshaper model to use.\n   */\n  model_name?:\n    | \"Lykon/dreamshaper-xl-1-0\"\n    | \"Lykon/dreamshaper-xl-v2-turbo\"\n    | \"Lykon/dreamshaper-8\";\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want in the image. Default value: `\"(worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)\"`\n   */\n  negative_prompt?: string;\n  /**\n   *  Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `35`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n};\nexport type DreamshaperOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type DubbingInput = {\n  /**\n   * Input video URL to be dubbed.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Target language to dub the video to Default value: `\"hindi\"`\n   */\n  target_language?: \"hindi\" | \"turkish\" | \"english\";\n  /**\n   * Whether to lip sync the audio to the video Default value: `true`\n   */\n  do_lipsync?: boolean;\n};\nexport type DubbingOutput = {\n  /**\n   * The generated video with the lip sync.\n   */\n  video: File;\n};\nexport type DwposeInput = {\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n};\nexport type DwposeOutput = {\n  /**\n   * The predicted pose image\n   */\n  image: Image;\n};\nexport type EditImageInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n};\nexport type ElevenlabsAudioIsolationInput = {\n  /**\n   * URL of the audio file to isolate voice from\n   */\n  audio_url: string | Blob | File;\n};\nexport type ElevenlabsAudioIsolationOutput = {\n  /**\n   * The generated audio file\n   */\n  audio: File;\n};\nexport type ElevenlabsSoundEffectsInput = {\n  /**\n   * The text describing the sound effect to generate\n   */\n  text: string;\n  /**\n   * Duration in seconds (0.5-22). If None, optimal duration will be determined from prompt.\n   */\n  duration_seconds?: number;\n  /**\n   * How closely to follow the prompt (0-1). Higher values mean less variation. Default value: `0.3`\n   */\n  prompt_influence?: number;\n};\nexport type ElevenlabsSoundEffectsOutput = {\n  /**\n   * The generated sound effect audio file in MP3 format\n   */\n  audio: File;\n};\nexport type ElevenlabsSpeechToTextInput = {\n  /**\n   * URL of the audio file to transcribe\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Language code of the audio\n   */\n  language_code?: string;\n  /**\n   * Tag audio events like laughter, applause, etc. Default value: `true`\n   */\n  tag_audio_events?: boolean;\n  /**\n   * Whether to annotate who is speaking Default value: `true`\n   */\n  diarize?: boolean;\n};\nexport type ElevenlabsSpeechToTextOutput = {\n  /**\n   * The full transcribed text\n   */\n  text: string;\n  /**\n   * Detected or specified language code\n   */\n  language_code: string;\n  /**\n   * Confidence in language detection\n   */\n  language_probability: number;\n  /**\n   * Word-level transcription details\n   */\n  words: Array<TranscriptionWord>;\n};\nexport type ElevenlabsTtsMultilingualV2Input = {\n  /**\n   * The text to convert to speech\n   */\n  text: string;\n  /**\n   * The voice to use for speech generation Default value: `\"Rachel\"`\n   */\n  voice?: string;\n  /**\n   * Voice stability (0-1) Default value: `0.5`\n   */\n  stability?: number;\n  /**\n   * Similarity boost (0-1) Default value: `0.75`\n   */\n  similarity_boost?: number;\n  /**\n   * Style exaggeration (0-1)\n   */\n  style?: number;\n};\nexport type ElevenlabsTtsMultilingualV2Output = {\n  /**\n   * The generated audio file\n   */\n  audio: File;\n};\nexport type ElevenlabsTtsTurboV25Input = {\n  /**\n   * The text to convert to speech\n   */\n  text: string;\n  /**\n   * The voice to use for speech generation Default value: `\"Rachel\"`\n   */\n  voice?: string;\n  /**\n   * Voice stability (0-1) Default value: `0.5`\n   */\n  stability?: number;\n  /**\n   * Similarity boost (0-1) Default value: `0.75`\n   */\n  similarity_boost?: number;\n  /**\n   * Style exaggeration (0-1)\n   */\n  style?: number;\n};\nexport type ElevenlabsTtsTurboV25Output = {\n  /**\n   * The generated audio file\n   */\n  audio: File;\n};\nexport type Era3dInput = {\n  /**\n   * URL of the image to remove background from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  cfg?: number;\n  /**\n   * Number of steps to run the model for Default value: `40`\n   */\n  steps?: number;\n  /**\n   * Size of the image to crop to Default value: `400`\n   */\n  crop_size?: number;\n  /**\n   * Seed for random number generation Default value: `-1`\n   */\n  seed?: number;\n  /**\n   * Background removal Default value: `true`\n   */\n  background_removal?: boolean;\n};\nexport type Era3dOutput = {\n  /**\n   * Images with background removed\n   */\n  images: Array<Image>;\n  /**\n   * Normal images with background removed\n   */\n  normal_images: Array<Image>;\n  /**\n   * Seed used for random number generation\n   */\n  seed: number;\n};\nexport type EraserInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the binary mask image that represents the area that will be cleaned.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * You can use this parameter to specify the type of the input mask from the list. 'manual' opttion should be used in cases in which the mask had been generated by a user (e.g. with a brush tool), and 'automatic' mask type should be used when mask had been generated by an algorithm like 'SAM'. Default value: `\"manual\"`\n   */\n  mask_type?: \"manual\" | \"automatic\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, attempts to preserve the alpha channel of the input image.\n   */\n  preserve_alpha?: boolean;\n};\nexport type EraserOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n};\nexport type EsrganInput = {\n  /**\n   * Url to input image\n   */\n  image_url: string | Blob | File;\n  /**\n   * Rescaling factor Default value: `2`\n   */\n  scale?: number;\n  /**\n   * Tile size. Default is 0, that is no tile. When encountering the out-of-GPU-memory issue, please specify it, e.g., 400 or 200\n   */\n  tile?: number;\n  /**\n   * Upscaling a face\n   */\n  face?: boolean;\n  /**\n   * Model to use for upscaling Default value: `\"RealESRGAN_x4plus\"`\n   */\n  model?:\n    | \"RealESRGAN_x4plus\"\n    | \"RealESRGAN_x2plus\"\n    | \"RealESRGAN_x4plus_anime_6B\"\n    | \"RealESRGAN_x4_v3\"\n    | \"RealESRGAN_x4_wdn_v3\"\n    | \"RealESRGAN_x4_anime_v3\";\n  /**\n   * Output image format (png or jpeg) Default value: `\"png\"`\n   */\n  output_format?: \"png\" | \"jpeg\";\n};\nexport type EsrganOutput = {\n  /**\n   * Upscaled image\n   */\n  image: Image;\n};\nexport type EvfSamInput = {\n  /**\n   * The prompt to generate segmentation from.\n   */\n  prompt: string;\n  /**\n   * Areas to exclude from segmentation (will be subtracted from prompt results)\n   */\n  negative_prompt?: string;\n  /**\n   * Enable semantic level segmentation for body parts, background or multi objects\n   */\n  semantic_type?: boolean;\n  /**\n   * URL of the input image\n   */\n  image_url: string | Blob | File;\n  /**\n   * Output only the binary mask instead of masked image Default value: `true`\n   */\n  mask_only?: boolean;\n  /**\n   * Use GroundingDINO instead of SAM for segmentation\n   */\n  use_grounding_dino?: boolean;\n  /**\n   * Invert the mask (background becomes foreground and vice versa)\n   */\n  revert_mask?: boolean;\n  /**\n   * Apply Gaussian blur to the mask. Value determines kernel size (must be odd number)\n   */\n  blur_mask?: number;\n  /**\n   * Expand/dilate the mask by specified pixels\n   */\n  expand_mask?: number;\n  /**\n   * Fill holes in the mask using morphological operations\n   */\n  fill_holes?: boolean;\n};\nexport type EvfSamOutput = {\n  /**\n   * The segmented output image\n   */\n  image: File;\n};\nexport type ExtendVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Video to be extended.\n   */\n  video: VideoConditioningInput;\n};\nexport type ExtendVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type EyeCorrectInput = {\n  /**\n   * The URL of the video to correct\n   */\n  video_url: string | Blob | File;\n};\nexport type EyeCorrectOutput = {\n  /**\n   * The corrected video\n   */\n  video: File;\n};\nexport type F5TtsInput = {\n  /**\n   * The text to be converted to speech.\n   */\n  gen_text: string;\n  /**\n   * The URL of the reference audio file.\n   */\n  ref_audio_url: string | Blob | File;\n  /**\n   * The reference text to be used for TTS. If not provided, an ASR (Automatic Speech Recognition) model will be used to generate the reference text. Default value: `\"\"`\n   */\n  ref_text?: string;\n  /**\n   * The name of the model to be used for TTS.\n   */\n  model_type: \"F5-TTS\" | \"E2-TTS\";\n  /**\n   * Whether to remove the silence from the audio file. Default value: `true`\n   */\n  remove_silence?: boolean;\n};\nexport type F5TtsOutput = {\n  /**\n   * The audio file containing the generated speech.\n   */\n  audio_url: AudioFile;\n};\nexport type FaceToStickerInput = {\n  /**\n   * URL of the video.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `20`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The strength of the instant ID. Default value: `0.7`\n   */\n  instant_id_strength?: number;\n  /**\n   * The weight of the IP adapter. Default value: `0.2`\n   */\n  ip_adapter_weight?: number;\n  /**\n   * The amount of noise to add to the IP adapter. Default value: `0.5`\n   */\n  ip_adapter_noise?: number;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Whether to upscale the image 2x.\n   */\n  upscale?: boolean;\n  /**\n   * The number of steps to use for upscaling. Only used if `upscale` is `true`. Default value: `10`\n   */\n  upscale_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FaceToStickerOutput = {\n  /**\n   * The generated images.\n   */\n  images: Array<Image>;\n  /**\n   * The generated face sticker image.\n   */\n  sticker_image: Image;\n  /**\n   * The generated face sticker image with the background removed.\n   */\n  sticker_image_background_removed: Image;\n  /**\n   * Seed used during the inference.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   * The key is the image type and the value is a boolean.\n   */\n  has_nsfw_concepts: any;\n};\nexport type FastAnimatediffTextToVideoInput = {\n  /**\n   * The prompt to use for generating the video. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"(bad quality, worst quality:1.2), ugly faces, bad anime\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of frames to generate for the video. Default value: `16`\n   */\n  num_frames?: number;\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of frames per second to extract from the video. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * The motions to apply to the video.\n   */\n  motions?: Array<\n    \"zoom-out\" | \"zoom-in\" | \"pan-left\" | \"pan-right\" | \"tilt-up\" | \"tilt-down\"\n  >;\n  /**\n   * The size of the video to generate. Default value: `square`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type FastAnimatediffTextToVideoOutput = {\n  /**\n   * Generated video file.\n   */\n  video: File;\n  /**\n   * Seed used for generating the video.\n   */\n  seed: number;\n};\nexport type FastAnimatediffTurboTextToVideoInput = {\n  /**\n   * The prompt to use for generating the video. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"(bad quality, worst quality:1.2), ugly faces, bad anime\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of frames to generate for the video. Default value: `16`\n   */\n  num_frames?: number;\n  /**\n   * The number of inference steps to perform. 4-12 is recommended for turbo mode. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of frames per second to extract from the video. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * The motions to apply to the video.\n   */\n  motions?: Array<\n    \"zoom-out\" | \"zoom-in\" | \"pan-left\" | \"pan-right\" | \"tilt-up\" | \"tilt-down\"\n  >;\n  /**\n   * The size of the video to generate. Default value: `square`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type FastAnimatediffTurboTextToVideoOutput = {\n  /**\n   * Generated video file.\n   */\n  video: File;\n  /**\n   * Seed used for generating the video.\n   */\n  seed: number;\n};\nexport type FastAnimatediffTurboVideoToVideoInput = {\n  /**\n   * URL of the video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The first N number of seconds of video to animate. Default value: `3`\n   */\n  first_n_seconds?: number;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"(bad quality, worst quality:1.2), ugly faces, bad anime\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. 4-12 is recommended for turbo mode. Default value: `8`\n   */\n  num_inference_steps?: number;\n  /**\n   * The strength of the input video in the final output. Default value: `0.7`\n   */\n  strength?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of frames per second to extract from the video. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * The motions to apply to the video.\n   */\n  motions?: Array<\n    \"zoom-out\" | \"zoom-in\" | \"pan-left\" | \"pan-right\" | \"tilt-up\" | \"tilt-down\"\n  >;\n};\nexport type FastAnimatediffTurboVideoToVideoOutput = {\n  /**\n   * Generated video file.\n   */\n  video: File;\n  /**\n   * Seed used for generating the video.\n   */\n  seed: number;\n};\nexport type FastAnimatediffVideoToVideoInput = {\n  /**\n   * URL of the video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The first N number of seconds of video to animate. Default value: `3`\n   */\n  first_n_seconds?: number;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"(bad quality, worst quality:1.2), ugly faces, bad anime\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The strength of the input video in the final output. Default value: `0.7`\n   */\n  strength?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of frames per second to extract from the video. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * The motions to apply to the video.\n   */\n  motions?: Array<\n    \"zoom-out\" | \"zoom-in\" | \"pan-left\" | \"pan-right\" | \"tilt-up\" | \"tilt-down\"\n  >;\n};\nexport type FastAnimatediffVideoToVideoOutput = {\n  /**\n   * Generated video file.\n   */\n  video: File;\n  /**\n   * Seed used for generating the video.\n   */\n  seed: number;\n};\nexport type FastFooocusSdxlImageToImageInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the prompt image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `8`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n  /**\n   * If set to true, a smaller model will try to refine the output after it was processed. Default value: `true`\n   */\n  enable_refiner?: boolean;\n};\nexport type FastFooocusSdxlImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FastFooocusSdxlInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `8`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n  /**\n   * If set to true, a smaller model will try to refine the output after it was processed. Default value: `true`\n   */\n  enable_refiner?: boolean;\n};\nexport type FastFooocusSdxlOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FastLcmDiffusionImageToImageInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/stable-diffusion-xl-base-1.0\"`\n   */\n  model_name?:\n    | \"stabilityai/stable-diffusion-xl-base-1.0\"\n    | \"runwayml/stable-diffusion-v1-5\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `6`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n};\nexport type FastLcmDiffusionImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FastLcmDiffusionInpaintingInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/stable-diffusion-xl-base-1.0\"`\n   */\n  model_name?:\n    | \"stabilityai/stable-diffusion-xl-base-1.0\"\n    | \"runwayml/stable-diffusion-v1-5\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `6`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n};\nexport type FastLcmDiffusionInpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FastLcmDiffusionInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/stable-diffusion-xl-base-1.0\"`\n   */\n  model_name?:\n    | \"stabilityai/stable-diffusion-xl-base-1.0\"\n    | \"runwayml/stable-diffusion-v1-5\";\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `6`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n};\nexport type FastLcmDiffusionOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FastLightningSdxlImageToImageInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"4\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\" | \"8\";\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type FastLightningSdxlImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FastLightningSdxlInpaintingInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"4\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\" | \"8\";\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type FastLightningSdxlInpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FastLightningSdxlInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"4\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\" | \"8\";\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type FastLightningSdxlOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FastSdxlControlnetCannyImageToImageInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The URL of the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type FastSdxlControlnetCannyImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n};\nexport type FastSdxlControlnetCannyInpaintingInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The URL of the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type FastSdxlControlnetCannyInpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n};\nexport type FastSdxlControlnetCannyInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The URL of the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, DeepCache will be enabled. TBD\n   */\n  enable_deep_cache?: boolean;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type FastSdxlControlnetCannyOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n};\nexport type FastSdxlImageToImageInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n  /**\n   * If set to true, the aspect ratio of the generated image will be preserved even\n   * if the image size is too large. However, if the image is not a multiple of 32\n   * in width or height, it will be resized to the nearest multiple of 32. By default,\n   * this snapping to the nearest multiple of 32 will not preserve the aspect ratio.\n   * Set crop_output to True, to crop the output to the proper aspect ratio\n   * after generating.\n   */\n  preserve_aspect_ratio?: boolean;\n  /**\n   * If set to true, the output cropped to the proper aspect ratio after generating.\n   */\n  crop_output?: boolean;\n};\nexport type FastSdxlImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FastSdxlInpaintingInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n};\nexport type FastSdxlInpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FastSdxlInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n};\nexport type FastSdxlOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FastSVDImageInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The motion bucket id determines the motion of the generated video. The\n   * higher the number, the more motion there will be. Default value: `127`\n   */\n  motion_bucket_id?: number;\n  /**\n   * The conditoning augmentation determines the amount of noise that will be\n   * added to the conditioning frame. The higher the number, the more noise\n   * there will be, and the less the video will look like the initial image.\n   * Increase it for more motion. Default value: `0.02`\n   */\n  cond_aug?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of steps to run the model for. The higher the number the better\n   * the quality and longer it will take to generate. Default value: `4`\n   */\n  steps?: number;\n  /**\n   * The FPS of the generated video. The higher the number, the faster the video will\n   * play. Total video length is 25 frames. Default value: `10`\n   */\n  fps?: number;\n};\nexport type FastSvdLcmInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The motion bucket id determines the motion of the generated video. The\n   * higher the number, the more motion there will be. Default value: `127`\n   */\n  motion_bucket_id?: number;\n  /**\n   * The conditoning augmentation determines the amount of noise that will be\n   * added to the conditioning frame. The higher the number, the more noise\n   * there will be, and the less the video will look like the initial image.\n   * Increase it for more motion. Default value: `0.02`\n   */\n  cond_aug?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of steps to run the model for. The higher the number the better\n   * the quality and longer it will take to generate. Default value: `4`\n   */\n  steps?: number;\n  /**\n   * The FPS of the generated video. The higher the number, the faster the video will\n   * play. Total video length is 25 frames. Default value: `10`\n   */\n  fps?: number;\n};\nexport type FastSvdLcmOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type FastSvdLcmTextToVideoInput = {\n  /**\n   * The prompt to use as a starting point for the generation.\n   */\n  prompt: string;\n  /**\n   * The motion bucket id determines the motion of the generated video. The\n   * higher the number, the more motion there will be. Default value: `127`\n   */\n  motion_bucket_id?: number;\n  /**\n   * The conditoning augmentation determines the amount of noise that will be\n   * added to the conditioning frame. The higher the number, the more noise\n   * there will be, and the less the video will look like the initial image.\n   * Increase it for more motion. Default value: `0.02`\n   */\n  cond_aug?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of steps to run the model for. The higher the number the better\n   * the quality and longer it will take to generate. Default value: `4`\n   */\n  steps?: number;\n  /**\n   * The FPS of the generated video. The higher the number, the faster the video will\n   * play. Total video length is 25 frames. Default value: `10`\n   */\n  fps?: number;\n  /**\n   * The size of the generated video. Default value: `landscape_16_9`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type FastSvdLcmTextToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type FastSVDTextInput = {\n  /**\n   * The prompt to use as a starting point for the generation.\n   */\n  prompt: string;\n  /**\n   * The motion bucket id determines the motion of the generated video. The\n   * higher the number, the more motion there will be. Default value: `127`\n   */\n  motion_bucket_id?: number;\n  /**\n   * The conditoning augmentation determines the amount of noise that will be\n   * added to the conditioning frame. The higher the number, the more noise\n   * there will be, and the less the video will look like the initial image.\n   * Increase it for more motion. Default value: `0.02`\n   */\n  cond_aug?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of steps to run the model for. The higher the number the better\n   * the quality and longer it will take to generate. Default value: `4`\n   */\n  steps?: number;\n  /**\n   * The FPS of the generated video. The higher the number, the faster the video will\n   * play. Total video length is 25 frames. Default value: `10`\n   */\n  fps?: number;\n  /**\n   * The size of the generated video. Default value: `landscape_16_9`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type FastSvdTextToVideoInput = {\n  /**\n   * The prompt to use as a starting point for the generation.\n   */\n  prompt: string;\n  /**\n   * The motion bucket id determines the motion of the generated video. The\n   * higher the number, the more motion there will be. Default value: `127`\n   */\n  motion_bucket_id?: number;\n  /**\n   * The conditoning augmentation determines the amount of noise that will be\n   * added to the conditioning frame. The higher the number, the more noise\n   * there will be, and the less the video will look like the initial image.\n   * Increase it for more motion. Default value: `0.02`\n   */\n  cond_aug?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of steps to run the model for. The higher the number the better\n   * the quality and longer it will take to generate. Default value: `20`\n   */\n  steps?: number;\n  /**\n   * Enabling [DeepCache](https://github.com/horseee/DeepCache) will make the execution\n   * faster, but might sometimes degrade overall quality. The higher the setting, the\n   * faster the execution will be, but the more quality might be lost. Default value: `\"none\"`\n   */\n  deep_cache?: \"none\" | \"minimum\" | \"medium\" | \"high\";\n  /**\n   * The FPS of the generated video. The higher the number, the faster the video will\n   * play. Total video length is 25 frames. Default value: `10`\n   */\n  fps?: number;\n  /**\n   * The negative prompt to use as a starting point for the generation. Default value: `\"unrealistic, saturated, high contrast, big nose, painting, drawing, sketch, cartoon, anime, manga, render, CG, 3d, watermark, signature, label\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated video. Default value: `landscape_16_9`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type FastSvdTextToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type FastTurboDiffusionImageToImageInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/sdxl-turbo\"`\n   */\n  model_name?: \"stabilityai/sdxl-turbo\" | \"stabilityai/sd-turbo\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `2`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type FastTurboDiffusionImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n};\nexport type FastTurboDiffusionInpaintingInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/sdxl-turbo\"`\n   */\n  model_name?: \"stabilityai/sdxl-turbo\" | \"stabilityai/sd-turbo\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `2`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type FastTurboDiffusionInpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n};\nexport type FastTurboDiffusionInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/sdxl-turbo\"`\n   */\n  model_name?: \"stabilityai/sdxl-turbo\" | \"stabilityai/sd-turbo\";\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `2`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type FastTurboDiffusionOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n};\nexport type FfmpegApiComposeInput = {\n  /**\n   * List of tracks to be combined into the final media\n   */\n  tracks: Array<Track>;\n};\nexport type FfmpegApiComposeOutput = {\n  /**\n   * URL of the processed video file\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the video's thumbnail image\n   */\n  thumbnail_url: string | Blob | File;\n};\nexport type FfmpegApiMetadataInput = {\n  /**\n   * URL of the media file (video or audio) to analyze\n   */\n  media_url: string | Blob | File;\n  /**\n   * Whether to extract the start and end frames for videos. Note that when true the request will be slower.\n   */\n  extract_frames?: boolean;\n};\nexport type FfmpegApiMetadataOutput = {\n  /**\n   * Metadata for the analyzed media file (either Video or Audio)\n   */\n  media: Video | Audio;\n};\nexport type FfmpegApiWaveformInput = {\n  /**\n   * URL of the audio file to analyze\n   */\n  media_url: string | Blob | File;\n  /**\n   * Controls how many points are sampled per second of audio. Lower values (e.g. 1-2) create a coarser waveform, higher values (e.g. 4-10) create a more detailed one. Default value: `4`\n   */\n  points_per_second?: number;\n  /**\n   * Number of decimal places for the waveform values. Higher values provide more precision but increase payload size. Default value: `2`\n   */\n  precision?: number;\n  /**\n   * Size of the smoothing window. Higher values create a smoother waveform. Must be an odd number. Default value: `3`\n   */\n  smoothing_window?: number;\n};\nexport type FfmpegApiWaveformOutput = {\n  /**\n   * Normalized waveform data as an array of values between -1 and 1. The number of points is determined by audio duration × points_per_second.\n   */\n  waveform: Array<number>;\n  /**\n   * Duration of the audio in seconds\n   */\n  duration: number;\n  /**\n   * Number of points in the waveform data\n   */\n  points: number;\n  /**\n   * Number of decimal places used in the waveform values\n   */\n  precision: number;\n};\nexport type Florence2LargeCaptionInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n};\nexport type Florence2LargeCaptionOutput = {\n  /**\n   * Results from the model\n   */\n  results: string;\n};\nexport type Florence2LargeCaptionToPhraseGroundingInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text input for the task\n   */\n  text_input: string;\n};\nexport type Florence2LargeCaptionToPhraseGroundingOutput = {\n  /**\n   * Results from the model\n   */\n  results: BoundingBoxes;\n  /**\n   * Processed image\n   */\n  image?: Image;\n};\nexport type Florence2LargeDenseRegionCaptionInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n};\nexport type Florence2LargeDenseRegionCaptionOutput = {\n  /**\n   * Results from the model\n   */\n  results: BoundingBoxes;\n  /**\n   * Processed image\n   */\n  image?: Image;\n};\nexport type Florence2LargeDetailedCaptionInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n};\nexport type Florence2LargeDetailedCaptionOutput = {\n  /**\n   * Results from the model\n   */\n  results: string;\n};\nexport type Florence2LargeMoreDetailedCaptionInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n};\nexport type Florence2LargeMoreDetailedCaptionOutput = {\n  /**\n   * Results from the model\n   */\n  results: string;\n};\nexport type Florence2LargeObjectDetectionInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n};\nexport type Florence2LargeObjectDetectionOutput = {\n  /**\n   * Results from the model\n   */\n  results: BoundingBoxes;\n  /**\n   * Processed image\n   */\n  image?: Image;\n};\nexport type Florence2LargeOcrInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n};\nexport type Florence2LargeOcrOutput = {\n  /**\n   * Results from the model\n   */\n  results: string;\n};\nexport type Florence2LargeOcrWithRegionInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n};\nexport type Florence2LargeOcrWithRegionOutput = {\n  /**\n   * Results from the model\n   */\n  results: OCRBoundingBox;\n  /**\n   * Processed image\n   */\n  image?: Image;\n};\nexport type Florence2LargeOpenVocabularyDetectionInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text input for the task\n   */\n  text_input: string;\n};\nexport type Florence2LargeOpenVocabularyDetectionOutput = {\n  /**\n   * Results from the model\n   */\n  results: BoundingBoxes;\n  /**\n   * Processed image\n   */\n  image?: Image;\n};\nexport type Florence2LargeReferringExpressionSegmentationInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text input for the task\n   */\n  text_input: string;\n};\nexport type Florence2LargeReferringExpressionSegmentationOutput = {\n  /**\n   * Results from the model\n   */\n  results: PolygonOutput;\n  /**\n   * Processed image\n   */\n  image?: Image;\n};\nexport type Florence2LargeRegionProposalInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n};\nexport type Florence2LargeRegionProposalOutput = {\n  /**\n   * Results from the model\n   */\n  results: BoundingBoxes;\n  /**\n   * Processed image\n   */\n  image?: Image;\n};\nexport type Florence2LargeRegionToCategoryInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The user input coordinates\n   */\n  region: Region;\n};\nexport type Florence2LargeRegionToCategoryOutput = {\n  /**\n   * Results from the model\n   */\n  results: string;\n};\nexport type Florence2LargeRegionToDescriptionInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The user input coordinates\n   */\n  region: Region;\n};\nexport type Florence2LargeRegionToDescriptionOutput = {\n  /**\n   * Results from the model\n   */\n  results: string;\n};\nexport type Florence2LargeRegionToSegmentationInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The user input coordinates\n   */\n  region: Region;\n};\nexport type Florence2LargeRegionToSegmentationOutput = {\n  /**\n   * Results from the model\n   */\n  results: PolygonOutput;\n  /**\n   * Processed image\n   */\n  image?: Image;\n};\nexport type FloweditInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Prompt of the image to be used.\n   */\n  source_prompt: string;\n  /**\n   * Prompt of the image to be made.\n   */\n  target_prompt: string;\n  /**\n   * Random seed for reproducible generation. If set none, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Steps for which the model should run. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for the source. Default value: `1.5`\n   */\n  src_guidance_scale?: number;\n  /**\n   * Guidance scale for target. Default value: `5.5`\n   */\n  tar_guidance_scale?: number;\n  /**\n   * Average step count Default value: `1`\n   */\n  n_avg?: number;\n  /**\n   * Control the strength of the edit Default value: `23`\n   */\n  n_max?: number;\n  /**\n   * Minimum step for improved style edits\n   */\n  n_min?: number;\n};\nexport type FloweditOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type FluxControlLoraCannyImageToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image to use for control lora. This is used to control the style of the generated image.\n   */\n  control_lora_image_url?: string | Blob | File;\n  /**\n   * The strength of the control lora. Default value: `1`\n   */\n  control_lora_strength?: number;\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type FluxControlLoraCannyImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxControlLoraCannyInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image to use for control lora. This is used to control the style of the generated image.\n   */\n  control_lora_image_url?: string | Blob | File;\n  /**\n   * The strength of the control lora. Default value: `1`\n   */\n  control_lora_strength?: number;\n};\nexport type FluxControlLoraCannyOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxControlLoraDepthImageToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image to use for control lora. This is used to control the style of the generated image.\n   */\n  control_lora_image_url?: string | Blob | File;\n  /**\n   * The strength of the control lora. Default value: `1`\n   */\n  control_lora_strength?: number;\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type FluxControlLoraDepthImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxControlLoraDepthInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image to use for control lora. This is used to control the style of the generated image.\n   */\n  control_lora_image_url?: string | Blob | File;\n  /**\n   * The strength of the control lora. Default value: `1`\n   */\n  control_lora_strength?: number;\n};\nexport type FluxControlLoraDepthOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxDevImageToImageInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FluxDevImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxDevInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FluxDevOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxDevReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FluxDevReduxOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxDifferentialDiffusionInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * URL of image to use as initial image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * URL of change map.\n   */\n  change_map_image_url: string | Blob | File;\n  /**\n   * The strength to use for image-to-image. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FluxDifferentialDiffusionOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxGeneralDifferentialDiffusionInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  control_loras?: Array<ControlLoraWeight>;\n  /**\n   * The controlnets to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnet_unions?: Array<ControlNetUnion>;\n  /**\n   * IP-Adapter to use for image generation. Default value: ``\n   */\n  ip_adapters?: Array<IPAdapter>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  real_cfg_scale?: number;\n  /**\n   * Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.\n   * If using XLabs IP-Adapter v1, this will be turned on!.\n   */\n  use_real_cfg?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of Image for Reference-Only\n   */\n  reference_image_url?: string | Blob | File;\n  /**\n   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`\n   */\n  reference_strength?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to bestarted.\n   */\n  reference_start?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`\n   */\n  reference_end?: number;\n  /**\n   * Base shift for the scheduled timesteps Default value: `0.5`\n   */\n  base_shift?: number;\n  /**\n   * Max shift for the scheduled timesteps Default value: `1.15`\n   */\n  max_shift?: number;\n  /**\n   * Specifies whether beta sigmas ought to be used.\n   */\n  use_beta_schedule?: boolean;\n  /**\n   * URL of image to use as initial image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * URL of change map.\n   */\n  change_map_image_url: string | Blob | File;\n  /**\n   * The strength to use for differential diffusion. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type FluxGeneralDifferentialDiffusionOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxGeneralImageToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  control_loras?: Array<ControlLoraWeight>;\n  /**\n   * The controlnets to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnet_unions?: Array<ControlNetUnion>;\n  /**\n   * IP-Adapter to use for image generation. Default value: ``\n   */\n  ip_adapters?: Array<IPAdapter>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  real_cfg_scale?: number;\n  /**\n   * Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.\n   * If using XLabs IP-Adapter v1, this will be turned on!.\n   */\n  use_real_cfg?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of Image for Reference-Only\n   */\n  reference_image_url?: string | Blob | File;\n  /**\n   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`\n   */\n  reference_strength?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to bestarted.\n   */\n  reference_start?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`\n   */\n  reference_end?: number;\n  /**\n   * Base shift for the scheduled timesteps Default value: `0.5`\n   */\n  base_shift?: number;\n  /**\n   * Max shift for the scheduled timesteps Default value: `1.15`\n   */\n  max_shift?: number;\n  /**\n   * Specifies whether beta sigmas ought to be used.\n   */\n  use_beta_schedule?: boolean;\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type FluxGeneralImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxGeneralInpaintingInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  control_loras?: Array<ControlLoraWeight>;\n  /**\n   * The controlnets to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnet_unions?: Array<ControlNetUnion>;\n  /**\n   * IP-Adapter to use for image generation. Default value: ``\n   */\n  ip_adapters?: Array<IPAdapter>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  real_cfg_scale?: number;\n  /**\n   * Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.\n   * If using XLabs IP-Adapter v1, this will be turned on!.\n   */\n  use_real_cfg?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of Image for Reference-Only\n   */\n  reference_image_url?: string | Blob | File;\n  /**\n   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`\n   */\n  reference_strength?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to bestarted.\n   */\n  reference_start?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`\n   */\n  reference_end?: number;\n  /**\n   * Base shift for the scheduled timesteps Default value: `0.5`\n   */\n  base_shift?: number;\n  /**\n   * Max shift for the scheduled timesteps Default value: `1.15`\n   */\n  max_shift?: number;\n  /**\n   * Specifies whether beta sigmas ought to be used.\n   */\n  use_beta_schedule?: boolean;\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n  /**\n   * The mask to area to Inpaint in.\n   */\n  mask_url: string | Blob | File;\n};\nexport type FluxGeneralInpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxGeneralInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  control_loras?: Array<ControlLoraWeight>;\n  /**\n   * The controlnets to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnet_unions?: Array<ControlNetUnion>;\n  /**\n   * IP-Adapter to use for image generation. Default value: ``\n   */\n  ip_adapters?: Array<IPAdapter>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  real_cfg_scale?: number;\n  /**\n   * Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.\n   * If using XLabs IP-Adapter v1, this will be turned on!.\n   */\n  use_real_cfg?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of Image for Reference-Only\n   */\n  reference_image_url?: string | Blob | File;\n  /**\n   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`\n   */\n  reference_strength?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to bestarted.\n   */\n  reference_start?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`\n   */\n  reference_end?: number;\n  /**\n   * Base shift for the scheduled timesteps Default value: `0.5`\n   */\n  base_shift?: number;\n  /**\n   * Max shift for the scheduled timesteps Default value: `1.15`\n   */\n  max_shift?: number;\n  /**\n   * Specifies whether beta sigmas ought to be used.\n   */\n  use_beta_schedule?: boolean;\n};\nexport type FluxGeneralOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxGeneralRfInversionInput = {\n  /**\n   * The prompt to edit the image with\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  control_loras?: Array<ControlLoraWeight>;\n  /**\n   * The controlnets to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnet_unions?: Array<ControlNetUnion>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of Image for Reference-Only\n   */\n  reference_image_url?: string | Blob | File;\n  /**\n   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`\n   */\n  reference_strength?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to bestarted.\n   */\n  reference_start?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`\n   */\n  reference_end?: number;\n  /**\n   * Base shift for the scheduled timesteps Default value: `0.5`\n   */\n  base_shift?: number;\n  /**\n   * Max shift for the scheduled timesteps Default value: `1.15`\n   */\n  max_shift?: number;\n  /**\n   * Specifies whether beta sigmas ought to be used.\n   */\n  use_beta_schedule?: boolean;\n  /**\n   * URL of image to be edited\n   */\n  image_url: string | Blob | File;\n  /**\n   * The controller guidance (gamma) used in the creation of structured noise. Default value: `0.6`\n   */\n  controller_guidance_forward?: number;\n  /**\n   * The controller guidance (eta) used in the denoising process.Using values closer to 1 will result in an image closer to input. Default value: `0.75`\n   */\n  controller_guidance_reverse?: number;\n  /**\n   * Timestep to start guidance during reverse process.\n   */\n  reverse_guidance_start?: number;\n  /**\n   * Timestep to stop guidance during reverse process. Default value: `8`\n   */\n  reverse_guidance_end?: number;\n  /**\n   * Scheduler for applying reverse guidance. Default value: `\"constant\"`\n   */\n  reverse_guidance_schedule?:\n    | \"constant\"\n    | \"linear_increase\"\n    | \"linear_decrease\";\n};\nexport type FluxGeneralRfInversionOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxLoraCannyInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `30`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for canny input\n   */\n  image_url: string | Blob | File;\n};\nexport type FluxLoraCannyOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxLoraDepthInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `10`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for depth input\n   */\n  image_url: string | Blob | File;\n};\nexport type FluxLoraDepthOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxLoraFastTrainingInput = {\n  /**\n   * URL to zip archive with images. Try to use at least 4 images in general the more the better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Trigger word to be used in the captions. If None, a trigger word will not be used.\n   * If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.\n   */\n  trigger_word?: string;\n  /**\n   * If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible. Default value: `true`\n   */\n  create_masks?: boolean;\n  /**\n   * Number of steps to train the LoRA on.\n   */\n  steps?: number;\n  /**\n   * If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.\n   */\n  is_style?: boolean;\n  /**\n   * Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.\n   */\n  is_input_format_already_preprocessed?: boolean;\n  /**\n   * The format of the archive. If not specified, the format will be inferred from the URL.\n   */\n  data_archive_format?: string;\n};\nexport type FluxLoraFastTrainingOutput = {\n  /**\n   * URL to the trained diffusers lora weights.\n   */\n  diffusers_lora_file: File;\n  /**\n   * URL to the training configuration file.\n   */\n  config_file: File;\n  /**\n   * URL to the preprocessed images.\n   */\n  debug_preprocessed_output?: File;\n};\nexport type FluxLoraFillInput = {\n  /**\n   * The prompt to generate an image from. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `30`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for fill operation\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask to area to Inpaint in.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * Specifies whether to paste-back the original image onto to the non-inpainted areas of the output Default value: `true`\n   */\n  paste_back?: boolean;\n  /**\n   * Use an image fill input to fill in particular images into the masked area.\n   */\n  fill_image?: ImageFillInput;\n  /**\n   * Resizes the image back to the original size. Use when you wish to preserve the exact image size as the originally provided image.\n   */\n  resize_to_original?: boolean;\n};\nexport type FluxLoraFillOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxLoraImageToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type FluxLoraImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxLoraInpaintingInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n  /**\n   * The mask to area to Inpaint in.\n   */\n  mask_url: string | Blob | File;\n};\nexport type FluxLoraInpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxLoraInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type FluxLoraOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxLoraPortraitTrainerInput = {\n  /**\n   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n   *\n   * The captions can include a special string `[trigger]`. If a trigger_word is specified, it will replace `[trigger]` in the captions.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Trigger phrase to be used in the captions. If None, a trigger word will not be used.\n   * If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.\n   */\n  trigger_phrase?: string;\n  /**\n   * Learning rate to use for training. Default value: `0.00009`\n   */\n  learning_rate?: number;\n  /**\n   * Number of steps to train the LoRA on. Default value: `2500`\n   */\n  steps?: number;\n  /**\n   * If True, multiresolution training will be used. Default value: `true`\n   */\n  multiresolution_training?: boolean;\n  /**\n   * If True, the subject will be cropped from the image. Default value: `true`\n   */\n  subject_crop?: boolean;\n  /**\n   * The format of the archive. If not specified, the format will be inferred from the URL.\n   */\n  data_archive_format?: string;\n  /**\n   * URL to a checkpoint to resume training from. Default value: `\"\"`\n   */\n  resume_from_checkpoint?: string;\n};\nexport type FluxLoraPortraitTrainerOutput = {\n  /**\n   * URL to the trained diffusers lora weights.\n   */\n  diffusers_lora_file: File;\n  /**\n   * URL to the training configuration file.\n   */\n  config_file: File;\n};\nexport type FluxProCannyControlFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `30`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The control image URL to generate the Canny edge map from.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProCannyControlInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The control image URL to generate the Canny edge map from.\n   */\n  control_image_url: string | Blob | File;\n};\nexport type FluxProDepthControlFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `15`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The control image URL to generate the depth map from.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProDepthControlInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The control image URL to generate the depth map from.\n   */\n  control_image_url: string | Blob | File;\n};\nexport type FluxProFillFinetunedInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProFillInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n};\nexport type FluxProNewInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type FluxProNewOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProOutpaintInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image URL to expand using outpainting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Pixels to expand at the top\n   */\n  expand_top?: number;\n  /**\n   * Pixels to expand at the bottom\n   */\n  expand_bottom?: number;\n  /**\n   * Pixels to expand on the left\n   */\n  expand_left?: number;\n  /**\n   * Pixels to expand on the right\n   */\n  expand_right?: number;\n};\nexport type FluxProPlusTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type FluxProTextToImageFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type FluxProTrainerInput = {\n  /**\n   * URL to the training data\n   */\n  data_url: string | Blob | File;\n  /**\n   * Determines the finetuning approach based on your concept Default value: `\"character\"`\n   */\n  mode?: \"character\" | \"product\" | \"style\" | \"general\";\n  /**\n   * Descriptive note to identify your fine-tune since names are UUIDs. Will be displayed in finetune_details.\n   */\n  finetune_comment: string;\n  /**\n   * Defines training duration Default value: `300`\n   */\n  iterations?: number;\n  /**\n   * Learning rate for training. Lower values may be needed for certain scenarios. Default is 1e-5 for full and 1e-4 for LoRA.\n   */\n  learning_rate?: number;\n  /**\n   * The speed priority will improve training and inference speed Default value: `\"quality\"`\n   */\n  priority?: \"speed\" | \"quality\" | \"high_res_only\";\n  /**\n   * Enables/disables automatic image captioning Default value: `true`\n   */\n  captioning?: boolean;\n  /**\n   * Unique word/phrase that will be used in the captions, to reference the newly introduced concepts Default value: `\"TOK\"`\n   */\n  trigger_word?: string;\n  /**\n   * Choose between 32 and 16. A lora_rank of 16 can increase training efficiency and decrease loading times. Default value: `32`\n   */\n  lora_rank?: number;\n  /**\n   * Choose between 'full' for a full finetuning + post hoc extraction of the trained weights into a LoRA or 'lora' for a raw LoRA training Default value: `\"full\"`\n   */\n  finetune_type?: \"full\" | \"lora\";\n};\nexport type FluxProTrainerOutput = {\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n};\nexport type FluxProUltraTextToImageFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image. Default value: `16:9`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\"\n    | string;\n  /**\n   * Generate less processed, more natural-looking images.\n   */\n  raw?: boolean;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProUltraTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image. Default value: `16:9`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\"\n    | string;\n  /**\n   * Generate less processed, more natural-looking images.\n   */\n  raw?: boolean;\n};\nexport type FluxProV11Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type FluxProV11Output = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV11ReduxInput = {\n  /**\n   * The prompt to generate an image from. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n};\nexport type FluxProV11ReduxOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV11UltraFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image. Default value: `16:9`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\"\n    | string;\n  /**\n   * Generate less processed, more natural-looking images.\n   */\n  raw?: boolean;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProV11UltraFinetunedOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV11UltraInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image. Default value: `16:9`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\"\n    | string;\n  /**\n   * Generate less processed, more natural-looking images.\n   */\n  raw?: boolean;\n};\nexport type FluxProV11UltraOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV11UltraReduxInput = {\n  /**\n   * The prompt to generate an image from. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image. Default value: `16:9`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\"\n    | string;\n  /**\n   * Generate less processed, more natural-looking images.\n   */\n  raw?: boolean;\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the image prompt, between 0 and 1. Default value: `0.1`\n   */\n  image_prompt_strength?: number;\n};\nexport type FluxProV11UltraReduxOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1CannyFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `30`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The control image URL to generate the Canny edge map from.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProV1CannyFinetunedOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1CannyInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The control image URL to generate the Canny edge map from.\n   */\n  control_image_url: string | Blob | File;\n};\nexport type FluxProV1CannyOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1DepthFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `15`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The control image URL to generate the depth map from.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProV1DepthFinetunedOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1DepthInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The control image URL to generate the depth map from.\n   */\n  control_image_url: string | Blob | File;\n};\nexport type FluxProV1DepthOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1FillFinetunedInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProV1FillFinetunedOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1FillInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n};\nexport type FluxProV1FillOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1ReduxInput = {\n  /**\n   * The prompt to generate an image from. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n};\nexport type FluxProV1ReduxOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxPulidInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * URL of image to use for inpainting.\n   */\n  reference_image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `20`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The prompt to generate an image from. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of steps to start the CFG from.\n   */\n  start_step?: number;\n  /**\n   * The weight of the CFG loss. Default value: `1`\n   */\n  true_cfg?: number;\n  /**\n   * The weight of the ID loss. Default value: `1`\n   */\n  id_weight?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The maximum sequence length for the model. Default value: `\"128\"`\n   */\n  max_sequence_length?: \"128\" | \"256\" | \"512\";\n};\nexport type FluxPulidOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxSchnellInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FluxSchnellOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxSchnellReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FluxSchnellReduxOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxSubjectInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * URL of image of the subject\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `8`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type FluxSubjectOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FooocusImagePromptInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style to use. Default value: `Fooocus Enhance,Fooocus V2,Fooocus Sharp`\n   */\n  styles?: Array<\n    | \"Fooocus V2\"\n    | \"Fooocus Enhance\"\n    | \"Fooocus Sharp\"\n    | \"Fooocus Semi Realistic\"\n    | \"Fooocus Masterpiece\"\n    | \"Fooocus Photograph\"\n    | \"Fooocus Negative\"\n    | \"Fooocus Cinematic\"\n    | \"SAI 3D Model\"\n    | \"SAI Analog Film\"\n    | \"SAI Anime\"\n    | \"SAI Cinematic\"\n    | \"SAI Comic Book\"\n    | \"SAI Craft Clay\"\n    | \"SAI Digital Art\"\n    | \"SAI Enhance\"\n    | \"SAI Fantasy Art\"\n    | \"SAI Isometric\"\n    | \"SAI Line Art\"\n    | \"SAI Lowpoly\"\n    | \"SAI Neonpunk\"\n    | \"SAI Origami\"\n    | \"SAI Photographic\"\n    | \"SAI Pixel Art\"\n    | \"SAI Texture\"\n    | \"MRE Cinematic Dynamic\"\n    | \"MRE Spontaneous Picture\"\n    | \"MRE Artistic Vision\"\n    | \"MRE Dark Dream\"\n    | \"MRE Gloomy Art\"\n    | \"MRE Bad Dream\"\n    | \"MRE Underground\"\n    | \"MRE Surreal Painting\"\n    | \"MRE Dynamic Illustration\"\n    | \"MRE Undead Art\"\n    | \"MRE Elemental Art\"\n    | \"MRE Space Art\"\n    | \"MRE Ancient Illustration\"\n    | \"MRE Brave Art\"\n    | \"MRE Heroic Fantasy\"\n    | \"MRE Dark Cyberpunk\"\n    | \"MRE Lyrical Geometry\"\n    | \"MRE Sumi E Symbolic\"\n    | \"MRE Sumi E Detailed\"\n    | \"MRE Manga\"\n    | \"MRE Anime\"\n    | \"MRE Comic\"\n    | \"Ads Advertising\"\n    | \"Ads Automotive\"\n    | \"Ads Corporate\"\n    | \"Ads Fashion Editorial\"\n    | \"Ads Food Photography\"\n    | \"Ads Gourmet Food Photography\"\n    | \"Ads Luxury\"\n    | \"Ads Real Estate\"\n    | \"Ads Retail\"\n    | \"Artstyle Abstract\"\n    | \"Artstyle Abstract Expressionism\"\n    | \"Artstyle Art Deco\"\n    | \"Artstyle Art Nouveau\"\n    | \"Artstyle Constructivist\"\n    | \"Artstyle Cubist\"\n    | \"Artstyle Expressionist\"\n    | \"Artstyle Graffiti\"\n    | \"Artstyle Hyperrealism\"\n    | \"Artstyle Impressionist\"\n    | \"Artstyle Pointillism\"\n    | \"Artstyle Pop Art\"\n    | \"Artstyle Psychedelic\"\n    | \"Artstyle Renaissance\"\n    | \"Artstyle Steampunk\"\n    | \"Artstyle Surrealist\"\n    | \"Artstyle Typography\"\n    | \"Artstyle Watercolor\"\n    | \"Futuristic Biomechanical\"\n    | \"Futuristic Biomechanical Cyberpunk\"\n    | \"Futuristic Cybernetic\"\n    | \"Futuristic Cybernetic Robot\"\n    | \"Futuristic Cyberpunk Cityscape\"\n    | \"Futuristic Futuristic\"\n    | \"Futuristic Retro Cyberpunk\"\n    | \"Futuristic Retro Futurism\"\n    | \"Futuristic Sci Fi\"\n    | \"Futuristic Vaporwave\"\n    | \"Game Bubble Bobble\"\n    | \"Game Cyberpunk Game\"\n    | \"Game Fighting Game\"\n    | \"Game Gta\"\n    | \"Game Mario\"\n    | \"Game Minecraft\"\n    | \"Game Pokemon\"\n    | \"Game Retro Arcade\"\n    | \"Game Retro Game\"\n    | \"Game Rpg Fantasy Game\"\n    | \"Game Strategy Game\"\n    | \"Game Streetfighter\"\n    | \"Game Zelda\"\n    | \"Misc Architectural\"\n    | \"Misc Disco\"\n    | \"Misc Dreamscape\"\n    | \"Misc Dystopian\"\n    | \"Misc Fairy Tale\"\n    | \"Misc Gothic\"\n    | \"Misc Grunge\"\n    | \"Misc Horror\"\n    | \"Misc Kawaii\"\n    | \"Misc Lovecraftian\"\n    | \"Misc Macabre\"\n    | \"Misc Manga\"\n    | \"Misc Metropolis\"\n    | \"Misc Minimalist\"\n    | \"Misc Monochrome\"\n    | \"Misc Nautical\"\n    | \"Misc Space\"\n    | \"Misc Stained Glass\"\n    | \"Misc Techwear Fashion\"\n    | \"Misc Tribal\"\n    | \"Misc Zentangle\"\n    | \"Papercraft Collage\"\n    | \"Papercraft Flat Papercut\"\n    | \"Papercraft Kirigami\"\n    | \"Papercraft Paper Mache\"\n    | \"Papercraft Paper Quilling\"\n    | \"Papercraft Papercut Collage\"\n    | \"Papercraft Papercut Shadow Box\"\n    | \"Papercraft Stacked Papercut\"\n    | \"Papercraft Thick Layered Papercut\"\n    | \"Photo Alien\"\n    | \"Photo Film Noir\"\n    | \"Photo Glamour\"\n    | \"Photo Hdr\"\n    | \"Photo Iphone Photographic\"\n    | \"Photo Long Exposure\"\n    | \"Photo Neon Noir\"\n    | \"Photo Silhouette\"\n    | \"Photo Tilt Shift\"\n    | \"Cinematic Diva\"\n    | \"Abstract Expressionism\"\n    | \"Academia\"\n    | \"Action Figure\"\n    | \"Adorable 3D Character\"\n    | \"Adorable Kawaii\"\n    | \"Art Deco\"\n    | \"Art Nouveau\"\n    | \"Astral Aura\"\n    | \"Avant Garde\"\n    | \"Baroque\"\n    | \"Bauhaus Style Poster\"\n    | \"Blueprint Schematic Drawing\"\n    | \"Caricature\"\n    | \"Cel Shaded Art\"\n    | \"Character Design Sheet\"\n    | \"Classicism Art\"\n    | \"Color Field Painting\"\n    | \"Colored Pencil Art\"\n    | \"Conceptual Art\"\n    | \"Constructivism\"\n    | \"Cubism\"\n    | \"Dadaism\"\n    | \"Dark Fantasy\"\n    | \"Dark Moody Atmosphere\"\n    | \"Dmt Art Style\"\n    | \"Doodle Art\"\n    | \"Double Exposure\"\n    | \"Dripping Paint Splatter Art\"\n    | \"Expressionism\"\n    | \"Faded Polaroid Photo\"\n    | \"Fauvism\"\n    | \"Flat 2d Art\"\n    | \"Fortnite Art Style\"\n    | \"Futurism\"\n    | \"Glitchcore\"\n    | \"Glo Fi\"\n    | \"Googie Art Style\"\n    | \"Graffiti Art\"\n    | \"Harlem Renaissance Art\"\n    | \"High Fashion\"\n    | \"Idyllic\"\n    | \"Impressionism\"\n    | \"Infographic Drawing\"\n    | \"Ink Dripping Drawing\"\n    | \"Japanese Ink Drawing\"\n    | \"Knolling Photography\"\n    | \"Light Cheery Atmosphere\"\n    | \"Logo Design\"\n    | \"Luxurious Elegance\"\n    | \"Macro Photography\"\n    | \"Mandola Art\"\n    | \"Marker Drawing\"\n    | \"Medievalism\"\n    | \"Minimalism\"\n    | \"Neo Baroque\"\n    | \"Neo Byzantine\"\n    | \"Neo Futurism\"\n    | \"Neo Impressionism\"\n    | \"Neo Rococo\"\n    | \"Neoclassicism\"\n    | \"Op Art\"\n    | \"Ornate And Intricate\"\n    | \"Pencil Sketch Drawing\"\n    | \"Pop Art 2\"\n    | \"Rococo\"\n    | \"Silhouette Art\"\n    | \"Simple Vector Art\"\n    | \"Sketchup\"\n    | \"Steampunk 2\"\n    | \"Surrealism\"\n    | \"Suprematism\"\n    | \"Terragen\"\n    | \"Tranquil Relaxing Atmosphere\"\n    | \"Sticker Designs\"\n    | \"Vibrant Rim Light\"\n    | \"Volumetric Lighting\"\n    | \"Watercolor 2\"\n    | \"Whimsical And Playful\"\n    | \"Mk Chromolithography\"\n    | \"Mk Cross Processing Print\"\n    | \"Mk Dufaycolor Photograph\"\n    | \"Mk Herbarium\"\n    | \"Mk Punk Collage\"\n    | \"Mk Mosaic\"\n    | \"Mk Van Gogh\"\n    | \"Mk Coloring Book\"\n    | \"Mk Singer Sargent\"\n    | \"Mk Pollock\"\n    | \"Mk Basquiat\"\n    | \"Mk Andy Warhol\"\n    | \"Mk Halftone Print\"\n    | \"Mk Gond Painting\"\n    | \"Mk Albumen Print\"\n    | \"Mk Aquatint Print\"\n    | \"Mk Anthotype Print\"\n    | \"Mk Inuit Carving\"\n    | \"Mk Bromoil Print\"\n    | \"Mk Calotype Print\"\n    | \"Mk Color Sketchnote\"\n    | \"Mk Cibulak Porcelain\"\n    | \"Mk Alcohol Ink Art\"\n    | \"Mk One Line Art\"\n    | \"Mk Blacklight Paint\"\n    | \"Mk Carnival Glass\"\n    | \"Mk Cyanotype Print\"\n    | \"Mk Cross Stitching\"\n    | \"Mk Encaustic Paint\"\n    | \"Mk Embroidery\"\n    | \"Mk Gyotaku\"\n    | \"Mk Luminogram\"\n    | \"Mk Lite Brite Art\"\n    | \"Mk Mokume Gane\"\n    | \"Pebble Art\"\n    | \"Mk Palekh\"\n    | \"Mk Suminagashi\"\n    | \"Mk Scrimshaw\"\n    | \"Mk Shibori\"\n    | \"Mk Vitreous Enamel\"\n    | \"Mk Ukiyo E\"\n    | \"Mk Vintage Airline Poster\"\n    | \"Mk Vintage Travel Poster\"\n    | \"Mk Bauhaus Style\"\n    | \"Mk Afrofuturism\"\n    | \"Mk Atompunk\"\n    | \"Mk Constructivism\"\n    | \"Mk Chicano Art\"\n    | \"Mk De Stijl\"\n    | \"Mk Dayak Art\"\n    | \"Mk Fayum Portrait\"\n    | \"Mk Illuminated Manuscript\"\n    | \"Mk Kalighat Painting\"\n    | \"Mk Madhubani Painting\"\n    | \"Mk Pictorialism\"\n    | \"Mk Pichwai Painting\"\n    | \"Mk Patachitra Painting\"\n    | \"Mk Samoan Art Inspired\"\n    | \"Mk Tlingit Art\"\n    | \"Mk Adnate Style\"\n    | \"Mk Ron English Style\"\n    | \"Mk Shepard Fairey Style\"\n  >;\n  /**\n   * You can choose Speed or Quality Default value: `\"Extreme Speed\"`\n   */\n  performance?: \"Speed\" | \"Quality\" | \"Extreme Speed\" | \"Lightning\";\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The sharpness of the generated image. Use it to control how sharp the generated\n   * image should be. Higher value means image and texture are sharper. Default value: `2`\n   */\n  sharpness?: number;\n  /**\n   * The size of the generated image. You can choose between some presets or\n   * custom height and width that **must be multiples of 8**. Default value: `\"1024x1024\"`\n   */\n  aspect_ratio?: string;\n  /**\n   * Number of images to generate in one request Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use up to 5 LoRAs\n   * and they will be merged together to generate the final image. Default value: `[object Object]`\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * Refiner (SDXL or SD 1.5) Default value: `\"None\"`\n   */\n  refiner_model?: \"None\" | \"realisticVisionV60B1_v51VAE.safetensors\";\n  /**\n   * Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models\n   * 0.8 for XL-refiners; or any value for switching two SDXL models. Default value: `0.8`\n   */\n  refiner_switch?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"png\" | \"jpeg\" | \"webp\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   *\n   */\n  image_prompt_1: ImagePrompt;\n  /**\n   *\n   */\n  image_prompt_2?: ImagePrompt;\n  /**\n   *\n   */\n  image_prompt_3?: ImagePrompt;\n  /**\n   *\n   */\n  image_prompt_4?: ImagePrompt;\n  /**\n   * The image to use as a reference for inpainting.\n   */\n  inpaint_image_url?: string | Blob | File;\n  /**\n   * The image to use as a mask for the generated image.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * The mode to use for inpainting. Default value: `\"Inpaint or Outpaint (default)\"`\n   */\n  inpaint_mode?:\n    | \"Inpaint or Outpaint (default)\"\n    | \"Improve Detail (face, hand, eyes, etc.)\"\n    | \"Modify Content (add objects, change background, etc.)\";\n  /**\n   * Describe what you want to inpaint. Default value: `\"\"`\n   */\n  inpaint_additional_prompt?: string;\n  /**\n   * The directions to outpaint. Default value: ``\n   */\n  outpaint_selections?: Array<\"Left\" | \"Right\" | \"Top\" | \"Bottom\">;\n  /**\n   * Mixing Image Prompt and Inpaint\n   */\n  mixing_image_prompt_and_inpaint?: boolean;\n  /**\n   * The image to upscale or vary.\n   */\n  uov_image_url?: string | Blob | File;\n  /**\n   * The method to use for upscaling or varying. Default value: `\"Disabled\"`\n   */\n  uov_method?:\n    | \"Disabled\"\n    | \"Vary (Subtle)\"\n    | \"Vary (Strong)\"\n    | \"Upscale (1.5x)\"\n    | \"Upscale (2x)\"\n    | \"Upscale (Fast 2x)\";\n  /**\n   * Mixing Image Prompt and Vary/Upscale\n   */\n  mixing_image_prompt_and_vary_upscale?: boolean;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FooocusImagePromptOutput = {\n  /**\n   * The generated image file info.\n   */\n  images: Array<Image>;\n  /**\n   * The time taken for the generation process.\n   */\n  timings: any;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n};\nexport type FooocusInpaintInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style to use. Default value: `Fooocus Enhance,Fooocus V2,Fooocus Sharp`\n   */\n  styles?: Array<\n    | \"Fooocus V2\"\n    | \"Fooocus Enhance\"\n    | \"Fooocus Sharp\"\n    | \"Fooocus Semi Realistic\"\n    | \"Fooocus Masterpiece\"\n    | \"Fooocus Photograph\"\n    | \"Fooocus Negative\"\n    | \"Fooocus Cinematic\"\n    | \"SAI 3D Model\"\n    | \"SAI Analog Film\"\n    | \"SAI Anime\"\n    | \"SAI Cinematic\"\n    | \"SAI Comic Book\"\n    | \"SAI Craft Clay\"\n    | \"SAI Digital Art\"\n    | \"SAI Enhance\"\n    | \"SAI Fantasy Art\"\n    | \"SAI Isometric\"\n    | \"SAI Line Art\"\n    | \"SAI Lowpoly\"\n    | \"SAI Neonpunk\"\n    | \"SAI Origami\"\n    | \"SAI Photographic\"\n    | \"SAI Pixel Art\"\n    | \"SAI Texture\"\n    | \"MRE Cinematic Dynamic\"\n    | \"MRE Spontaneous Picture\"\n    | \"MRE Artistic Vision\"\n    | \"MRE Dark Dream\"\n    | \"MRE Gloomy Art\"\n    | \"MRE Bad Dream\"\n    | \"MRE Underground\"\n    | \"MRE Surreal Painting\"\n    | \"MRE Dynamic Illustration\"\n    | \"MRE Undead Art\"\n    | \"MRE Elemental Art\"\n    | \"MRE Space Art\"\n    | \"MRE Ancient Illustration\"\n    | \"MRE Brave Art\"\n    | \"MRE Heroic Fantasy\"\n    | \"MRE Dark Cyberpunk\"\n    | \"MRE Lyrical Geometry\"\n    | \"MRE Sumi E Symbolic\"\n    | \"MRE Sumi E Detailed\"\n    | \"MRE Manga\"\n    | \"MRE Anime\"\n    | \"MRE Comic\"\n    | \"Ads Advertising\"\n    | \"Ads Automotive\"\n    | \"Ads Corporate\"\n    | \"Ads Fashion Editorial\"\n    | \"Ads Food Photography\"\n    | \"Ads Gourmet Food Photography\"\n    | \"Ads Luxury\"\n    | \"Ads Real Estate\"\n    | \"Ads Retail\"\n    | \"Artstyle Abstract\"\n    | \"Artstyle Abstract Expressionism\"\n    | \"Artstyle Art Deco\"\n    | \"Artstyle Art Nouveau\"\n    | \"Artstyle Constructivist\"\n    | \"Artstyle Cubist\"\n    | \"Artstyle Expressionist\"\n    | \"Artstyle Graffiti\"\n    | \"Artstyle Hyperrealism\"\n    | \"Artstyle Impressionist\"\n    | \"Artstyle Pointillism\"\n    | \"Artstyle Pop Art\"\n    | \"Artstyle Psychedelic\"\n    | \"Artstyle Renaissance\"\n    | \"Artstyle Steampunk\"\n    | \"Artstyle Surrealist\"\n    | \"Artstyle Typography\"\n    | \"Artstyle Watercolor\"\n    | \"Futuristic Biomechanical\"\n    | \"Futuristic Biomechanical Cyberpunk\"\n    | \"Futuristic Cybernetic\"\n    | \"Futuristic Cybernetic Robot\"\n    | \"Futuristic Cyberpunk Cityscape\"\n    | \"Futuristic Futuristic\"\n    | \"Futuristic Retro Cyberpunk\"\n    | \"Futuristic Retro Futurism\"\n    | \"Futuristic Sci Fi\"\n    | \"Futuristic Vaporwave\"\n    | \"Game Bubble Bobble\"\n    | \"Game Cyberpunk Game\"\n    | \"Game Fighting Game\"\n    | \"Game Gta\"\n    | \"Game Mario\"\n    | \"Game Minecraft\"\n    | \"Game Pokemon\"\n    | \"Game Retro Arcade\"\n    | \"Game Retro Game\"\n    | \"Game Rpg Fantasy Game\"\n    | \"Game Strategy Game\"\n    | \"Game Streetfighter\"\n    | \"Game Zelda\"\n    | \"Misc Architectural\"\n    | \"Misc Disco\"\n    | \"Misc Dreamscape\"\n    | \"Misc Dystopian\"\n    | \"Misc Fairy Tale\"\n    | \"Misc Gothic\"\n    | \"Misc Grunge\"\n    | \"Misc Horror\"\n    | \"Misc Kawaii\"\n    | \"Misc Lovecraftian\"\n    | \"Misc Macabre\"\n    | \"Misc Manga\"\n    | \"Misc Metropolis\"\n    | \"Misc Minimalist\"\n    | \"Misc Monochrome\"\n    | \"Misc Nautical\"\n    | \"Misc Space\"\n    | \"Misc Stained Glass\"\n    | \"Misc Techwear Fashion\"\n    | \"Misc Tribal\"\n    | \"Misc Zentangle\"\n    | \"Papercraft Collage\"\n    | \"Papercraft Flat Papercut\"\n    | \"Papercraft Kirigami\"\n    | \"Papercraft Paper Mache\"\n    | \"Papercraft Paper Quilling\"\n    | \"Papercraft Papercut Collage\"\n    | \"Papercraft Papercut Shadow Box\"\n    | \"Papercraft Stacked Papercut\"\n    | \"Papercraft Thick Layered Papercut\"\n    | \"Photo Alien\"\n    | \"Photo Film Noir\"\n    | \"Photo Glamour\"\n    | \"Photo Hdr\"\n    | \"Photo Iphone Photographic\"\n    | \"Photo Long Exposure\"\n    | \"Photo Neon Noir\"\n    | \"Photo Silhouette\"\n    | \"Photo Tilt Shift\"\n    | \"Cinematic Diva\"\n    | \"Abstract Expressionism\"\n    | \"Academia\"\n    | \"Action Figure\"\n    | \"Adorable 3D Character\"\n    | \"Adorable Kawaii\"\n    | \"Art Deco\"\n    | \"Art Nouveau\"\n    | \"Astral Aura\"\n    | \"Avant Garde\"\n    | \"Baroque\"\n    | \"Bauhaus Style Poster\"\n    | \"Blueprint Schematic Drawing\"\n    | \"Caricature\"\n    | \"Cel Shaded Art\"\n    | \"Character Design Sheet\"\n    | \"Classicism Art\"\n    | \"Color Field Painting\"\n    | \"Colored Pencil Art\"\n    | \"Conceptual Art\"\n    | \"Constructivism\"\n    | \"Cubism\"\n    | \"Dadaism\"\n    | \"Dark Fantasy\"\n    | \"Dark Moody Atmosphere\"\n    | \"Dmt Art Style\"\n    | \"Doodle Art\"\n    | \"Double Exposure\"\n    | \"Dripping Paint Splatter Art\"\n    | \"Expressionism\"\n    | \"Faded Polaroid Photo\"\n    | \"Fauvism\"\n    | \"Flat 2d Art\"\n    | \"Fortnite Art Style\"\n    | \"Futurism\"\n    | \"Glitchcore\"\n    | \"Glo Fi\"\n    | \"Googie Art Style\"\n    | \"Graffiti Art\"\n    | \"Harlem Renaissance Art\"\n    | \"High Fashion\"\n    | \"Idyllic\"\n    | \"Impressionism\"\n    | \"Infographic Drawing\"\n    | \"Ink Dripping Drawing\"\n    | \"Japanese Ink Drawing\"\n    | \"Knolling Photography\"\n    | \"Light Cheery Atmosphere\"\n    | \"Logo Design\"\n    | \"Luxurious Elegance\"\n    | \"Macro Photography\"\n    | \"Mandola Art\"\n    | \"Marker Drawing\"\n    | \"Medievalism\"\n    | \"Minimalism\"\n    | \"Neo Baroque\"\n    | \"Neo Byzantine\"\n    | \"Neo Futurism\"\n    | \"Neo Impressionism\"\n    | \"Neo Rococo\"\n    | \"Neoclassicism\"\n    | \"Op Art\"\n    | \"Ornate And Intricate\"\n    | \"Pencil Sketch Drawing\"\n    | \"Pop Art 2\"\n    | \"Rococo\"\n    | \"Silhouette Art\"\n    | \"Simple Vector Art\"\n    | \"Sketchup\"\n    | \"Steampunk 2\"\n    | \"Surrealism\"\n    | \"Suprematism\"\n    | \"Terragen\"\n    | \"Tranquil Relaxing Atmosphere\"\n    | \"Sticker Designs\"\n    | \"Vibrant Rim Light\"\n    | \"Volumetric Lighting\"\n    | \"Watercolor 2\"\n    | \"Whimsical And Playful\"\n    | \"Mk Chromolithography\"\n    | \"Mk Cross Processing Print\"\n    | \"Mk Dufaycolor Photograph\"\n    | \"Mk Herbarium\"\n    | \"Mk Punk Collage\"\n    | \"Mk Mosaic\"\n    | \"Mk Van Gogh\"\n    | \"Mk Coloring Book\"\n    | \"Mk Singer Sargent\"\n    | \"Mk Pollock\"\n    | \"Mk Basquiat\"\n    | \"Mk Andy Warhol\"\n    | \"Mk Halftone Print\"\n    | \"Mk Gond Painting\"\n    | \"Mk Albumen Print\"\n    | \"Mk Aquatint Print\"\n    | \"Mk Anthotype Print\"\n    | \"Mk Inuit Carving\"\n    | \"Mk Bromoil Print\"\n    | \"Mk Calotype Print\"\n    | \"Mk Color Sketchnote\"\n    | \"Mk Cibulak Porcelain\"\n    | \"Mk Alcohol Ink Art\"\n    | \"Mk One Line Art\"\n    | \"Mk Blacklight Paint\"\n    | \"Mk Carnival Glass\"\n    | \"Mk Cyanotype Print\"\n    | \"Mk Cross Stitching\"\n    | \"Mk Encaustic Paint\"\n    | \"Mk Embroidery\"\n    | \"Mk Gyotaku\"\n    | \"Mk Luminogram\"\n    | \"Mk Lite Brite Art\"\n    | \"Mk Mokume Gane\"\n    | \"Pebble Art\"\n    | \"Mk Palekh\"\n    | \"Mk Suminagashi\"\n    | \"Mk Scrimshaw\"\n    | \"Mk Shibori\"\n    | \"Mk Vitreous Enamel\"\n    | \"Mk Ukiyo E\"\n    | \"Mk Vintage Airline Poster\"\n    | \"Mk Vintage Travel Poster\"\n    | \"Mk Bauhaus Style\"\n    | \"Mk Afrofuturism\"\n    | \"Mk Atompunk\"\n    | \"Mk Constructivism\"\n    | \"Mk Chicano Art\"\n    | \"Mk De Stijl\"\n    | \"Mk Dayak Art\"\n    | \"Mk Fayum Portrait\"\n    | \"Mk Illuminated Manuscript\"\n    | \"Mk Kalighat Painting\"\n    | \"Mk Madhubani Painting\"\n    | \"Mk Pictorialism\"\n    | \"Mk Pichwai Painting\"\n    | \"Mk Patachitra Painting\"\n    | \"Mk Samoan Art Inspired\"\n    | \"Mk Tlingit Art\"\n    | \"Mk Adnate Style\"\n    | \"Mk Ron English Style\"\n    | \"Mk Shepard Fairey Style\"\n  >;\n  /**\n   * You can choose Speed or Quality Default value: `\"Extreme Speed\"`\n   */\n  performance?: \"Speed\" | \"Quality\" | \"Extreme Speed\" | \"Lightning\";\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The sharpness of the generated image. Use it to control how sharp the generated\n   * image should be. Higher value means image and texture are sharper. Default value: `2`\n   */\n  sharpness?: number;\n  /**\n   * The size of the generated image. You can choose between some presets or\n   * custom height and width that **must be multiples of 8**. Default value: `\"1024x1024\"`\n   */\n  aspect_ratio?: string;\n  /**\n   * Number of images to generate in one request Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use up to 5 LoRAs\n   * and they will be merged together to generate the final image. Default value: `[object Object]`\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * Refiner (SDXL or SD 1.5) Default value: `\"None\"`\n   */\n  refiner_model?: \"None\" | \"realisticVisionV60B1_v51VAE.safetensors\";\n  /**\n   * Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models\n   * 0.8 for XL-refiners; or any value for switching two SDXL models. Default value: `0.8`\n   */\n  refiner_switch?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"png\" | \"jpeg\" | \"webp\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The image to use as a reference for inpainting.\n   */\n  inpaint_image_url: string | Blob | File;\n  /**\n   * The image to use as a mask for the generated image.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * The mode to use for inpainting. Default value: `\"Inpaint or Outpaint (default)\"`\n   */\n  inpaint_mode?:\n    | \"Inpaint or Outpaint (default)\"\n    | \"Improve Detail (face, hand, eyes, etc.)\"\n    | \"Modify Content (add objects, change background, etc.)\";\n  /**\n   * Describe what you want to inpaint. Default value: `\"\"`\n   */\n  inpaint_additional_prompt?: string;\n  /**\n   * The directions to outpaint. Default value: ``\n   */\n  outpaint_selections?: Array<\"Left\" | \"Right\" | \"Top\" | \"Bottom\">;\n  /**\n   * If set to true, the advanced inpaint options ('inpaint_disable_initial_latent',\n   * 'inpaint_engine', 'inpaint_strength', 'inpaint_respective_field',\n   * 'inpaint_erode_or_dilate') will be overridden.\n   * Otherwise, the default values will be used.\n   */\n  override_inpaint_options?: boolean;\n  /**\n   * If set to true, the initial preprocessing will be disabled.\n   */\n  inpaint_disable_initial_latent?: boolean;\n  /**\n   * Version of Fooocus inpaint model Default value: `\"v2.6\"`\n   */\n  inpaint_engine?: \"None\" | \"v1\" | \"v2.5\" | \"v2.6\";\n  /**\n   * Same as the denoising strength in A1111 inpaint. Only used in inpaint, not\n   * used in outpaint. (Outpaint always use 1.0) Default value: `1`\n   */\n  inpaint_strength?: number;\n  /**\n   * The area to inpaint. Value 0 is same as \"Only Masked\" in A1111. Value 1 is\n   * same as \"Whole Image\" in A1111. Only used in inpaint, not used in outpaint.\n   * (Outpaint always use 1.0) Default value: `0.618`\n   */\n  inpaint_respective_field?: number;\n  /**\n   * Positive value will make white area in the mask larger, negative value will\n   * make white area smaller. (default is 0, always process before any mask\n   * invert)\n   */\n  inpaint_erode_or_dilate?: number;\n  /**\n   * If set to true, the mask will be inverted.\n   */\n  invert_mask?: boolean;\n  /**\n   *\n   */\n  image_prompt_1?: ImagePrompt;\n  /**\n   *\n   */\n  image_prompt_2?: ImagePrompt;\n  /**\n   *\n   */\n  image_prompt_3?: ImagePrompt;\n  /**\n   *\n   */\n  image_prompt_4?: ImagePrompt;\n  /**\n   * Mixing Image Prompt and Inpaint\n   */\n  mixing_image_prompt_and_inpaint?: boolean;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FooocusInpaintOutput = {\n  /**\n   * The generated image file info.\n   */\n  images: Array<Image>;\n  /**\n   * The time taken for the generation process.\n   */\n  timings: any;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n};\nexport type FooocusInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style to use. Default value: `Fooocus Enhance,Fooocus V2,Fooocus Sharp`\n   */\n  styles?: Array<\n    | \"Fooocus V2\"\n    | \"Fooocus Enhance\"\n    | \"Fooocus Sharp\"\n    | \"Fooocus Semi Realistic\"\n    | \"Fooocus Masterpiece\"\n    | \"Fooocus Photograph\"\n    | \"Fooocus Negative\"\n    | \"Fooocus Cinematic\"\n    | \"SAI 3D Model\"\n    | \"SAI Analog Film\"\n    | \"SAI Anime\"\n    | \"SAI Cinematic\"\n    | \"SAI Comic Book\"\n    | \"SAI Craft Clay\"\n    | \"SAI Digital Art\"\n    | \"SAI Enhance\"\n    | \"SAI Fantasy Art\"\n    | \"SAI Isometric\"\n    | \"SAI Line Art\"\n    | \"SAI Lowpoly\"\n    | \"SAI Neonpunk\"\n    | \"SAI Origami\"\n    | \"SAI Photographic\"\n    | \"SAI Pixel Art\"\n    | \"SAI Texture\"\n    | \"MRE Cinematic Dynamic\"\n    | \"MRE Spontaneous Picture\"\n    | \"MRE Artistic Vision\"\n    | \"MRE Dark Dream\"\n    | \"MRE Gloomy Art\"\n    | \"MRE Bad Dream\"\n    | \"MRE Underground\"\n    | \"MRE Surreal Painting\"\n    | \"MRE Dynamic Illustration\"\n    | \"MRE Undead Art\"\n    | \"MRE Elemental Art\"\n    | \"MRE Space Art\"\n    | \"MRE Ancient Illustration\"\n    | \"MRE Brave Art\"\n    | \"MRE Heroic Fantasy\"\n    | \"MRE Dark Cyberpunk\"\n    | \"MRE Lyrical Geometry\"\n    | \"MRE Sumi E Symbolic\"\n    | \"MRE Sumi E Detailed\"\n    | \"MRE Manga\"\n    | \"MRE Anime\"\n    | \"MRE Comic\"\n    | \"Ads Advertising\"\n    | \"Ads Automotive\"\n    | \"Ads Corporate\"\n    | \"Ads Fashion Editorial\"\n    | \"Ads Food Photography\"\n    | \"Ads Gourmet Food Photography\"\n    | \"Ads Luxury\"\n    | \"Ads Real Estate\"\n    | \"Ads Retail\"\n    | \"Artstyle Abstract\"\n    | \"Artstyle Abstract Expressionism\"\n    | \"Artstyle Art Deco\"\n    | \"Artstyle Art Nouveau\"\n    | \"Artstyle Constructivist\"\n    | \"Artstyle Cubist\"\n    | \"Artstyle Expressionist\"\n    | \"Artstyle Graffiti\"\n    | \"Artstyle Hyperrealism\"\n    | \"Artstyle Impressionist\"\n    | \"Artstyle Pointillism\"\n    | \"Artstyle Pop Art\"\n    | \"Artstyle Psychedelic\"\n    | \"Artstyle Renaissance\"\n    | \"Artstyle Steampunk\"\n    | \"Artstyle Surrealist\"\n    | \"Artstyle Typography\"\n    | \"Artstyle Watercolor\"\n    | \"Futuristic Biomechanical\"\n    | \"Futuristic Biomechanical Cyberpunk\"\n    | \"Futuristic Cybernetic\"\n    | \"Futuristic Cybernetic Robot\"\n    | \"Futuristic Cyberpunk Cityscape\"\n    | \"Futuristic Futuristic\"\n    | \"Futuristic Retro Cyberpunk\"\n    | \"Futuristic Retro Futurism\"\n    | \"Futuristic Sci Fi\"\n    | \"Futuristic Vaporwave\"\n    | \"Game Bubble Bobble\"\n    | \"Game Cyberpunk Game\"\n    | \"Game Fighting Game\"\n    | \"Game Gta\"\n    | \"Game Mario\"\n    | \"Game Minecraft\"\n    | \"Game Pokemon\"\n    | \"Game Retro Arcade\"\n    | \"Game Retro Game\"\n    | \"Game Rpg Fantasy Game\"\n    | \"Game Strategy Game\"\n    | \"Game Streetfighter\"\n    | \"Game Zelda\"\n    | \"Misc Architectural\"\n    | \"Misc Disco\"\n    | \"Misc Dreamscape\"\n    | \"Misc Dystopian\"\n    | \"Misc Fairy Tale\"\n    | \"Misc Gothic\"\n    | \"Misc Grunge\"\n    | \"Misc Horror\"\n    | \"Misc Kawaii\"\n    | \"Misc Lovecraftian\"\n    | \"Misc Macabre\"\n    | \"Misc Manga\"\n    | \"Misc Metropolis\"\n    | \"Misc Minimalist\"\n    | \"Misc Monochrome\"\n    | \"Misc Nautical\"\n    | \"Misc Space\"\n    | \"Misc Stained Glass\"\n    | \"Misc Techwear Fashion\"\n    | \"Misc Tribal\"\n    | \"Misc Zentangle\"\n    | \"Papercraft Collage\"\n    | \"Papercraft Flat Papercut\"\n    | \"Papercraft Kirigami\"\n    | \"Papercraft Paper Mache\"\n    | \"Papercraft Paper Quilling\"\n    | \"Papercraft Papercut Collage\"\n    | \"Papercraft Papercut Shadow Box\"\n    | \"Papercraft Stacked Papercut\"\n    | \"Papercraft Thick Layered Papercut\"\n    | \"Photo Alien\"\n    | \"Photo Film Noir\"\n    | \"Photo Glamour\"\n    | \"Photo Hdr\"\n    | \"Photo Iphone Photographic\"\n    | \"Photo Long Exposure\"\n    | \"Photo Neon Noir\"\n    | \"Photo Silhouette\"\n    | \"Photo Tilt Shift\"\n    | \"Cinematic Diva\"\n    | \"Abstract Expressionism\"\n    | \"Academia\"\n    | \"Action Figure\"\n    | \"Adorable 3D Character\"\n    | \"Adorable Kawaii\"\n    | \"Art Deco\"\n    | \"Art Nouveau\"\n    | \"Astral Aura\"\n    | \"Avant Garde\"\n    | \"Baroque\"\n    | \"Bauhaus Style Poster\"\n    | \"Blueprint Schematic Drawing\"\n    | \"Caricature\"\n    | \"Cel Shaded Art\"\n    | \"Character Design Sheet\"\n    | \"Classicism Art\"\n    | \"Color Field Painting\"\n    | \"Colored Pencil Art\"\n    | \"Conceptual Art\"\n    | \"Constructivism\"\n    | \"Cubism\"\n    | \"Dadaism\"\n    | \"Dark Fantasy\"\n    | \"Dark Moody Atmosphere\"\n    | \"Dmt Art Style\"\n    | \"Doodle Art\"\n    | \"Double Exposure\"\n    | \"Dripping Paint Splatter Art\"\n    | \"Expressionism\"\n    | \"Faded Polaroid Photo\"\n    | \"Fauvism\"\n    | \"Flat 2d Art\"\n    | \"Fortnite Art Style\"\n    | \"Futurism\"\n    | \"Glitchcore\"\n    | \"Glo Fi\"\n    | \"Googie Art Style\"\n    | \"Graffiti Art\"\n    | \"Harlem Renaissance Art\"\n    | \"High Fashion\"\n    | \"Idyllic\"\n    | \"Impressionism\"\n    | \"Infographic Drawing\"\n    | \"Ink Dripping Drawing\"\n    | \"Japanese Ink Drawing\"\n    | \"Knolling Photography\"\n    | \"Light Cheery Atmosphere\"\n    | \"Logo Design\"\n    | \"Luxurious Elegance\"\n    | \"Macro Photography\"\n    | \"Mandola Art\"\n    | \"Marker Drawing\"\n    | \"Medievalism\"\n    | \"Minimalism\"\n    | \"Neo Baroque\"\n    | \"Neo Byzantine\"\n    | \"Neo Futurism\"\n    | \"Neo Impressionism\"\n    | \"Neo Rococo\"\n    | \"Neoclassicism\"\n    | \"Op Art\"\n    | \"Ornate And Intricate\"\n    | \"Pencil Sketch Drawing\"\n    | \"Pop Art 2\"\n    | \"Rococo\"\n    | \"Silhouette Art\"\n    | \"Simple Vector Art\"\n    | \"Sketchup\"\n    | \"Steampunk 2\"\n    | \"Surrealism\"\n    | \"Suprematism\"\n    | \"Terragen\"\n    | \"Tranquil Relaxing Atmosphere\"\n    | \"Sticker Designs\"\n    | \"Vibrant Rim Light\"\n    | \"Volumetric Lighting\"\n    | \"Watercolor 2\"\n    | \"Whimsical And Playful\"\n    | \"Mk Chromolithography\"\n    | \"Mk Cross Processing Print\"\n    | \"Mk Dufaycolor Photograph\"\n    | \"Mk Herbarium\"\n    | \"Mk Punk Collage\"\n    | \"Mk Mosaic\"\n    | \"Mk Van Gogh\"\n    | \"Mk Coloring Book\"\n    | \"Mk Singer Sargent\"\n    | \"Mk Pollock\"\n    | \"Mk Basquiat\"\n    | \"Mk Andy Warhol\"\n    | \"Mk Halftone Print\"\n    | \"Mk Gond Painting\"\n    | \"Mk Albumen Print\"\n    | \"Mk Aquatint Print\"\n    | \"Mk Anthotype Print\"\n    | \"Mk Inuit Carving\"\n    | \"Mk Bromoil Print\"\n    | \"Mk Calotype Print\"\n    | \"Mk Color Sketchnote\"\n    | \"Mk Cibulak Porcelain\"\n    | \"Mk Alcohol Ink Art\"\n    | \"Mk One Line Art\"\n    | \"Mk Blacklight Paint\"\n    | \"Mk Carnival Glass\"\n    | \"Mk Cyanotype Print\"\n    | \"Mk Cross Stitching\"\n    | \"Mk Encaustic Paint\"\n    | \"Mk Embroidery\"\n    | \"Mk Gyotaku\"\n    | \"Mk Luminogram\"\n    | \"Mk Lite Brite Art\"\n    | \"Mk Mokume Gane\"\n    | \"Pebble Art\"\n    | \"Mk Palekh\"\n    | \"Mk Suminagashi\"\n    | \"Mk Scrimshaw\"\n    | \"Mk Shibori\"\n    | \"Mk Vitreous Enamel\"\n    | \"Mk Ukiyo E\"\n    | \"Mk Vintage Airline Poster\"\n    | \"Mk Vintage Travel Poster\"\n    | \"Mk Bauhaus Style\"\n    | \"Mk Afrofuturism\"\n    | \"Mk Atompunk\"\n    | \"Mk Constructivism\"\n    | \"Mk Chicano Art\"\n    | \"Mk De Stijl\"\n    | \"Mk Dayak Art\"\n    | \"Mk Fayum Portrait\"\n    | \"Mk Illuminated Manuscript\"\n    | \"Mk Kalighat Painting\"\n    | \"Mk Madhubani Painting\"\n    | \"Mk Pictorialism\"\n    | \"Mk Pichwai Painting\"\n    | \"Mk Patachitra Painting\"\n    | \"Mk Samoan Art Inspired\"\n    | \"Mk Tlingit Art\"\n    | \"Mk Adnate Style\"\n    | \"Mk Ron English Style\"\n    | \"Mk Shepard Fairey Style\"\n  >;\n  /**\n   * You can choose Speed or Quality Default value: `\"Extreme Speed\"`\n   */\n  performance?: \"Speed\" | \"Quality\" | \"Extreme Speed\" | \"Lightning\";\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The sharpness of the generated image. Use it to control how sharp the generated\n   * image should be. Higher value means image and texture are sharper. Default value: `2`\n   */\n  sharpness?: number;\n  /**\n   * The size of the generated image. You can choose between some presets or\n   * custom height and width that **must be multiples of 8**. Default value: `\"1024x1024\"`\n   */\n  aspect_ratio?: string;\n  /**\n   * Number of images to generate in one request Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use up to 5 LoRAs\n   * and they will be merged together to generate the final image. Default value: `[object Object]`\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * Refiner (SDXL or SD 1.5) Default value: `\"None\"`\n   */\n  refiner_model?: \"None\" | \"realisticVisionV60B1_v51VAE.safetensors\";\n  /**\n   * Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models\n   * 0.8 for XL-refiners; or any value for switching two SDXL models. Default value: `0.8`\n   */\n  refiner_switch?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"png\" | \"jpeg\" | \"webp\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The image to use as a reference for the generated image.\n   */\n  control_image_url?: string | Blob | File;\n  /**\n   * The type of image control Default value: `\"PyraCanny\"`\n   */\n  control_type?: \"ImagePrompt\" | \"PyraCanny\" | \"CPDS\" | \"FaceSwap\";\n  /**\n   * The strength of the control image. Use it to control how much the generated image\n   * should look like the control image. Default value: `1`\n   */\n  control_image_weight?: number;\n  /**\n   * The stop at value of the control image. Use it to control how much the generated image\n   * should look like the control image. Default value: `1`\n   */\n  control_image_stop_at?: number;\n  /**\n   * The image to use as a reference for inpainting.\n   */\n  inpaint_image_url?: string | Blob | File;\n  /**\n   * The image to use as a mask for the generated image.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   *\n   */\n  mixing_image_prompt_and_inpaint?: boolean;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FooocusLegacyInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style to use. Default value: `Fooocus Enhance,Fooocus V2,Fooocus Sharp`\n   */\n  styles?: Array<\n    | \"Fooocus V2\"\n    | \"Fooocus Enhance\"\n    | \"Fooocus Sharp\"\n    | \"Fooocus Semi Realistic\"\n    | \"Fooocus Masterpiece\"\n    | \"Fooocus Photograph\"\n    | \"Fooocus Negative\"\n    | \"Fooocus Cinematic\"\n    | \"SAI 3D Model\"\n    | \"SAI Analog Film\"\n    | \"SAI Anime\"\n    | \"SAI Cinematic\"\n    | \"SAI Comic Book\"\n    | \"SAI Craft Clay\"\n    | \"SAI Digital Art\"\n    | \"SAI Enhance\"\n    | \"SAI Fantasy Art\"\n    | \"SAI Isometric\"\n    | \"SAI Line Art\"\n    | \"SAI Lowpoly\"\n    | \"SAI Neonpunk\"\n    | \"SAI Origami\"\n    | \"SAI Photographic\"\n    | \"SAI Pixel Art\"\n    | \"SAI Texture\"\n    | \"MRE Cinematic Dynamic\"\n    | \"MRE Spontaneous Picture\"\n    | \"MRE Artistic Vision\"\n    | \"MRE Dark Dream\"\n    | \"MRE Gloomy Art\"\n    | \"MRE Bad Dream\"\n    | \"MRE Underground\"\n    | \"MRE Surreal Painting\"\n    | \"MRE Dynamic Illustration\"\n    | \"MRE Undead Art\"\n    | \"MRE Elemental Art\"\n    | \"MRE Space Art\"\n    | \"MRE Ancient Illustration\"\n    | \"MRE Brave Art\"\n    | \"MRE Heroic Fantasy\"\n    | \"MRE Dark Cyberpunk\"\n    | \"MRE Lyrical Geometry\"\n    | \"MRE Sumi E Symbolic\"\n    | \"MRE Sumi E Detailed\"\n    | \"MRE Manga\"\n    | \"MRE Anime\"\n    | \"MRE Comic\"\n    | \"Ads Advertising\"\n    | \"Ads Automotive\"\n    | \"Ads Corporate\"\n    | \"Ads Fashion Editorial\"\n    | \"Ads Food Photography\"\n    | \"Ads Gourmet Food Photography\"\n    | \"Ads Luxury\"\n    | \"Ads Real Estate\"\n    | \"Ads Retail\"\n    | \"Artstyle Abstract\"\n    | \"Artstyle Abstract Expressionism\"\n    | \"Artstyle Art Deco\"\n    | \"Artstyle Art Nouveau\"\n    | \"Artstyle Constructivist\"\n    | \"Artstyle Cubist\"\n    | \"Artstyle Expressionist\"\n    | \"Artstyle Graffiti\"\n    | \"Artstyle Hyperrealism\"\n    | \"Artstyle Impressionist\"\n    | \"Artstyle Pointillism\"\n    | \"Artstyle Pop Art\"\n    | \"Artstyle Psychedelic\"\n    | \"Artstyle Renaissance\"\n    | \"Artstyle Steampunk\"\n    | \"Artstyle Surrealist\"\n    | \"Artstyle Typography\"\n    | \"Artstyle Watercolor\"\n    | \"Futuristic Biomechanical\"\n    | \"Futuristic Biomechanical Cyberpunk\"\n    | \"Futuristic Cybernetic\"\n    | \"Futuristic Cybernetic Robot\"\n    | \"Futuristic Cyberpunk Cityscape\"\n    | \"Futuristic Futuristic\"\n    | \"Futuristic Retro Cyberpunk\"\n    | \"Futuristic Retro Futurism\"\n    | \"Futuristic Sci Fi\"\n    | \"Futuristic Vaporwave\"\n    | \"Game Bubble Bobble\"\n    | \"Game Cyberpunk Game\"\n    | \"Game Fighting Game\"\n    | \"Game Gta\"\n    | \"Game Mario\"\n    | \"Game Minecraft\"\n    | \"Game Pokemon\"\n    | \"Game Retro Arcade\"\n    | \"Game Retro Game\"\n    | \"Game Rpg Fantasy Game\"\n    | \"Game Strategy Game\"\n    | \"Game Streetfighter\"\n    | \"Game Zelda\"\n    | \"Misc Architectural\"\n    | \"Misc Disco\"\n    | \"Misc Dreamscape\"\n    | \"Misc Dystopian\"\n    | \"Misc Fairy Tale\"\n    | \"Misc Gothic\"\n    | \"Misc Grunge\"\n    | \"Misc Horror\"\n    | \"Misc Kawaii\"\n    | \"Misc Lovecraftian\"\n    | \"Misc Macabre\"\n    | \"Misc Manga\"\n    | \"Misc Metropolis\"\n    | \"Misc Minimalist\"\n    | \"Misc Monochrome\"\n    | \"Misc Nautical\"\n    | \"Misc Space\"\n    | \"Misc Stained Glass\"\n    | \"Misc Techwear Fashion\"\n    | \"Misc Tribal\"\n    | \"Misc Zentangle\"\n    | \"Papercraft Collage\"\n    | \"Papercraft Flat Papercut\"\n    | \"Papercraft Kirigami\"\n    | \"Papercraft Paper Mache\"\n    | \"Papercraft Paper Quilling\"\n    | \"Papercraft Papercut Collage\"\n    | \"Papercraft Papercut Shadow Box\"\n    | \"Papercraft Stacked Papercut\"\n    | \"Papercraft Thick Layered Papercut\"\n    | \"Photo Alien\"\n    | \"Photo Film Noir\"\n    | \"Photo Glamour\"\n    | \"Photo Hdr\"\n    | \"Photo Iphone Photographic\"\n    | \"Photo Long Exposure\"\n    | \"Photo Neon Noir\"\n    | \"Photo Silhouette\"\n    | \"Photo Tilt Shift\"\n    | \"Cinematic Diva\"\n    | \"Abstract Expressionism\"\n    | \"Academia\"\n    | \"Action Figure\"\n    | \"Adorable 3D Character\"\n    | \"Adorable Kawaii\"\n    | \"Art Deco\"\n    | \"Art Nouveau\"\n    | \"Astral Aura\"\n    | \"Avant Garde\"\n    | \"Baroque\"\n    | \"Bauhaus Style Poster\"\n    | \"Blueprint Schematic Drawing\"\n    | \"Caricature\"\n    | \"Cel Shaded Art\"\n    | \"Character Design Sheet\"\n    | \"Classicism Art\"\n    | \"Color Field Painting\"\n    | \"Colored Pencil Art\"\n    | \"Conceptual Art\"\n    | \"Constructivism\"\n    | \"Cubism\"\n    | \"Dadaism\"\n    | \"Dark Fantasy\"\n    | \"Dark Moody Atmosphere\"\n    | \"Dmt Art Style\"\n    | \"Doodle Art\"\n    | \"Double Exposure\"\n    | \"Dripping Paint Splatter Art\"\n    | \"Expressionism\"\n    | \"Faded Polaroid Photo\"\n    | \"Fauvism\"\n    | \"Flat 2d Art\"\n    | \"Fortnite Art Style\"\n    | \"Futurism\"\n    | \"Glitchcore\"\n    | \"Glo Fi\"\n    | \"Googie Art Style\"\n    | \"Graffiti Art\"\n    | \"Harlem Renaissance Art\"\n    | \"High Fashion\"\n    | \"Idyllic\"\n    | \"Impressionism\"\n    | \"Infographic Drawing\"\n    | \"Ink Dripping Drawing\"\n    | \"Japanese Ink Drawing\"\n    | \"Knolling Photography\"\n    | \"Light Cheery Atmosphere\"\n    | \"Logo Design\"\n    | \"Luxurious Elegance\"\n    | \"Macro Photography\"\n    | \"Mandola Art\"\n    | \"Marker Drawing\"\n    | \"Medievalism\"\n    | \"Minimalism\"\n    | \"Neo Baroque\"\n    | \"Neo Byzantine\"\n    | \"Neo Futurism\"\n    | \"Neo Impressionism\"\n    | \"Neo Rococo\"\n    | \"Neoclassicism\"\n    | \"Op Art\"\n    | \"Ornate And Intricate\"\n    | \"Pencil Sketch Drawing\"\n    | \"Pop Art 2\"\n    | \"Rococo\"\n    | \"Silhouette Art\"\n    | \"Simple Vector Art\"\n    | \"Sketchup\"\n    | \"Steampunk 2\"\n    | \"Surrealism\"\n    | \"Suprematism\"\n    | \"Terragen\"\n    | \"Tranquil Relaxing Atmosphere\"\n    | \"Sticker Designs\"\n    | \"Vibrant Rim Light\"\n    | \"Volumetric Lighting\"\n    | \"Watercolor 2\"\n    | \"Whimsical And Playful\"\n    | \"Mk Chromolithography\"\n    | \"Mk Cross Processing Print\"\n    | \"Mk Dufaycolor Photograph\"\n    | \"Mk Herbarium\"\n    | \"Mk Punk Collage\"\n    | \"Mk Mosaic\"\n    | \"Mk Van Gogh\"\n    | \"Mk Coloring Book\"\n    | \"Mk Singer Sargent\"\n    | \"Mk Pollock\"\n    | \"Mk Basquiat\"\n    | \"Mk Andy Warhol\"\n    | \"Mk Halftone Print\"\n    | \"Mk Gond Painting\"\n    | \"Mk Albumen Print\"\n    | \"Mk Aquatint Print\"\n    | \"Mk Anthotype Print\"\n    | \"Mk Inuit Carving\"\n    | \"Mk Bromoil Print\"\n    | \"Mk Calotype Print\"\n    | \"Mk Color Sketchnote\"\n    | \"Mk Cibulak Porcelain\"\n    | \"Mk Alcohol Ink Art\"\n    | \"Mk One Line Art\"\n    | \"Mk Blacklight Paint\"\n    | \"Mk Carnival Glass\"\n    | \"Mk Cyanotype Print\"\n    | \"Mk Cross Stitching\"\n    | \"Mk Encaustic Paint\"\n    | \"Mk Embroidery\"\n    | \"Mk Gyotaku\"\n    | \"Mk Luminogram\"\n    | \"Mk Lite Brite Art\"\n    | \"Mk Mokume Gane\"\n    | \"Pebble Art\"\n    | \"Mk Palekh\"\n    | \"Mk Suminagashi\"\n    | \"Mk Scrimshaw\"\n    | \"Mk Shibori\"\n    | \"Mk Vitreous Enamel\"\n    | \"Mk Ukiyo E\"\n    | \"Mk Vintage Airline Poster\"\n    | \"Mk Vintage Travel Poster\"\n    | \"Mk Bauhaus Style\"\n    | \"Mk Afrofuturism\"\n    | \"Mk Atompunk\"\n    | \"Mk Constructivism\"\n    | \"Mk Chicano Art\"\n    | \"Mk De Stijl\"\n    | \"Mk Dayak Art\"\n    | \"Mk Fayum Portrait\"\n    | \"Mk Illuminated Manuscript\"\n    | \"Mk Kalighat Painting\"\n    | \"Mk Madhubani Painting\"\n    | \"Mk Pictorialism\"\n    | \"Mk Pichwai Painting\"\n    | \"Mk Patachitra Painting\"\n    | \"Mk Samoan Art Inspired\"\n    | \"Mk Tlingit Art\"\n    | \"Mk Adnate Style\"\n    | \"Mk Ron English Style\"\n    | \"Mk Shepard Fairey Style\"\n  >;\n  /**\n   * You can choose Speed or Quality Default value: `\"Extreme Speed\"`\n   */\n  performance?: \"Speed\" | \"Quality\" | \"Extreme Speed\" | \"Lightning\";\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The sharpness of the generated image. Use it to control how sharp the generated\n   * image should be. Higher value means image and texture are sharper. Default value: `2`\n   */\n  sharpness?: number;\n  /**\n   * The size of the generated image. You can choose between some presets or\n   * custom height and width that **must be multiples of 8**. Default value: `\"1024x1024\"`\n   */\n  aspect_ratio?: string;\n  /**\n   * Number of images to generate in one request Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use up to 5 LoRAs\n   * and they will be merged together to generate the final image. Default value: `[object Object]`\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * Refiner (SDXL or SD 1.5) Default value: `\"None\"`\n   */\n  refiner_model?: \"None\" | \"realisticVisionV60B1_v51VAE.safetensors\";\n  /**\n   * Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models\n   * 0.8 for XL-refiners; or any value for switching two SDXL models. Default value: `0.8`\n   */\n  refiner_switch?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"png\" | \"jpeg\" | \"webp\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The image to use as a reference for the generated image.\n   */\n  control_image_url?: string | Blob | File;\n  /**\n   * The type of image control Default value: `\"PyraCanny\"`\n   */\n  control_type?: \"ImagePrompt\" | \"PyraCanny\" | \"CPDS\" | \"FaceSwap\";\n  /**\n   * The strength of the control image. Use it to control how much the generated image\n   * should look like the control image. Default value: `1`\n   */\n  control_image_weight?: number;\n  /**\n   * The stop at value of the control image. Use it to control how much the generated image\n   * should look like the control image. Default value: `1`\n   */\n  control_image_stop_at?: number;\n  /**\n   * The image to use as a reference for inpainting.\n   */\n  inpaint_image_url?: string | Blob | File;\n  /**\n   * The image to use as a mask for the generated image.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   *\n   */\n  mixing_image_prompt_and_inpaint?: boolean;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FooocusOutput = {\n  /**\n   * The generated image file info.\n   */\n  images: Array<Image>;\n  /**\n   * The time taken for the generation process.\n   */\n  timings: any;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n};\nexport type FooocusUpscaleOrVaryInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style to use. Default value: `Fooocus Enhance,Fooocus V2,Fooocus Sharp`\n   */\n  styles?: Array<\n    | \"Fooocus V2\"\n    | \"Fooocus Enhance\"\n    | \"Fooocus Sharp\"\n    | \"Fooocus Semi Realistic\"\n    | \"Fooocus Masterpiece\"\n    | \"Fooocus Photograph\"\n    | \"Fooocus Negative\"\n    | \"Fooocus Cinematic\"\n    | \"SAI 3D Model\"\n    | \"SAI Analog Film\"\n    | \"SAI Anime\"\n    | \"SAI Cinematic\"\n    | \"SAI Comic Book\"\n    | \"SAI Craft Clay\"\n    | \"SAI Digital Art\"\n    | \"SAI Enhance\"\n    | \"SAI Fantasy Art\"\n    | \"SAI Isometric\"\n    | \"SAI Line Art\"\n    | \"SAI Lowpoly\"\n    | \"SAI Neonpunk\"\n    | \"SAI Origami\"\n    | \"SAI Photographic\"\n    | \"SAI Pixel Art\"\n    | \"SAI Texture\"\n    | \"MRE Cinematic Dynamic\"\n    | \"MRE Spontaneous Picture\"\n    | \"MRE Artistic Vision\"\n    | \"MRE Dark Dream\"\n    | \"MRE Gloomy Art\"\n    | \"MRE Bad Dream\"\n    | \"MRE Underground\"\n    | \"MRE Surreal Painting\"\n    | \"MRE Dynamic Illustration\"\n    | \"MRE Undead Art\"\n    | \"MRE Elemental Art\"\n    | \"MRE Space Art\"\n    | \"MRE Ancient Illustration\"\n    | \"MRE Brave Art\"\n    | \"MRE Heroic Fantasy\"\n    | \"MRE Dark Cyberpunk\"\n    | \"MRE Lyrical Geometry\"\n    | \"MRE Sumi E Symbolic\"\n    | \"MRE Sumi E Detailed\"\n    | \"MRE Manga\"\n    | \"MRE Anime\"\n    | \"MRE Comic\"\n    | \"Ads Advertising\"\n    | \"Ads Automotive\"\n    | \"Ads Corporate\"\n    | \"Ads Fashion Editorial\"\n    | \"Ads Food Photography\"\n    | \"Ads Gourmet Food Photography\"\n    | \"Ads Luxury\"\n    | \"Ads Real Estate\"\n    | \"Ads Retail\"\n    | \"Artstyle Abstract\"\n    | \"Artstyle Abstract Expressionism\"\n    | \"Artstyle Art Deco\"\n    | \"Artstyle Art Nouveau\"\n    | \"Artstyle Constructivist\"\n    | \"Artstyle Cubist\"\n    | \"Artstyle Expressionist\"\n    | \"Artstyle Graffiti\"\n    | \"Artstyle Hyperrealism\"\n    | \"Artstyle Impressionist\"\n    | \"Artstyle Pointillism\"\n    | \"Artstyle Pop Art\"\n    | \"Artstyle Psychedelic\"\n    | \"Artstyle Renaissance\"\n    | \"Artstyle Steampunk\"\n    | \"Artstyle Surrealist\"\n    | \"Artstyle Typography\"\n    | \"Artstyle Watercolor\"\n    | \"Futuristic Biomechanical\"\n    | \"Futuristic Biomechanical Cyberpunk\"\n    | \"Futuristic Cybernetic\"\n    | \"Futuristic Cybernetic Robot\"\n    | \"Futuristic Cyberpunk Cityscape\"\n    | \"Futuristic Futuristic\"\n    | \"Futuristic Retro Cyberpunk\"\n    | \"Futuristic Retro Futurism\"\n    | \"Futuristic Sci Fi\"\n    | \"Futuristic Vaporwave\"\n    | \"Game Bubble Bobble\"\n    | \"Game Cyberpunk Game\"\n    | \"Game Fighting Game\"\n    | \"Game Gta\"\n    | \"Game Mario\"\n    | \"Game Minecraft\"\n    | \"Game Pokemon\"\n    | \"Game Retro Arcade\"\n    | \"Game Retro Game\"\n    | \"Game Rpg Fantasy Game\"\n    | \"Game Strategy Game\"\n    | \"Game Streetfighter\"\n    | \"Game Zelda\"\n    | \"Misc Architectural\"\n    | \"Misc Disco\"\n    | \"Misc Dreamscape\"\n    | \"Misc Dystopian\"\n    | \"Misc Fairy Tale\"\n    | \"Misc Gothic\"\n    | \"Misc Grunge\"\n    | \"Misc Horror\"\n    | \"Misc Kawaii\"\n    | \"Misc Lovecraftian\"\n    | \"Misc Macabre\"\n    | \"Misc Manga\"\n    | \"Misc Metropolis\"\n    | \"Misc Minimalist\"\n    | \"Misc Monochrome\"\n    | \"Misc Nautical\"\n    | \"Misc Space\"\n    | \"Misc Stained Glass\"\n    | \"Misc Techwear Fashion\"\n    | \"Misc Tribal\"\n    | \"Misc Zentangle\"\n    | \"Papercraft Collage\"\n    | \"Papercraft Flat Papercut\"\n    | \"Papercraft Kirigami\"\n    | \"Papercraft Paper Mache\"\n    | \"Papercraft Paper Quilling\"\n    | \"Papercraft Papercut Collage\"\n    | \"Papercraft Papercut Shadow Box\"\n    | \"Papercraft Stacked Papercut\"\n    | \"Papercraft Thick Layered Papercut\"\n    | \"Photo Alien\"\n    | \"Photo Film Noir\"\n    | \"Photo Glamour\"\n    | \"Photo Hdr\"\n    | \"Photo Iphone Photographic\"\n    | \"Photo Long Exposure\"\n    | \"Photo Neon Noir\"\n    | \"Photo Silhouette\"\n    | \"Photo Tilt Shift\"\n    | \"Cinematic Diva\"\n    | \"Abstract Expressionism\"\n    | \"Academia\"\n    | \"Action Figure\"\n    | \"Adorable 3D Character\"\n    | \"Adorable Kawaii\"\n    | \"Art Deco\"\n    | \"Art Nouveau\"\n    | \"Astral Aura\"\n    | \"Avant Garde\"\n    | \"Baroque\"\n    | \"Bauhaus Style Poster\"\n    | \"Blueprint Schematic Drawing\"\n    | \"Caricature\"\n    | \"Cel Shaded Art\"\n    | \"Character Design Sheet\"\n    | \"Classicism Art\"\n    | \"Color Field Painting\"\n    | \"Colored Pencil Art\"\n    | \"Conceptual Art\"\n    | \"Constructivism\"\n    | \"Cubism\"\n    | \"Dadaism\"\n    | \"Dark Fantasy\"\n    | \"Dark Moody Atmosphere\"\n    | \"Dmt Art Style\"\n    | \"Doodle Art\"\n    | \"Double Exposure\"\n    | \"Dripping Paint Splatter Art\"\n    | \"Expressionism\"\n    | \"Faded Polaroid Photo\"\n    | \"Fauvism\"\n    | \"Flat 2d Art\"\n    | \"Fortnite Art Style\"\n    | \"Futurism\"\n    | \"Glitchcore\"\n    | \"Glo Fi\"\n    | \"Googie Art Style\"\n    | \"Graffiti Art\"\n    | \"Harlem Renaissance Art\"\n    | \"High Fashion\"\n    | \"Idyllic\"\n    | \"Impressionism\"\n    | \"Infographic Drawing\"\n    | \"Ink Dripping Drawing\"\n    | \"Japanese Ink Drawing\"\n    | \"Knolling Photography\"\n    | \"Light Cheery Atmosphere\"\n    | \"Logo Design\"\n    | \"Luxurious Elegance\"\n    | \"Macro Photography\"\n    | \"Mandola Art\"\n    | \"Marker Drawing\"\n    | \"Medievalism\"\n    | \"Minimalism\"\n    | \"Neo Baroque\"\n    | \"Neo Byzantine\"\n    | \"Neo Futurism\"\n    | \"Neo Impressionism\"\n    | \"Neo Rococo\"\n    | \"Neoclassicism\"\n    | \"Op Art\"\n    | \"Ornate And Intricate\"\n    | \"Pencil Sketch Drawing\"\n    | \"Pop Art 2\"\n    | \"Rococo\"\n    | \"Silhouette Art\"\n    | \"Simple Vector Art\"\n    | \"Sketchup\"\n    | \"Steampunk 2\"\n    | \"Surrealism\"\n    | \"Suprematism\"\n    | \"Terragen\"\n    | \"Tranquil Relaxing Atmosphere\"\n    | \"Sticker Designs\"\n    | \"Vibrant Rim Light\"\n    | \"Volumetric Lighting\"\n    | \"Watercolor 2\"\n    | \"Whimsical And Playful\"\n    | \"Mk Chromolithography\"\n    | \"Mk Cross Processing Print\"\n    | \"Mk Dufaycolor Photograph\"\n    | \"Mk Herbarium\"\n    | \"Mk Punk Collage\"\n    | \"Mk Mosaic\"\n    | \"Mk Van Gogh\"\n    | \"Mk Coloring Book\"\n    | \"Mk Singer Sargent\"\n    | \"Mk Pollock\"\n    | \"Mk Basquiat\"\n    | \"Mk Andy Warhol\"\n    | \"Mk Halftone Print\"\n    | \"Mk Gond Painting\"\n    | \"Mk Albumen Print\"\n    | \"Mk Aquatint Print\"\n    | \"Mk Anthotype Print\"\n    | \"Mk Inuit Carving\"\n    | \"Mk Bromoil Print\"\n    | \"Mk Calotype Print\"\n    | \"Mk Color Sketchnote\"\n    | \"Mk Cibulak Porcelain\"\n    | \"Mk Alcohol Ink Art\"\n    | \"Mk One Line Art\"\n    | \"Mk Blacklight Paint\"\n    | \"Mk Carnival Glass\"\n    | \"Mk Cyanotype Print\"\n    | \"Mk Cross Stitching\"\n    | \"Mk Encaustic Paint\"\n    | \"Mk Embroidery\"\n    | \"Mk Gyotaku\"\n    | \"Mk Luminogram\"\n    | \"Mk Lite Brite Art\"\n    | \"Mk Mokume Gane\"\n    | \"Pebble Art\"\n    | \"Mk Palekh\"\n    | \"Mk Suminagashi\"\n    | \"Mk Scrimshaw\"\n    | \"Mk Shibori\"\n    | \"Mk Vitreous Enamel\"\n    | \"Mk Ukiyo E\"\n    | \"Mk Vintage Airline Poster\"\n    | \"Mk Vintage Travel Poster\"\n    | \"Mk Bauhaus Style\"\n    | \"Mk Afrofuturism\"\n    | \"Mk Atompunk\"\n    | \"Mk Constructivism\"\n    | \"Mk Chicano Art\"\n    | \"Mk De Stijl\"\n    | \"Mk Dayak Art\"\n    | \"Mk Fayum Portrait\"\n    | \"Mk Illuminated Manuscript\"\n    | \"Mk Kalighat Painting\"\n    | \"Mk Madhubani Painting\"\n    | \"Mk Pictorialism\"\n    | \"Mk Pichwai Painting\"\n    | \"Mk Patachitra Painting\"\n    | \"Mk Samoan Art Inspired\"\n    | \"Mk Tlingit Art\"\n    | \"Mk Adnate Style\"\n    | \"Mk Ron English Style\"\n    | \"Mk Shepard Fairey Style\"\n  >;\n  /**\n   * You can choose Speed or Quality Default value: `\"Extreme Speed\"`\n   */\n  performance?: \"Speed\" | \"Quality\" | \"Extreme Speed\" | \"Lightning\";\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The sharpness of the generated image. Use it to control how sharp the generated\n   * image should be. Higher value means image and texture are sharper. Default value: `2`\n   */\n  sharpness?: number;\n  /**\n   * The size of the generated image. You can choose between some presets or\n   * custom height and width that **must be multiples of 8**. Default value: `\"1024x1024\"`\n   */\n  aspect_ratio?: string;\n  /**\n   * Number of images to generate in one request Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use up to 5 LoRAs\n   * and they will be merged together to generate the final image. Default value: `[object Object]`\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * Refiner (SDXL or SD 1.5) Default value: `\"None\"`\n   */\n  refiner_model?: \"None\" | \"realisticVisionV60B1_v51VAE.safetensors\";\n  /**\n   * Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models\n   * 0.8 for XL-refiners; or any value for switching two SDXL models. Default value: `0.8`\n   */\n  refiner_switch?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"png\" | \"jpeg\" | \"webp\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The image to upscale or vary.\n   */\n  uov_image_url: string | Blob | File;\n  /**\n   * The method to use for upscaling or varying. Default value: `\"Vary (Strong)\"`\n   */\n  uov_method?:\n    | \"Disabled\"\n    | \"Vary (Subtle)\"\n    | \"Vary (Strong)\"\n    | \"Upscale (1.5x)\"\n    | \"Upscale (2x)\"\n    | \"Upscale (Fast 2x)\";\n  /**\n   *\n   */\n  image_prompt_1?: ImagePrompt;\n  /**\n   *\n   */\n  image_prompt_2?: ImagePrompt;\n  /**\n   *\n   */\n  image_prompt_3?: ImagePrompt;\n  /**\n   *\n   */\n  image_prompt_4?: ImagePrompt;\n  /**\n   * Mixing Image Prompt and Vary/Upscale\n   */\n  mixing_image_prompt_and_vary_upscale?: boolean;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FooocusUpscaleOrVaryOutput = {\n  /**\n   * The generated image file info.\n   */\n  images: Array<Image>;\n  /**\n   * The time taken for the generation process.\n   */\n  timings: any;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n};\nexport type FrenchOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type GeminiFlashEditInput = {\n  /**\n   * The prompt for image generation or editing\n   */\n  prompt: string;\n  /**\n   * Optional URL of an input image for editing. If not provided, generates a new image.\n   */\n  image_url: string | Blob | File;\n};\nexport type GeminiFlashEditOutput = {\n  /**\n   * The generated or edited image\n   */\n  image: Image;\n  /**\n   * Text description or response from Gemini\n   */\n  description: string;\n};\nexport type GenFillInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the binary mask image that represents the area that will be cleaned.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of Images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type GenFillOutput = {\n  /**\n   * Generated Images\n   */\n  images: Array<Image>;\n};\nexport type GotOcrV2Input = {\n  /**\n   * URL of images. Default value: ``\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * Generate the output in formatted mode.\n   */\n  do_format?: boolean;\n  /**\n   * Use provided images to generate a single output.\n   */\n  multi_page?: boolean;\n};\nexport type GotOcrV2Output = {\n  /**\n   * Generated output\n   */\n  outputs: Array<string>;\n};\nexport type GrowMaskInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of pixels to grow the mask. Default value: `5`\n   */\n  pixels?: number;\n  /**\n   * The threshold to convert the image to a mask. 0-255. Default value: `128`\n   */\n  threshold?: number;\n};\nexport type GrowMaskOutput = {\n  /**\n   * The mask\n   */\n  image: Image;\n};\nexport type GuidanceInput = {\n  /**\n   * The image that should be used as guidance, in base64 format, with the method defined in guidance_method_1. Accepted formats are jpeg, jpg, png, webp. Maximum file size 12MB. If more then one guidance method is used, all guidance images must be of the same aspect ratio, and this will be the aspect ratio of the generated results. If guidance_method_1 is selected, an image must be provided.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Which guidance type you would like to include in the generation. Up to 4 guidance methods can be combined during a single inference. This parameter is optional.\n   */\n  method?:\n    | \"controlnet_canny\"\n    | \"controlnet_depth\"\n    | \"controlnet_recoloring\"\n    | \"controlnet_color_grid\";\n  /**\n   * Impact of the guidance. Default value: `1`\n   */\n  scale?: number;\n};\nexport type HEDInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the safe version of the HED detector\n   */\n  safe?: boolean;\n  /**\n   * Whether to use the scribble version of the HED detector\n   */\n  scribble?: boolean;\n};\nexport type HEDOutput = {\n  /**\n   * Image with lines detected using the HED detector\n   */\n  image: Image;\n};\nexport type HindiOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type HunyuanVideoImageToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * URL of the image input.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. Default value: `\"720p\"`\n   */\n  resolution?: \"720p\";\n  /**\n   * The number of frames to generate. Default value: `\"129\"`\n   */\n  num_frames?: \"129\";\n  /**\n   * Turning on I2V Stability reduces hallucination but also reduces motion.\n   */\n  i2v_stability?: boolean;\n};\nexport type HunyuanVideoImageToVideoOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type HunyuanVideoImg2vidLoraInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The URL to the image to generate the video from. The image must be 960x544 or it will get cropped and resized to that size.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n};\nexport type HunyuanVideoImg2vidLoraOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type HunyuanVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The number of inference steps to run. Lower gets faster results, higher gets better results. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.\n   */\n  pro_mode?: boolean;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * The number of frames to generate. Default value: `\"129\"`\n   */\n  num_frames?: \"129\" | \"85\";\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n};\nexport type HunyuanVideoLoraInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.\n   */\n  pro_mode?: boolean;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * The number of frames to generate. Default value: `\"129\"`\n   */\n  num_frames?: \"129\" | \"85\";\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n};\nexport type HunyuanVideoLoraOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type HunyuanVideoLoraTrainingInput = {\n  /**\n   * URL to zip archive with images. Try to use at least 4 images in general the more the better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Number of steps to train the LoRA on.\n   */\n  steps: number;\n  /**\n   * The trigger word to use. Default value: `\"\"`\n   */\n  trigger_word?: string;\n  /**\n   * Learning rate to use for training. Default value: `0.0001`\n   */\n  learning_rate?: number;\n  /**\n   * Whether to generate captions for the images. Default value: `true`\n   */\n  do_caption?: boolean;\n  /**\n   * The format of the archive. If not specified, the format will be inferred from the URL.\n   */\n  data_archive_format?: string;\n};\nexport type HunyuanVideoLoraTrainingOutput = {\n  /**\n   * URL to the trained diffusers lora weights.\n   */\n  diffusers_lora_file: File;\n  /**\n   * URL to the lora configuration file.\n   */\n  config_file: File;\n};\nexport type HunyuanVideoLoraVideoToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.\n   */\n  pro_mode?: boolean;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * The number of frames to generate. Default value: `\"129\"`\n   */\n  num_frames?: \"129\" | \"85\";\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of the video input.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Strength for Video-to-Video Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type HunyuanVideoLoraVideoToVideoOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type HunyuanVideoOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type HunyuanVideoVideoToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The number of inference steps to run. Lower gets faster results, higher gets better results. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.\n   */\n  pro_mode?: boolean;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * The number of frames to generate. Default value: `\"129\"`\n   */\n  num_frames?: \"129\" | \"85\";\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of the video input.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Strength for Video-to-Video Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type HunyuanVideoVideoToVideoOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type Hyper3dRodinInput = {\n  /**\n   * A textual prompt to guide model generation. Required for Text-to-3D mode. Optional for Image-to-3D mode. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * URL of images to use while generating the 3D model. Required for Image-to-3D mode. Optional for Text-to-3D mode.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * For fuse mode, One or more images are required.It will generate a model by extracting and fusing features of objects from multiple images.For concat mode, need to upload multiple multi-view images of the same object and generate the model.(You can upload multi-view images in any order, regardless of the order of view.) Default value: `\"concat\"`\n   */\n  condition_mode?: \"fuse\" | \"concat\";\n  /**\n   * Seed value for randomization, ranging from 0 to 65535. Optional.\n   */\n  seed?: number;\n  /**\n   * Format of the geometry file. Possible values: glb, usdz, fbx, obj, stl. Default is glb. Default value: `\"glb\"`\n   */\n  geometry_file_format?: \"glb\" | \"usdz\" | \"fbx\" | \"obj\" | \"stl\";\n  /**\n   * Material type. Possible values: PBR, Shaded. Default is PBR. Default value: `\"PBR\"`\n   */\n  material?: \"PBR\" | \"Shaded\";\n  /**\n   * Generation quality. Possible values: high, medium, low, extra-low. Default is medium. Default value: `\"medium\"`\n   */\n  quality?: \"high\" | \"medium\" | \"low\" | \"extra-low\";\n  /**\n   * Whether to export the model using hyper mode. Default is false.\n   */\n  use_hyper?: boolean;\n  /**\n   * Tier of generation. For Rodin Sketch, set to Sketch. For Rodin Regular, set to Regular. Default value: `\"Regular\"`\n   */\n  tier?: \"Regular\" | \"Sketch\";\n  /**\n   * When generating the human-like model, this parameter control the generation result to T/A Pose.\n   */\n  TAPose?: boolean;\n  /**\n   * An array that specifies the dimensions and scaling factor of the bounding box. Typically, this array contains 3 elements, Length(X-axis), Width(Y-axis) and Height(Z-axis).\n   */\n  bbox_condition?: Array<number>;\n  /**\n   * Generation add-on features. Default is []. Possible values are HighPack. The HighPack option will provide 4K resolution textures instead of the default 1K, as well as models with high-poly. It will cost triple the billable units.\n   */\n  addons?: string;\n};\nexport type Hyper3dRodinOutput = {\n  /**\n   * Generated 3D object file.\n   */\n  model_mesh: File;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n  /**\n   * Generated textures for the 3D object.\n   */\n  textures: Array<Image>;\n};\nexport type HyperSdxlImageToImageInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"1\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\";\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type HyperSdxlImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type HyperSdxlInpaintingInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"1\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\";\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type HyperSdxlInpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type HyperSdxlInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"1\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\";\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type HyperSdxlOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type I2VDirectorOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type I2VLiveOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type I2VOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type IclightV2Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * Negative Prompt for the image Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * URL of mask to be used for ic-light conditioning image\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * Threshold for the background removal algorithm. A high threshold will produce sharper masks. Note: This parameter is currently deprecated and has no effect on the output. Default value: `0.67`\n   */\n  background_threshold?: number;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Provide lighting conditions for the model Default value: `\"None\"`\n   */\n  initial_latent?: \"None\" | \"Left\" | \"Right\" | \"Top\" | \"Bottom\";\n  /**\n   * Use HR fix\n   */\n  enable_hr_fix?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The real classifier-free-guidance scale for the generation. Default value: `1`\n   */\n  cfg?: number;\n  /**\n   * Strength for low-resolution pass. Default value: `0.98`\n   */\n  lowres_denoise?: number;\n  /**\n   * Strength for high-resolution pass. Only used if enable_hr_fix is True. Default value: `0.95`\n   */\n  highres_denoise?: number;\n  /**\n   *  Default value: `0.5`\n   */\n  hr_downscale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type IclightV2Output = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type IdeogramUpscaleInput = {\n  /**\n   * The image URL to upscale\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to upscale the image with Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The resemblance of the upscaled image to the original image Default value: `50`\n   */\n  resemblance?: number;\n  /**\n   * The detail of the upscaled image Default value: `50`\n   */\n  detail?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n};\nexport type IdeogramUpscaleOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2aInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * A negative prompt to avoid in the generated image Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type IdeogramV2aOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2aRemixInput = {\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n};\nexport type IdeogramV2aRemixOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2aTurboInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * A negative prompt to avoid in the generated image Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type IdeogramV2aTurboOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2aTurboRemixInput = {\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n};\nexport type IdeogramV2aTurboRemixOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2EditInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n};\nexport type IdeogramV2EditOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2Input = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * A negative prompt to avoid in the generated image Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type IdeogramV2Output = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2RemixInput = {\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n};\nexport type IdeogramV2RemixOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2TurboEditInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n};\nexport type IdeogramV2TurboEditOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2TurboInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * A negative prompt to avoid in the generated image Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type IdeogramV2TurboOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2TurboRemixInput = {\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n};\nexport type IdeogramV2TurboRemixOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IllusionDiffusionInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The scale of the ControlNet. Default value: `1`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   *\n   */\n  control_guidance_start?: number;\n  /**\n   *  Default value: `1`\n   */\n  control_guidance_end?: number;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed?: number;\n  /**\n   * Scheduler / sampler to use for the image denoising process. Default value: `\"Euler\"`\n   */\n  scheduler?: \"DPM++ Karras SDE\" | \"Euler\";\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The size of the generated image. You can choose between some presets or\n   * custom height and width that **must be multiples of 8**. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type IllusionDiffusionOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type ImageConditioningInput = {\n  /**\n   * URL of image to use as conditioning\n   */\n  image_url: string | Blob | File;\n  /**\n   * Frame number of the image from which the conditioning starts. Must be a multiple of 8.\n   */\n  start_frame_num: number;\n};\nexport type ImageExpansionInput = {\n  /**\n   * The URL of the input image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The desired size of the final image, after the expansion. should have an area of less than 5000x5000 pixels.\n   */\n  canvas_size: Array<number>;\n  /**\n   * The desired size of the original image, inside the full canvas. Ensure that the ratio of input image foreground or main subject to the canvas area is greater than 15% to achieve optimal results.\n   */\n  original_image_size: Array<number>;\n  /**\n   * The desired location of the original image, inside the full canvas. Provide the location of the upper left corner of the original image. The location can also be outside the canvas (the original image will be cropped).\n   */\n  original_image_location: Array<number>;\n  /**\n   * Text on which you wish to base the image expansion. This parameter is optional. Bria currently supports prompts in English only, excluding special characters. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * You can choose whether you want your generated expension to be random or predictable. You can recreate the same result in the future by using the seed value of a result from the response. You can exclude this parameter if you are not interested in recreating your results. This parameter is optional.\n   */\n  seed?: number;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of Images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageExpansionOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type ImageFillInput = {\n  /**\n   * URLs of images to be filled into the masked area.\n   */\n  fill_image_url?: Array<string | Blob | File> | string | Blob | File;\n  /**\n   * Uses the provided fill image in context with the base image to fill in more faithfully. Will increase price.\n   */\n  in_context_fill?: boolean;\n  /**\n   * Whether to use the prompt as well in the generation, along with the redux image.\n   */\n  use_prompt?: boolean;\n};\nexport type ImageInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n};\nexport type Imagen3FastInput = {\n  /**\n   * The text prompt describing what you want to see\n   */\n  prompt: string;\n  /**\n   * A description of what to discourage in the generated images Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"1:1\" | \"16:9\" | \"9:16\" | \"3:4\" | \"4:3\";\n  /**\n   * Number of images to generate (1-4) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type Imagen3FastOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type Imagen3Input = {\n  /**\n   * The text prompt describing what you want to see\n   */\n  prompt: string;\n  /**\n   * A description of what to discourage in the generated images Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"1:1\" | \"16:9\" | \"9:16\" | \"3:4\" | \"4:3\";\n  /**\n   * Number of images to generate (1-4) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type Imagen3Output = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type ImagePreprocessorsDepthAnythingV2Input = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n};\nexport type ImagePreprocessorsDepthAnythingV2Output = {\n  /**\n   * Image with depth map\n   */\n  image: Image;\n};\nexport type ImagePreprocessorsHedInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the safe version of the HED detector\n   */\n  safe?: boolean;\n  /**\n   * Whether to use the scribble version of the HED detector\n   */\n  scribble?: boolean;\n};\nexport type ImagePreprocessorsHedOutput = {\n  /**\n   * Image with lines detected using the HED detector\n   */\n  image: Image;\n};\nexport type ImagePreprocessorsLineartInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the coarse model\n   */\n  coarse?: boolean;\n};\nexport type ImagePreprocessorsLineartOutput = {\n  /**\n   * Image with edges detected using the Canny algorithm\n   */\n  image: Image;\n};\nexport type ImagePreprocessorsMidasInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * A parameter for the MiDaS detector Default value: `6.283185307179586`\n   */\n  a?: number;\n  /**\n   * Background threshold for the MiDaS detector Default value: `0.1`\n   */\n  background_threshold?: number;\n};\nexport type ImagePreprocessorsMidasOutput = {\n  /**\n   * Image with MiDaS depth map\n   */\n  depth_map: Image;\n  /**\n   * Image with MiDaS normal map\n   */\n  normal_map: Image;\n};\nexport type ImagePreprocessorsMlsdInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Score threshold for the MLSD detector Default value: `0.1`\n   */\n  score_threshold?: number;\n  /**\n   * Distance threshold for the MLSD detector Default value: `0.1`\n   */\n  distance_threshold?: number;\n};\nexport type ImagePreprocessorsMlsdOutput = {\n  /**\n   * Image with lines detected using the MLSD detector\n   */\n  image: Image;\n};\nexport type ImagePreprocessorsPidiInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the safe version of the Pidi detector\n   */\n  safe?: boolean;\n  /**\n   * Whether to use the scribble version of the Pidi detector\n   */\n  scribble?: boolean;\n  /**\n   * Whether to apply the filter to the image.\n   */\n  apply_filter?: boolean;\n};\nexport type ImagePreprocessorsPidiOutput = {\n  /**\n   * Image with Pidi lines detected\n   */\n  image: Image;\n};\nexport type ImagePreprocessorsSamInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n};\nexport type ImagePreprocessorsSamOutput = {\n  /**\n   * Image with SAM segmentation map\n   */\n  image: Image;\n};\nexport type ImagePreprocessorsScribbleInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * The model to use for the Scribble detector Default value: `\"HED\"`\n   */\n  model?: \"HED\" | \"PiDi\";\n  /**\n   * Whether to use the safe version of the Scribble detector\n   */\n  safe?: boolean;\n};\nexport type ImagePreprocessorsScribbleOutput = {\n  /**\n   * Image with lines detected using the Scribble detector\n   */\n  image: Image;\n};\nexport type ImagePreprocessorsTeedInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n};\nexport type ImagePreprocessorsTeedOutput = {\n  /**\n   * Image with TeeD lines detected\n   */\n  image: Image;\n};\nexport type ImagePreprocessorsZoeInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n};\nexport type ImagePreprocessorsZoeOutput = {\n  /**\n   * Image with depth map\n   */\n  image: Image;\n};\nexport type ImageSizeOutput = {\n  /**\n   * Image size\n   */\n  image_size: any;\n};\nexport type ImageToImageControlNetInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The URL of the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type ImageToImageControlNetUnionInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `35`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n  /**\n   * The URL of the control image.\n   */\n  openpose_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the openpose image. Default value: `true`\n   */\n  openpose_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  depth_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the depth image. Default value: `true`\n   */\n  depth_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  teed_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the teed image. Default value: `true`\n   */\n  teed_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  canny_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the canny image. Default value: `true`\n   */\n  canny_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  normal_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the normal image. Default value: `true`\n   */\n  normal_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  segmentation_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the segmentation image. Default value: `true`\n   */\n  segmentation_preprocess?: boolean;\n};\nexport type ImageToImageFooocusInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the prompt image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `8`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n  /**\n   * If set to true, a smaller model will try to refine the output after it was processed. Default value: `true`\n   */\n  enable_refiner?: boolean;\n};\nexport type ImageToImageHyperInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"1\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\";\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type ImageToImageInput = {\n  /**\n   * URL or HuggingFace ID of the base model to generate the image.\n   */\n  model_name: string;\n  /**\n   * URL or HuggingFace ID of the custom U-Net model to use for the image generation.\n   */\n  unet_name?: string;\n  /**\n   * The variant of the model to use for huggingface models, e.g. 'fp16'.\n   */\n  variant?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, the prompt weighting syntax will be used.\n   * Additionally, this will lift the 77 token limit by averaging embeddings.\n   */\n  prompt_weighting?: boolean;\n  /**\n   * URL of image to use for image to image/inpainting.\n   */\n  image_url?: string | Blob | File;\n  /**\n   * The amount of noise to add to noise image for image. Only used if the image_url is provided. 1.0 is complete noise and 0 is no noise. Default value: `0.5`\n   */\n  noise_strength?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The embeddings to use for the image generation. Only a single embedding is supported at the moment.\n   * The embeddings will be used to map the tokens in the prompt to the embedding weights. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * The control nets to use for the image generation. You can use any number of control nets\n   * and they will be applied to the image at the specified timesteps. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * If set to true, the controlnet will be applied to only the conditional predictions.\n   */\n  controlnet_guess_mode?: boolean;\n  /**\n   * The IP adapter to use for the image generation. Default value: ``\n   */\n  ip_adapter?: Array<IPAdapter>;\n  /**\n   * The path to the image encoder model to use for the image generation.\n   */\n  image_encoder_path?: string;\n  /**\n   * The subfolder of the image encoder model to use for the image generation.\n   */\n  image_encoder_subfolder?: string;\n  /**\n   * The weight name of the image encoder model to use for the image generation. Default value: `\"pytorch_model.bin\"`\n   */\n  image_encoder_weight_name?: string;\n  /**\n   * The URL of the IC Light model to use for the image generation.\n   */\n  ic_light_model_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model background image to use for the image generation.\n   * Make sure to use a background compatible with the model.\n   */\n  ic_light_model_background_image_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model image to use for the image generation.\n   */\n  ic_light_image_url?: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Skips part of the image generation process, leading to slightly different results.\n   * This means the image renders faster, too.\n   */\n  clip_skip?: number;\n  /**\n   * Scheduler / sampler to use for the image denoising process.\n   */\n  scheduler?:\n    | \"DPM++ 2M\"\n    | \"DPM++ 2M Karras\"\n    | \"DPM++ 2M SDE\"\n    | \"DPM++ 2M SDE Karras\"\n    | \"Euler\"\n    | \"Euler A\"\n    | \"Euler (trailing timesteps)\"\n    | \"LCM\"\n    | \"LCM (trailing timesteps)\"\n    | \"DDIM\"\n    | \"TCD\";\n  /**\n   * Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter.\n   * If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set. Default value: `[object Object]`\n   */\n  timesteps?: TimestepsInput;\n  /**\n   * Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter.\n   * If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set. Default value: `[object Object]`\n   */\n  sigmas?: SigmasInput;\n  /**\n   * The type of prediction to use for the image generation.\n   * The `epsilon` is the default. Default value: `\"epsilon\"`\n   */\n  prediction_type?: \"v_prediction\" | \"epsilon\";\n  /**\n   * Whether to set the rescale_betas_snr_zero option or not for the sampler\n   */\n  rescale_betas_snr_zero?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  image_format?: \"jpeg\" | \"png\";\n  /**\n   * Number of images to generate in one request. Note that the higher the batch size,\n   * the longer it will take to generate the images. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_width?: number;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_height?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_width?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_height?: number;\n  /**\n   * The eta value to be used for the image generation.\n   */\n  eta?: number;\n  /**\n   * If set to true, the latents will be saved for debugging.\n   */\n  debug_latents?: boolean;\n  /**\n   * If set to true, the latents will be saved for debugging per pass.\n   */\n  debug_per_pass_latents?: boolean;\n};\nexport type ImageToImageLCMInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/stable-diffusion-xl-base-1.0\"`\n   */\n  model_name?:\n    | \"stabilityai/stable-diffusion-xl-base-1.0\"\n    | \"runwayml/stable-diffusion-v1-5\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `6`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n};\nexport type ImageToImageLightningInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"4\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\" | \"8\";\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type ImageToImagePlaygroundv25Input = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n};\nexport type ImageToImageSD15Input = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type ImageToImageTurboInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/sdxl-turbo\"`\n   */\n  model_name?: \"stabilityai/sdxl-turbo\" | \"stabilityai/sd-turbo\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `2`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type ImageToVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Image URL for Image-to-Video task\n   */\n  image_url: string | Blob | File;\n};\nexport type ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ImageutilsDepthInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * a Default value: `6.283185307179586`\n   */\n  a?: number;\n  /**\n   * bg_th Default value: `0.1`\n   */\n  bg_th?: number;\n  /**\n   * depth_and_normal\n   */\n  depth_and_normal?: boolean;\n};\nexport type ImageutilsDepthOutput = {\n  /**\n   * The depth map.\n   */\n  image: Image;\n};\nexport type ImageutilsMarigoldDepthInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Number of denoising steps. Defaults to `10`. The higher the number, the more accurate the result, but the slower the inference. Default value: `10`\n   */\n  num_inference_steps?: number;\n  /**\n   * Number of predictions to average over. Defaults to `10`. The higher the number, the more accurate the result, but the slower the inference. Default value: `10`\n   */\n  ensemble_size?: number;\n  /**\n   * Maximum processing resolution. Defaults `0` which means it uses the size of the input image.\n   */\n  processing_res?: number;\n};\nexport type ImageutilsMarigoldDepthOutput = {\n  /**\n   * The depth map.\n   */\n  image: Image;\n};\nexport type ImageutilsNsfwInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n};\nexport type ImageutilsNsfwOutput = {\n  /**\n   * The probability of the image being NSFW.\n   */\n  nsfw_probability: number;\n};\nexport type ImageutilsRembgInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the resulting image be cropped to a bounding box around the subject\n   */\n  crop_to_bbox?: boolean;\n};\nexport type ImageutilsRembgOutput = {\n  /**\n   * Background removed image.\n   */\n  image: Image;\n};\nexport type ImageutilsSamInput = {\n  /**\n   * Url to input image\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use when generating masks\n   */\n  text_prompt?: string;\n  /**\n   * Image size Default value: `1024`\n   */\n  size?: number;\n  /**\n   * IOU threshold for filtering the annotations Default value: `0.9`\n   */\n  iou?: number;\n  /**\n   * Draw high-resolution segmentation masks Default value: `true`\n   */\n  retina?: boolean;\n  /**\n   * Object confidence threshold Default value: `0.4`\n   */\n  confidence?: number;\n  /**\n   * Coordinates for multiple boxes, e.g. [[x,y,w,h],[x2,y2,w2,h2]] Default value: `0,0,0,0`\n   */\n  box_prompt?: Array<Array<void>>;\n  /**\n   * Coordinates for multiple points [[x1,y1],[x2,y2]] Default value: `0,0`\n   */\n  point_prompt?: Array<Array<void>>;\n  /**\n   * Label for point, [1,0], 0 = background, 1 = foreground Default value: `0`\n   */\n  point_label?: Array<number>;\n  /**\n   * Draw the edges of the masks\n   */\n  with_contours?: boolean;\n  /**\n   * Attempt better quality output using morphologyEx\n   */\n  better_quality?: boolean;\n  /**\n   * Output black and white, multiple masks will be combined into one mask\n   */\n  black_white?: boolean;\n  /**\n   * Invert mask colors\n   */\n  invert?: boolean;\n};\nexport type ImageutilsSamOutput = {\n  /**\n   * Combined image of all detected masks\n   */\n  image?: Image;\n};\nexport type ImageWithTextInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text input for the task\n   */\n  text_input: string;\n};\nexport type ImageWithUserCoordinatesInput = {\n  /**\n   * The URL of the image to be processed.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The user input coordinates\n   */\n  region: Region;\n};\nexport type InpaintingControlNetInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The URL of the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type InpaintingControlNetUnionInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `35`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n  /**\n   * The URL of the control image.\n   */\n  openpose_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the openpose image. Default value: `true`\n   */\n  openpose_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  depth_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the depth image. Default value: `true`\n   */\n  depth_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  teed_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the teed image. Default value: `true`\n   */\n  teed_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  canny_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the canny image. Default value: `true`\n   */\n  canny_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  normal_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the normal image. Default value: `true`\n   */\n  normal_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  segmentation_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the segmentation image. Default value: `true`\n   */\n  segmentation_preprocess?: boolean;\n};\nexport type InpaintingFooocusInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `8`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n  /**\n   * If set to true, a smaller model will try to refine the output after it was processed. Default value: `true`\n   */\n  enable_refiner?: boolean;\n};\nexport type InpaintingHyperInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"1\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\";\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type InpaintingInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n};\nexport type InpaintingLCMInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/stable-diffusion-xl-base-1.0\"`\n   */\n  model_name?:\n    | \"stabilityai/stable-diffusion-xl-base-1.0\"\n    | \"runwayml/stable-diffusion-v1-5\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `6`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n};\nexport type InpaintingLightningInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"4\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\" | \"8\";\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type InpaintingPlaygroundv25Input = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n};\nexport type InpaintingSD15Input = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type InpaintingTurboInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/sdxl-turbo\"`\n   */\n  model_name?: \"stabilityai/sdxl-turbo\" | \"stabilityai/sd-turbo\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `2`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type InpaintInput = {\n  /**\n   * URL or HuggingFace ID of the base model to generate the image.\n   */\n  model_name: string;\n  /**\n   * URL or HuggingFace ID of the custom U-Net model to use for the image generation.\n   */\n  unet_name?: string;\n  /**\n   * The variant of the model to use for huggingface models, e.g. 'fp16'.\n   */\n  variant?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, the prompt weighting syntax will be used.\n   * Additionally, this will lift the 77 token limit by averaging embeddings.\n   */\n  prompt_weighting?: boolean;\n  /**\n   * URL of image to use for image to image/inpainting.\n   */\n  image_url?: string | Blob | File;\n  /**\n   * URL of black-and-white image to use as mask during inpainting.\n   */\n  mask_url?: string | Blob | File;\n  /**\n   * The amount of noise to add to noise image for image. Only used if the image_url is provided. 1.0 is complete noise and 0 is no noise. Default value: `0.5`\n   */\n  noise_strength?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The embeddings to use for the image generation. Only a single embedding is supported at the moment.\n   * The embeddings will be used to map the tokens in the prompt to the embedding weights. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * The control nets to use for the image generation. You can use any number of control nets\n   * and they will be applied to the image at the specified timesteps. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * If set to true, the controlnet will be applied to only the conditional predictions.\n   */\n  controlnet_guess_mode?: boolean;\n  /**\n   * The IP adapter to use for the image generation. Default value: ``\n   */\n  ip_adapter?: Array<IPAdapter>;\n  /**\n   * The path to the image encoder model to use for the image generation.\n   */\n  image_encoder_path?: string;\n  /**\n   * The subfolder of the image encoder model to use for the image generation.\n   */\n  image_encoder_subfolder?: string;\n  /**\n   * The weight name of the image encoder model to use for the image generation. Default value: `\"pytorch_model.bin\"`\n   */\n  image_encoder_weight_name?: string;\n  /**\n   * The URL of the IC Light model to use for the image generation.\n   */\n  ic_light_model_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model background image to use for the image generation.\n   * Make sure to use a background compatible with the model.\n   */\n  ic_light_model_background_image_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model image to use for the image generation.\n   */\n  ic_light_image_url?: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Skips part of the image generation process, leading to slightly different results.\n   * This means the image renders faster, too.\n   */\n  clip_skip?: number;\n  /**\n   * Scheduler / sampler to use for the image denoising process.\n   */\n  scheduler?:\n    | \"DPM++ 2M\"\n    | \"DPM++ 2M Karras\"\n    | \"DPM++ 2M SDE\"\n    | \"DPM++ 2M SDE Karras\"\n    | \"Euler\"\n    | \"Euler A\"\n    | \"Euler (trailing timesteps)\"\n    | \"LCM\"\n    | \"LCM (trailing timesteps)\"\n    | \"DDIM\"\n    | \"TCD\";\n  /**\n   * Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter.\n   * If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set. Default value: `[object Object]`\n   */\n  timesteps?: TimestepsInput;\n  /**\n   * Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter.\n   * If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set. Default value: `[object Object]`\n   */\n  sigmas?: SigmasInput;\n  /**\n   * The type of prediction to use for the image generation.\n   * The `epsilon` is the default. Default value: `\"epsilon\"`\n   */\n  prediction_type?: \"v_prediction\" | \"epsilon\";\n  /**\n   * Whether to set the rescale_betas_snr_zero option or not for the sampler\n   */\n  rescale_betas_snr_zero?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  image_format?: \"jpeg\" | \"png\";\n  /**\n   * Number of images to generate in one request. Note that the higher the batch size,\n   * the longer it will take to generate the images. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_width?: number;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_height?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_width?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_height?: number;\n  /**\n   * The eta value to be used for the image generation.\n   */\n  eta?: number;\n  /**\n   * If set to true, the latents will be saved for debugging.\n   */\n  debug_latents?: boolean;\n  /**\n   * If set to true, the latents will be saved for debugging per pass.\n   */\n  debug_per_pass_latents?: boolean;\n};\nexport type InpaintOutput = {\n  /**\n   * The generated image files info.\n   */\n  image: Image;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type InpaintTurboInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you.\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * URL of Image for inpainting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Strength for Image-to-Image. Default value: `0.83`\n   */\n  strength?: number;\n  /**\n   * URL of mask image for inpainting.\n   */\n  mask_image_url: string | Blob | File;\n};\nexport type Input = {\n  /**\n   * List of tracks to be combined into the final media\n   */\n  tracks: Array<Track>;\n};\nexport type InsertTextInput = {\n  /**\n   * Input text\n   */\n  text: string;\n  /**\n   * Template to insert text into\n   */\n  template: string;\n};\nexport type InsightfaceInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Threshold for the edge map. Default value: `0.5`\n   */\n  threshold?: number;\n  /**\n   * Size of the detection. Default value: `640`\n   */\n  det_size_width?: number;\n  /**\n   * Size of the detection. Default value: `640`\n   */\n  det_size_height?: number;\n  /**\n   * Maximum number of faces to detect. Default value: `1`\n   */\n  max_face_num?: number;\n  /**\n   * URL of the model weights. Default value: `\"buffalo_l\"`\n   */\n  model_url?: string | Blob | File;\n  /**\n   * Sorting of the faces. Default value: `\"size\"`\n   */\n  sorting?: string;\n  /**\n   * Whether to run in sync mode. Default value: `true`\n   */\n  sync_mode?: boolean;\n};\nexport type InsightfaceOutput = {\n  /**\n   * faces detected sorted by size\n   */\n  faces: Array<FaceDetection>;\n  /**\n   * Bounding box of the face.\n   */\n  bbox: Array<number>;\n  /**\n   * Keypoints of the face.\n   */\n  kps?: Array<Array<number>>;\n  /**\n   * Keypoints of the face on the image.\n   */\n  kps_image: Image;\n  /**\n   * Confidence score of the detection.\n   */\n  det_score: number;\n  /**\n   * Embedding of the face.\n   */\n  embedding_file: File;\n  /**\n   * Either M or F if available.\n   */\n  sex?: string;\n};\nexport type InvertMaskOutput = {\n  /**\n   * The mask\n   */\n  image: Image;\n};\nexport type InvisibleWatermarkInput = {\n  /**\n   * URL of image to be watermarked\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text to use as watermark Default value: `\"watermark\"`\n   */\n  watermark?: string;\n};\nexport type InvisibleWatermarkOutput = {\n  /**\n   * The watermarked image file info.\n   */\n  image: Image;\n};\nexport type IpAdapterFaceIdInput = {\n  /**\n   * The model type to use. 1_5 is the default and is recommended for most use cases. Default value: `\"1_5-v1\"`\n   */\n  model_type?:\n    | \"1_5-v1\"\n    | \"1_5-v1-plus\"\n    | \"1_5-v2-plus\"\n    | \"SDXL-v1\"\n    | \"SDXL-v2-plus\"\n    | \"1_5-auraface-v1\";\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * An image of a face to match. If an image with a size of 640x640 is not provided, it will be scaled and cropped to that size.\n   */\n  face_image_url?: string | Blob | File;\n  /**\n   * URL to zip archive with images of faces. The images embedding will be averaged to\n   * create a more accurate face id.\n   */\n  face_images_data_url?: string | Blob | File;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"blurry, low resolution, bad, ugly, low quality, pixelated, interpolated, compression artifacts, noisey, grainy\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to use for generating the image. The more steps\n   * the better the image will be but it will also take longer to generate. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The number of samples for face id. The more samples the better the image will\n   * be but it will also take longer to generate. Default is 4. Default value: `4`\n   */\n  num_samples?: number;\n  /**\n   * The width of the generated image. Default value: `512`\n   */\n  width?: number;\n  /**\n   * The height of the generated image. Default value: `512`\n   */\n  height?: number;\n  /**\n   * The size of the face detection model. The higher the number the more accurate\n   * the detection will be but it will also take longer to run. The higher the number the more\n   * likely it will fail to find a face as well. Lower it if you are having trouble\n   * finding a face in the image. Default value: `640`\n   */\n  face_id_det_size?: number;\n  /**\n   * The URL to the base 1.5 model. Default is SG161222/Realistic_Vision_V4.0_noVAE Default value: `\"SG161222/Realistic_Vision_V4.0_noVAE\"`\n   */\n  base_1_5_model_repo?: string;\n  /**\n   * The URL to the base SDXL model. Default is SG161222/RealVisXL_V3.0 Default value: `\"SG161222/RealVisXL_V3.0\"`\n   */\n  base_sdxl_model_repo?: string;\n};\nexport type IpAdapterFaceIdOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type ItalianOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type JanusInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Controls randomness in the generation. Higher values make output more random. Default value: `1`\n   */\n  temperature?: number;\n  /**\n   * Classifier Free Guidance scale - how closely to follow the prompt. Default value: `5`\n   */\n  cfg_weight?: number;\n  /**\n   * Number of images to generate in parallel. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JanusOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JapaneseOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type JuggernautFluxBaseImageToImageInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JuggernautFluxBaseImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JuggernautFluxBaseInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JuggernautFluxBaseOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JuggernautFluxLightningInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JuggernautFluxLightningOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JuggernautFluxLoraInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type JuggernautFluxLoraOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JuggernautFluxProImageToImageInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JuggernautFluxProImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JuggernautFluxProInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JuggernautFluxProOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type KlingV15KolorsVirtualTryOnInput = {\n  /**\n   * Url for the human image.\n   */\n  human_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  garment_image_url: string | Blob | File;\n};\nexport type KlingV15KolorsVirtualTryOnOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n};\nexport type KlingV1I2VOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV15ProEffectsInput = {\n  /**\n   * URL of images to be used for hug, kiss or heart_gesture video.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * URL of the image to be used for the squish and expansion video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The effect scene to use for the video generation\n   */\n  effect_scene: \"hug\" | \"kiss\" | \"heart_gesture\" | \"squish\" | \"expansion\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n};\nexport type KlingVideoV15ProEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV15ProImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * URL of the image to be used for the end of the video Default value: `\"false\"`\n   */\n  tail_image_url?: string | Blob | File;\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV15ProImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV15ProTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV15ProTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16ProEffectsInput = {\n  /**\n   * URL of images to be used for hug, kiss or heart_gesture video.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * URL of the image to be used for the squish and expansion video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The effect scene to use for the video generation\n   */\n  effect_scene: \"hug\" | \"kiss\" | \"heart_gesture\" | \"squish\" | \"expansion\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n};\nexport type KlingVideoV16ProEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16ProImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * URL of the image to be used for the end of the video Default value: `\"false\"`\n   */\n  tail_image_url?: string | Blob | File;\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV16ProImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16ProTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV16ProTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16StandardEffectsInput = {\n  /**\n   * URL of images to be used for hug, kiss or heart_gesture video.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * URL of the image to be used for the squish and expansion video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The effect scene to use for the video generation\n   */\n  effect_scene: \"hug\" | \"kiss\" | \"heart_gesture\" | \"squish\" | \"expansion\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n};\nexport type KlingVideoV16StandardEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16StandardImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV16StandardImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16StandardTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV16StandardTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV1ProImageToVideoInput = {\n  /**\n   * The prompt for the video\n   */\n  prompt: string;\n  /**\n   * URL of the image to be used for the video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n  /**\n   * URL of the image to be used for the end of the video Default value: `\"false\"`\n   */\n  tail_image_url?: string | Blob | File;\n  /**\n   * URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)\n   */\n  static_mask_url?: string | Blob | File;\n  /**\n   * List of dynamic masks\n   */\n  dynamic_masks?: Array<DynamicMask>;\n};\nexport type KlingVideoV1ProImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV1ProTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV1ProTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV1StandardEffectsInput = {\n  /**\n   * URL of images to be used for hug, kiss or heart_gesture video.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * URL of the image to be used for the squish and expansion video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The effect scene to use for the video generation\n   */\n  effect_scene: \"hug\" | \"kiss\" | \"heart_gesture\" | \"squish\" | \"expansion\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n};\nexport type KlingVideoV1StandardEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV1StandardImageToVideoInput = {\n  /**\n   * The prompt for the video\n   */\n  prompt: string;\n  /**\n   * URL of the image to be used for the video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n  /**\n   * URL of the image to be used for the end of the video Default value: `\"false\"`\n   */\n  tail_image_url?: string | Blob | File;\n  /**\n   * URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)\n   */\n  static_mask_url?: string | Blob | File;\n  /**\n   * List of dynamic masks\n   */\n  dynamic_masks?: Array<DynamicMask>;\n};\nexport type KlingVideoV1StandardImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV1StandardTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n  /**\n   * Camera control parameters\n   */\n  camera_control?:\n    | \"down_back\"\n    | \"forward_up\"\n    | \"right_turn_forward\"\n    | \"left_turn_forward\";\n  /**\n   * Advanced Camera control parameters\n   */\n  advanced_camera_control?: CameraControl;\n};\nexport type KlingVideoV1StandardTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KokoroAmericanEnglishInput = {\n  /**\n   *  Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * Voice ID for the desired voice. Default value: `\"af_heart\"`\n   */\n  voice?:\n    | \"af_heart\"\n    | \"af_alloy\"\n    | \"af_aoede\"\n    | \"af_bella\"\n    | \"af_jessica\"\n    | \"af_kore\"\n    | \"af_nicole\"\n    | \"af_nova\"\n    | \"af_river\"\n    | \"af_sarah\"\n    | \"af_sky\"\n    | \"am_adam\"\n    | \"am_echo\"\n    | \"am_eric\"\n    | \"am_fenrir\"\n    | \"am_liam\"\n    | \"am_michael\"\n    | \"am_onyx\"\n    | \"am_puck\"\n    | \"am_santa\";\n};\nexport type KokoroAmericanEnglishOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroBrazilianPortugueseInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"pf_dora\" | \"pm_alex\" | \"pm_santa\";\n};\nexport type KokoroBrazilianPortugueseOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroBritishEnglishInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice:\n    | \"bf_alice\"\n    | \"bf_emma\"\n    | \"bf_isabella\"\n    | \"bf_lily\"\n    | \"bm_daniel\"\n    | \"bm_fable\"\n    | \"bm_george\"\n    | \"bm_lewis\";\n};\nexport type KokoroBritishEnglishOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroFrenchInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"ff_siwis\";\n};\nexport type KokoroFrenchOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroHindiInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"hf_alpha\" | \"hf_beta\" | \"hm_omega\" | \"hm_psi\";\n};\nexport type KokoroHindiOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroItalianInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"if_sara\" | \"im_nicola\";\n};\nexport type KokoroItalianOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroJapaneseInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"jf_alpha\" | \"jf_gongitsune\" | \"jf_nezumi\" | \"jf_tebukuro\" | \"jm_kumo\";\n};\nexport type KokoroJapaneseOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroMandarinChineseInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice:\n    | \"zf_xiaobei\"\n    | \"zf_xiaoni\"\n    | \"zf_xiaoxiao\"\n    | \"zf_xiaoyi\"\n    | \"zm_yunjian\"\n    | \"zm_yunxi\"\n    | \"zm_yunxia\"\n    | \"zm_yunyang\";\n};\nexport type KokoroMandarinChineseOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroSpanishInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"ef_dora\" | \"em_alex\" | \"em_santa\";\n};\nexport type KokoroSpanishOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KolorsImageToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small\n   * details (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show\n   * you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Seed\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and\n   * uploaded before returning the response. This will increase the latency of\n   * the function but it allows you to get the image directly in the response\n   * without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Enable safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The scheduler to use for the model. Default value: `\"EulerDiscreteScheduler\"`\n   */\n  scheduler?:\n    | \"EulerDiscreteScheduler\"\n    | \"EulerAncestralDiscreteScheduler\"\n    | \"DPMSolverMultistepScheduler\"\n    | \"DPMSolverMultistepScheduler_SDE_karras\"\n    | \"UniPCMultistepScheduler\"\n    | \"DEISMultistepScheduler\";\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for image to image\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for image-to-image. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type KolorsImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type KolorsImg2ImgInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small\n   * details (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show\n   * you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Seed\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and\n   * uploaded before returning the response. This will increase the latency of\n   * the function but it allows you to get the image directly in the response\n   * without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Enable safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The scheduler to use for the model. Default value: `\"EulerDiscreteScheduler\"`\n   */\n  scheduler?:\n    | \"EulerDiscreteScheduler\"\n    | \"EulerAncestralDiscreteScheduler\"\n    | \"DPMSolverMultistepScheduler\"\n    | \"DPMSolverMultistepScheduler_SDE_karras\"\n    | \"UniPCMultistepScheduler\"\n    | \"DEISMultistepScheduler\";\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for image to image\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for image-to-image. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type KolorsInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible\n   * for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small\n   * details (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show\n   * you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Seed\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and\n   * uploaded before returning the response. This will increase the latency of\n   * the function but it allows you to get the image directly in the response\n   * without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Enable safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The scheduler to use for the model. Default value: `\"EulerDiscreteScheduler\"`\n   */\n  scheduler?:\n    | \"EulerDiscreteScheduler\"\n    | \"EulerAncestralDiscreteScheduler\"\n    | \"DPMSolverMultistepScheduler\"\n    | \"DPMSolverMultistepScheduler_SDE_karras\"\n    | \"UniPCMultistepScheduler\"\n    | \"DEISMultistepScheduler\";\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type KolorsOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type LatentsyncInput = {\n  /**\n   * The URL of the video to generate the lip sync for.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The URL of the audio to generate the lip sync for.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Guidance scale for the model inference Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * Random seed for generation. If None, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Video loop mode when audio is longer than video. Options: pingpong, loop\n   */\n  loop_mode?: \"pingpong\" | \"loop\";\n};\nexport type LatentsyncOutput = {\n  /**\n   * The generated video with the lip sync.\n   */\n  video: File;\n};\nexport type LayerDiffusionInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The prompt to use for generating the negative image. Be as descriptive as possible for best results. Default value: `\"text, watermark\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The guidance scale for the model. Default value: `8`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps for the model. Default value: `20`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type LayerDiffusionOutput = {\n  /**\n   * The URL of the generated image.\n   */\n  image: Image;\n  /**\n   * The seed used to generate the image.\n   */\n  seed: number;\n};\nexport type LcmInput = {\n  /**\n   * The model to use for generating the image. Default value: `\"sdv1-5\"`\n   */\n  model?: \"sdxl\" | \"sdv1-5\";\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The base image to use for guiding the image generation on image-to-image\n   * generations. If the either width or height of the image is larger than 1024\n   * pixels, the image will be resized to 1024 pixels while keeping the aspect ratio.\n   */\n  image_url?: string | Blob | File;\n  /**\n   * The mask to use for guiding the image generation on image\n   * inpainting. The model will focus on the mask area and try to fill it with\n   * the most relevant content.\n   *\n   * The mask must be a black and white image where the white area is the area\n   * that needs to be filled and the black area is the area that should be\n   * ignored.\n   *\n   * The mask must have the same dimensions as the image passed as `image_url`.\n   */\n  mask_url?: string | Blob | File;\n  /**\n   * The strength of the image that is passed as `image_url`. The strength\n   * determines how much the generated image will be similar to the image passed as\n   * `image_url`. The higher the strength the more model gets \"creative\" and\n   * generates an image that's different from the initial image. A strength of 1.0\n   * means that the initial image is more or less ignored and the model will try to\n   * generate an image that's as close as possible to the prompt. Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to use for generating the image. The more steps\n   * the better the image will be but it will also take longer to generate. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The size of the generated image. You can choose between some presets or\n   * custom height and width that **must be multiples of 8**.\n   *\n   * If not provided:\n   * - For text-to-image generations, the default size is 512x512.\n   * - For image-to-image generations, the default size is the same as the input image.\n   * - For inpainting generations, the default size is the same as the input image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. The function will return a list of images\n   * with the same prompt and negative prompt but different seeds. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the resulting image will be checked whether it includes any\n   * potentially unsafe content. If it does, it will be replaced with a black\n   * image. Default value: `true`\n   */\n  enable_safety_checks?: boolean;\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n  /**\n   * If set to true, the inpainting pipeline will only inpaint the provided mask\n   * area. Only effective for inpainting pipelines.\n   */\n  inpaint_mask_only?: boolean;\n  /**\n   * If set to true, the inpainting pipeline will use controlnet inpainting.\n   * Only effective for inpainting pipelines.\n   */\n  controlnet_inpaint?: boolean;\n  /**\n   * The url of the lora server to use for image generation.\n   */\n  lora_url?: string | Blob | File;\n  /**\n   * The scale of the lora server to use for image generation. Default value: `1`\n   */\n  lora_scale?: number;\n};\nexport type LcmOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Number of inference steps used to generate the image. It will be the same value of the one passed in the\n   * input or the default one in case none was passed. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n  /**\n   * A list of booleans indicating whether the generated image contains any\n   * potentially unsafe content. If the safety check is disabled, this field\n   * will all will be false.\n   */\n  nsfw_content_detected: Array<boolean>;\n};\nexport type LcmSd15I2iInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The image to use as a base.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the image. Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to use for generating the image. The more steps\n   * the better the image will be but it will also take longer to generate. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. The function will return a list of images\n   * with the same prompt and negative prompt but different seeds. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the resulting image will be checked whether it includes any\n   * potentially unsafe content. If it does, it will be replaced with a black\n   * image. Default value: `true`\n   */\n  enable_safety_checks?: boolean;\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n};\nexport type LcmSd15I2iOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Number of inference steps used to generate the image. It will be the same value of the one passed in the\n   * input or the default one in case none was passed. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n  /**\n   * A list of booleans indicating whether the generated image contains any\n   * potentially unsafe content. If the safety check is disabled, this field\n   * will have a false for each generated image.\n   */\n  nsfw_content_detected: Array<boolean>;\n};\nexport type LDMTTSInput = {\n  /**\n   * The dialogue text with turn prefixes to distinguish speakers.\n   */\n  input: string;\n  /**\n   * A list of voice definitions for each speaker in the dialogue. Must be between 1 and 2 voices. Default value: `[object Object],[object Object]`\n   */\n  voices?: Array<LDMVoiceInput>;\n  /**\n   * S3 URI of the autoregressive (AR) model.\n   */\n  ar?: string;\n  /**\n   * S3 URI of the AR LoRA model.\n   */\n  lora?: string;\n  /**\n   * S3 URI of the vocoder model.\n   */\n  vocoder?: string;\n  /**\n   * The format of the response. Default value: `\"url\"`\n   */\n  response_format?: \"url\" | \"bytes\";\n  /**\n   * An integer number greater than or equal to 0. If equal to null or not provided, a random seed will be used. Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file.\n   */\n  seed?: number;\n};\nexport type LDMTTSOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: AudioFile;\n};\nexport type LDMVoiceInput = {\n  /**\n   * The unique ID of a PlayHT or Cloned Voice, or a name from the available presets.\n   */\n  voice: string;\n  /**\n   * A prefix to identify the speaker in multi-turn dialogues. Default value: `\"Speaker 1: \"`\n   */\n  turn_prefix?: string;\n};\nexport type LeffaPoseTransferInput = {\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same input given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your input when generating the image. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Url for the human image.\n   */\n  pose_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  person_image_url: string | Blob | File;\n};\nexport type LeffaPoseTransferOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n  /**\n   * The seed for the inference.\n   */\n  seed: number;\n  /**\n   * Whether the image contains NSFW concepts.\n   */\n  has_nsfw_concepts: boolean;\n};\nexport type LeffaVirtualTryonInput = {\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same input given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your input when generating the image. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Url for the human image.\n   */\n  human_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  garment_image_url: string | Blob | File;\n};\nexport type LeffaVirtualTryonOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n  /**\n   * The seed for the inference.\n   */\n  seed: number;\n  /**\n   * Whether the image contains NSFW concepts.\n   */\n  has_nsfw_concepts: boolean;\n};\nexport type LightningModelsImageToImageInput = {\n  /**\n   * The Lightning model to use.\n   */\n  model_name?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   *  Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `5`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2`\n   */\n  guidance_scale?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * Scheduler / sampler to use for the image denoising process.\n   */\n  scheduler?:\n    | \"DPM++ 2M\"\n    | \"DPM++ 2M Karras\"\n    | \"DPM++ 2M SDE\"\n    | \"DPM++ 2M SDE Karras\"\n    | \"DPM++ SDE\"\n    | \"DPM++ SDE Karras\"\n    | \"KDPM 2A\"\n    | \"Euler\"\n    | \"Euler (trailing timesteps)\"\n    | \"Euler A\"\n    | \"LCM\"\n    | \"EDMDPMSolverMultistepScheduler\"\n    | \"TCDScheduler\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n};\nexport type LightningModelsInpaintingInput = {\n  /**\n   * The Lightning model to use.\n   */\n  model_name?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   *  Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `5`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2`\n   */\n  guidance_scale?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * Scheduler / sampler to use for the image denoising process.\n   */\n  scheduler?:\n    | \"DPM++ 2M\"\n    | \"DPM++ 2M Karras\"\n    | \"DPM++ 2M SDE\"\n    | \"DPM++ 2M SDE Karras\"\n    | \"DPM++ SDE\"\n    | \"DPM++ SDE Karras\"\n    | \"KDPM 2A\"\n    | \"Euler\"\n    | \"Euler (trailing timesteps)\"\n    | \"Euler A\"\n    | \"LCM\"\n    | \"EDMDPMSolverMultistepScheduler\"\n    | \"TCDScheduler\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n};\nexport type LightningModelsInput = {\n  /**\n   * The Lightning model to use.\n   */\n  model_name?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want in the image. Default value: `\"(worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)\"`\n   */\n  negative_prompt?: string;\n  /**\n   *  Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `5`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2`\n   */\n  guidance_scale?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * Scheduler / sampler to use for the image denoising process.\n   */\n  scheduler?:\n    | \"DPM++ 2M\"\n    | \"DPM++ 2M Karras\"\n    | \"DPM++ 2M SDE\"\n    | \"DPM++ 2M SDE Karras\"\n    | \"DPM++ SDE\"\n    | \"DPM++ SDE Karras\"\n    | \"KDPM 2A\"\n    | \"Euler\"\n    | \"Euler (trailing timesteps)\"\n    | \"Euler A\"\n    | \"LCM\"\n    | \"EDMDPMSolverMultistepScheduler\"\n    | \"TCDScheduler\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n};\nexport type LightningModelsOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type LineartInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the coarse model\n   */\n  coarse?: boolean;\n};\nexport type LineartOutput = {\n  /**\n   * Image with edges detected using the Canny algorithm\n   */\n  image: Image;\n};\nexport type LivePortraitImageInput = {\n  /**\n   * URL of the image to be animated\n   */\n  image_url: string | Blob | File;\n  /**\n   * Amount to blink the eyes\n   */\n  blink?: number;\n  /**\n   * Amount to raise or lower eyebrows\n   */\n  eyebrow?: number;\n  /**\n   * Amount to wink\n   */\n  wink?: number;\n  /**\n   * Amount to move pupils horizontally\n   */\n  pupil_x?: number;\n  /**\n   * Amount to move pupils vertically\n   */\n  pupil_y?: number;\n  /**\n   * Amount to open mouth in 'aaa' shape\n   */\n  aaa?: number;\n  /**\n   * Amount to shape mouth in 'eee' position\n   */\n  eee?: number;\n  /**\n   * Amount to shape mouth in 'woo' position\n   */\n  woo?: number;\n  /**\n   * Amount to smile\n   */\n  smile?: number;\n  /**\n   * Amount to rotate the face in pitch\n   */\n  rotate_pitch?: number;\n  /**\n   * Amount to rotate the face in yaw\n   */\n  rotate_yaw?: number;\n  /**\n   * Amount to rotate the face in roll\n   */\n  rotate_roll?: number;\n  /**\n   * Whether to paste-back/stitch the animated face cropping from the face-cropping space to the original image space. Default value: `true`\n   */\n  flag_pasteback?: boolean;\n  /**\n   * Whether to crop the source portrait to the face-cropping space. Default value: `true`\n   */\n  flag_do_crop?: boolean;\n  /**\n   * Whether to conduct the rotation when flag_do_crop is True. Default value: `true`\n   */\n  flag_do_rot?: boolean;\n  /**\n   * Whether to set the lip to closed state before animation. Only takes effect when flag_eye_retargeting and flag_lip_retargeting are False. Default value: `true`\n   */\n  flag_lip_zero?: boolean;\n  /**\n   * Size of the output image. Default value: `512`\n   */\n  dsize?: number;\n  /**\n   * Scaling factor for the face crop. Default value: `2.3`\n   */\n  scale?: number;\n  /**\n   * Horizontal offset ratio for face crop.\n   */\n  vx_ratio?: number;\n  /**\n   * Vertical offset ratio for face crop. Positive values move up, negative values move down. Default value: `-0.125`\n   */\n  vy_ratio?: number;\n  /**\n   * Whether to enable the safety checker. If enabled, the model will check if the input image contains a face before processing it.\n   * The safety checker will process the input image\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Output format Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type LivePortraitImageOutput = {\n  /**\n   * The generated image file.\n   */\n  image: Image;\n};\nexport type LivePortraitInput = {\n  /**\n   * URL of the video to drive the lip syncing.\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the image to be animated\n   */\n  image_url: string | Blob | File;\n  /**\n   * Amount to blink the eyes\n   */\n  blink?: number;\n  /**\n   * Amount to raise or lower eyebrows\n   */\n  eyebrow?: number;\n  /**\n   * Amount to wink\n   */\n  wink?: number;\n  /**\n   * Amount to move pupils horizontally\n   */\n  pupil_x?: number;\n  /**\n   * Amount to move pupils vertically\n   */\n  pupil_y?: number;\n  /**\n   * Amount to open mouth in 'aaa' shape\n   */\n  aaa?: number;\n  /**\n   * Amount to shape mouth in 'eee' position\n   */\n  eee?: number;\n  /**\n   * Amount to shape mouth in 'woo' position\n   */\n  woo?: number;\n  /**\n   * Amount to smile\n   */\n  smile?: number;\n  /**\n   * Whether to set the lip to closed state before animation. Only takes effect when flag_eye_retargeting and flag_lip_retargeting are False. Default value: `true`\n   */\n  flag_lip_zero?: boolean;\n  /**\n   * Amount to rotate the face in pitch\n   */\n  rotate_pitch?: number;\n  /**\n   * Amount to rotate the face in yaw\n   */\n  rotate_yaw?: number;\n  /**\n   * Amount to rotate the face in roll\n   */\n  rotate_roll?: number;\n  /**\n   * Whether to enable eye retargeting.\n   */\n  flag_eye_retargeting?: boolean;\n  /**\n   * Whether to enable lip retargeting.\n   */\n  flag_lip_retargeting?: boolean;\n  /**\n   * Whether to enable stitching. Recommended to set to True. Default value: `true`\n   */\n  flag_stitching?: boolean;\n  /**\n   * Whether to use relative motion. Default value: `true`\n   */\n  flag_relative?: boolean;\n  /**\n   * Whether to paste-back/stitch the animated face cropping from the face-cropping space to the original image space. Default value: `true`\n   */\n  flag_pasteback?: boolean;\n  /**\n   * Whether to crop the source portrait to the face-cropping space. Default value: `true`\n   */\n  flag_do_crop?: boolean;\n  /**\n   * Whether to conduct the rotation when flag_do_crop is True. Default value: `true`\n   */\n  flag_do_rot?: boolean;\n  /**\n   * Size of the output image. Default value: `512`\n   */\n  dsize?: number;\n  /**\n   * Scaling factor for the face crop. Default value: `2.3`\n   */\n  scale?: number;\n  /**\n   * Horizontal offset ratio for face crop.\n   */\n  vx_ratio?: number;\n  /**\n   * Vertical offset ratio for face crop. Positive values move up, negative values move down. Default value: `-0.125`\n   */\n  vy_ratio?: number;\n  /**\n   * Batch size for the model. The larger the batch size, the faster the model will run, but the more memory it will consume. Default value: `32`\n   */\n  batch_size?: number;\n  /**\n   * Whether to enable the safety checker. If enabled, the model will check if the input image contains a face before processing it.\n   * The safety checker will process the input image\n   */\n  enable_safety_checker?: boolean;\n};\nexport type LivePortraitOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n};\nexport type LivePortraitVideoInput = {\n  /**\n   * URL of the video to drive the lip syncing.\n   */\n  source_video_url: string | Blob | File;\n  /**\n   * URL of the video to drive the lip syncing.\n   */\n  driving_video_url: string | Blob | File;\n  /**\n   * Whether to prioritize source or driving audio. Default value: `\"source\"`\n   */\n  audio_priority?: \"source\" | \"driving\";\n  /**\n   * Whether to filter out NSFW content. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type LlavaNextInput = {\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Prompt to be used for the image\n   */\n  prompt: string;\n  /**\n   * Maximum number of tokens to generate Default value: `64`\n   */\n  max_tokens?: number;\n  /**\n   * Temperature for sampling Default value: `0.2`\n   */\n  temperature?: number;\n  /**\n   * Top P for sampling Default value: `1`\n   */\n  top_p?: number;\n};\nexport type LlavaNextOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Whether the output is partial\n   */\n  partial?: boolean;\n};\nexport type Llavav1513bInput = {\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Prompt to be used for the image\n   */\n  prompt: string;\n  /**\n   * Maximum number of tokens to generate Default value: `64`\n   */\n  max_tokens?: number;\n  /**\n   * Temperature for sampling Default value: `0.2`\n   */\n  temperature?: number;\n  /**\n   * Top P for sampling Default value: `1`\n   */\n  top_p?: number;\n};\nexport type Llavav1513bOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Whether the output is partial\n   */\n  partial?: boolean;\n};\nexport type LoraImageToImageInput = {\n  /**\n   * URL or HuggingFace ID of the base model to generate the image.\n   */\n  model_name: string;\n  /**\n   * URL or HuggingFace ID of the custom U-Net model to use for the image generation.\n   */\n  unet_name?: string;\n  /**\n   * The variant of the model to use for huggingface models, e.g. 'fp16'.\n   */\n  variant?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, the prompt weighting syntax will be used.\n   * Additionally, this will lift the 77 token limit by averaging embeddings.\n   */\n  prompt_weighting?: boolean;\n  /**\n   * URL of image to use for image to image/inpainting.\n   */\n  image_url?: string | Blob | File;\n  /**\n   * The amount of noise to add to noise image for image. Only used if the image_url is provided. 1.0 is complete noise and 0 is no noise. Default value: `0.5`\n   */\n  noise_strength?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The embeddings to use for the image generation. Only a single embedding is supported at the moment.\n   * The embeddings will be used to map the tokens in the prompt to the embedding weights. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * The control nets to use for the image generation. You can use any number of control nets\n   * and they will be applied to the image at the specified timesteps. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * If set to true, the controlnet will be applied to only the conditional predictions.\n   */\n  controlnet_guess_mode?: boolean;\n  /**\n   * The IP adapter to use for the image generation. Default value: ``\n   */\n  ip_adapter?: Array<IPAdapter>;\n  /**\n   * The path to the image encoder model to use for the image generation.\n   */\n  image_encoder_path?: string;\n  /**\n   * The subfolder of the image encoder model to use for the image generation.\n   */\n  image_encoder_subfolder?: string;\n  /**\n   * The weight name of the image encoder model to use for the image generation. Default value: `\"pytorch_model.bin\"`\n   */\n  image_encoder_weight_name?: string;\n  /**\n   * The URL of the IC Light model to use for the image generation.\n   */\n  ic_light_model_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model background image to use for the image generation.\n   * Make sure to use a background compatible with the model.\n   */\n  ic_light_model_background_image_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model image to use for the image generation.\n   */\n  ic_light_image_url?: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Skips part of the image generation process, leading to slightly different results.\n   * This means the image renders faster, too.\n   */\n  clip_skip?: number;\n  /**\n   * Scheduler / sampler to use for the image denoising process.\n   */\n  scheduler?:\n    | \"DPM++ 2M\"\n    | \"DPM++ 2M Karras\"\n    | \"DPM++ 2M SDE\"\n    | \"DPM++ 2M SDE Karras\"\n    | \"Euler\"\n    | \"Euler A\"\n    | \"Euler (trailing timesteps)\"\n    | \"LCM\"\n    | \"LCM (trailing timesteps)\"\n    | \"DDIM\"\n    | \"TCD\";\n  /**\n   * Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter.\n   * If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set. Default value: `[object Object]`\n   */\n  timesteps?: TimestepsInput;\n  /**\n   * Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter.\n   * If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set. Default value: `[object Object]`\n   */\n  sigmas?: SigmasInput;\n  /**\n   * The type of prediction to use for the image generation.\n   * The `epsilon` is the default. Default value: `\"epsilon\"`\n   */\n  prediction_type?: \"v_prediction\" | \"epsilon\";\n  /**\n   * Whether to set the rescale_betas_snr_zero option or not for the sampler\n   */\n  rescale_betas_snr_zero?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  image_format?: \"jpeg\" | \"png\";\n  /**\n   * Number of images to generate in one request. Note that the higher the batch size,\n   * the longer it will take to generate the images. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_width?: number;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_height?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_width?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_height?: number;\n  /**\n   * The eta value to be used for the image generation.\n   */\n  eta?: number;\n  /**\n   * If set to true, the latents will be saved for debugging.\n   */\n  debug_latents?: boolean;\n  /**\n   * If set to true, the latents will be saved for debugging per pass.\n   */\n  debug_per_pass_latents?: boolean;\n};\nexport type LoraImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The latents saved for debugging.\n   */\n  debug_latents?: File;\n  /**\n   * The latents saved for debugging per pass.\n   */\n  debug_per_pass_latents?: File;\n};\nexport type LoraInpaintInput = {\n  /**\n   * URL or HuggingFace ID of the base model to generate the image.\n   */\n  model_name: string;\n  /**\n   * URL or HuggingFace ID of the custom U-Net model to use for the image generation.\n   */\n  unet_name?: string;\n  /**\n   * The variant of the model to use for huggingface models, e.g. 'fp16'.\n   */\n  variant?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, the prompt weighting syntax will be used.\n   * Additionally, this will lift the 77 token limit by averaging embeddings.\n   */\n  prompt_weighting?: boolean;\n  /**\n   * URL of image to use for image to image/inpainting.\n   */\n  image_url?: string | Blob | File;\n  /**\n   * URL of black-and-white image to use as mask during inpainting.\n   */\n  mask_url?: string | Blob | File;\n  /**\n   * The amount of noise to add to noise image for image. Only used if the image_url is provided. 1.0 is complete noise and 0 is no noise. Default value: `0.5`\n   */\n  noise_strength?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The embeddings to use for the image generation. Only a single embedding is supported at the moment.\n   * The embeddings will be used to map the tokens in the prompt to the embedding weights. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * The control nets to use for the image generation. You can use any number of control nets\n   * and they will be applied to the image at the specified timesteps. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * If set to true, the controlnet will be applied to only the conditional predictions.\n   */\n  controlnet_guess_mode?: boolean;\n  /**\n   * The IP adapter to use for the image generation. Default value: ``\n   */\n  ip_adapter?: Array<IPAdapter>;\n  /**\n   * The path to the image encoder model to use for the image generation.\n   */\n  image_encoder_path?: string;\n  /**\n   * The subfolder of the image encoder model to use for the image generation.\n   */\n  image_encoder_subfolder?: string;\n  /**\n   * The weight name of the image encoder model to use for the image generation. Default value: `\"pytorch_model.bin\"`\n   */\n  image_encoder_weight_name?: string;\n  /**\n   * The URL of the IC Light model to use for the image generation.\n   */\n  ic_light_model_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model background image to use for the image generation.\n   * Make sure to use a background compatible with the model.\n   */\n  ic_light_model_background_image_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model image to use for the image generation.\n   */\n  ic_light_image_url?: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Skips part of the image generation process, leading to slightly different results.\n   * This means the image renders faster, too.\n   */\n  clip_skip?: number;\n  /**\n   * Scheduler / sampler to use for the image denoising process.\n   */\n  scheduler?:\n    | \"DPM++ 2M\"\n    | \"DPM++ 2M Karras\"\n    | \"DPM++ 2M SDE\"\n    | \"DPM++ 2M SDE Karras\"\n    | \"Euler\"\n    | \"Euler A\"\n    | \"Euler (trailing timesteps)\"\n    | \"LCM\"\n    | \"LCM (trailing timesteps)\"\n    | \"DDIM\"\n    | \"TCD\";\n  /**\n   * Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter.\n   * If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set. Default value: `[object Object]`\n   */\n  timesteps?: TimestepsInput;\n  /**\n   * Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter.\n   * If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set. Default value: `[object Object]`\n   */\n  sigmas?: SigmasInput;\n  /**\n   * The type of prediction to use for the image generation.\n   * The `epsilon` is the default. Default value: `\"epsilon\"`\n   */\n  prediction_type?: \"v_prediction\" | \"epsilon\";\n  /**\n   * Whether to set the rescale_betas_snr_zero option or not for the sampler\n   */\n  rescale_betas_snr_zero?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  image_format?: \"jpeg\" | \"png\";\n  /**\n   * Number of images to generate in one request. Note that the higher the batch size,\n   * the longer it will take to generate the images. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_width?: number;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_height?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_width?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_height?: number;\n  /**\n   * The eta value to be used for the image generation.\n   */\n  eta?: number;\n  /**\n   * If set to true, the latents will be saved for debugging.\n   */\n  debug_latents?: boolean;\n  /**\n   * If set to true, the latents will be saved for debugging per pass.\n   */\n  debug_per_pass_latents?: boolean;\n};\nexport type LoraInpaintOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The latents saved for debugging.\n   */\n  debug_latents?: File;\n  /**\n   * The latents saved for debugging per pass.\n   */\n  debug_per_pass_latents?: File;\n};\nexport type LoraInput = {\n  /**\n   * URL or HuggingFace ID of the base model to generate the image.\n   */\n  model_name: string;\n  /**\n   * URL or HuggingFace ID of the custom U-Net model to use for the image generation.\n   */\n  unet_name?: string;\n  /**\n   * The variant of the model to use for huggingface models, e.g. 'fp16'.\n   */\n  variant?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, the prompt weighting syntax will be used.\n   * Additionally, this will lift the 77 token limit by averaging embeddings.\n   */\n  prompt_weighting?: boolean;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The embeddings to use for the image generation. Only a single embedding is supported at the moment.\n   * The embeddings will be used to map the tokens in the prompt to the embedding weights. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * The control nets to use for the image generation. You can use any number of control nets\n   * and they will be applied to the image at the specified timesteps. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * If set to true, the controlnet will be applied to only the conditional predictions.\n   */\n  controlnet_guess_mode?: boolean;\n  /**\n   * The IP adapter to use for the image generation. Default value: ``\n   */\n  ip_adapter?: Array<IPAdapter>;\n  /**\n   * The path to the image encoder model to use for the image generation.\n   */\n  image_encoder_path?: string;\n  /**\n   * The subfolder of the image encoder model to use for the image generation.\n   */\n  image_encoder_subfolder?: string;\n  /**\n   * The weight name of the image encoder model to use for the image generation. Default value: `\"pytorch_model.bin\"`\n   */\n  image_encoder_weight_name?: string;\n  /**\n   * The URL of the IC Light model to use for the image generation.\n   */\n  ic_light_model_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model background image to use for the image generation.\n   * Make sure to use a background compatible with the model.\n   */\n  ic_light_model_background_image_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model image to use for the image generation.\n   */\n  ic_light_image_url?: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The size of the generated image. You can choose between some presets or custom height and width\n   * that **must be multiples of 8**. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Skips part of the image generation process, leading to slightly different results.\n   * This means the image renders faster, too.\n   */\n  clip_skip?: number;\n  /**\n   * Scheduler / sampler to use for the image denoising process.\n   */\n  scheduler?:\n    | \"DPM++ 2M\"\n    | \"DPM++ 2M Karras\"\n    | \"DPM++ 2M SDE\"\n    | \"DPM++ 2M SDE Karras\"\n    | \"Euler\"\n    | \"Euler A\"\n    | \"Euler (trailing timesteps)\"\n    | \"LCM\"\n    | \"LCM (trailing timesteps)\"\n    | \"DDIM\"\n    | \"TCD\";\n  /**\n   * Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter.\n   * If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set. Default value: `[object Object]`\n   */\n  timesteps?: TimestepsInput;\n  /**\n   * Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter.\n   * If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set. Default value: `[object Object]`\n   */\n  sigmas?: SigmasInput;\n  /**\n   * The type of prediction to use for the image generation.\n   * The `epsilon` is the default. Default value: `\"epsilon\"`\n   */\n  prediction_type?: \"v_prediction\" | \"epsilon\";\n  /**\n   * Whether to set the rescale_betas_snr_zero option or not for the sampler\n   */\n  rescale_betas_snr_zero?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  image_format?: \"jpeg\" | \"png\";\n  /**\n   * Number of images to generate in one request. Note that the higher the batch size,\n   * the longer it will take to generate the images. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_width?: number;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_height?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_width?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_height?: number;\n  /**\n   * The eta value to be used for the image generation.\n   */\n  eta?: number;\n  /**\n   * If set to true, the latents will be saved for debugging.\n   */\n  debug_latents?: boolean;\n  /**\n   * If set to true, the latents will be saved for debugging per pass.\n   */\n  debug_per_pass_latents?: boolean;\n};\nexport type LoraOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The latents saved for debugging.\n   */\n  debug_latents?: File;\n  /**\n   * The latents saved for debugging per pass.\n   */\n  debug_per_pass_latents?: File;\n};\nexport type LtxVideoImageToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to generate the video from. Default value: `\"low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The seed to use for random number generation.\n   */\n  seed?: number;\n  /**\n   * The number of inference steps to take. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The guidance scale to use. Default value: `3`\n   */\n  guidance_scale?: number;\n  /**\n   * The URL of the image to generate the video from.\n   */\n  image_url: string | Blob | File;\n};\nexport type LtxVideoImageToVideoOutput = {\n  /**\n   * The generated video.\n   */\n  video: File;\n  /**\n   * The seed used for random number generation.\n   */\n  seed: number;\n};\nexport type LtxVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to generate the video from. Default value: `\"low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The seed to use for random number generation.\n   */\n  seed?: number;\n  /**\n   * The number of inference steps to take. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The guidance scale to use. Default value: `3`\n   */\n  guidance_scale?: number;\n};\nexport type LtxVideoOutput = {\n  /**\n   * The generated video.\n   */\n  video: File;\n  /**\n   * The seed used for random number generation.\n   */\n  seed: number;\n};\nexport type LtxVideoV095ExtendInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Video to be extended.\n   */\n  video: VideoConditioningInput;\n};\nexport type LtxVideoV095ExtendOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideoV095ImageToVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Image URL for Image-to-Video task\n   */\n  image_url: string | Blob | File;\n};\nexport type LtxVideoV095ImageToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideoV095Input = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n};\nexport type LtxVideoV095MulticonditioningInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * URL of images to use as conditioning Default value: ``\n   */\n  images?: Array<ImageConditioningInput>;\n  /**\n   * Videos to use as conditioning Default value: ``\n   */\n  videos?: Array<VideoConditioningInput>;\n};\nexport type LtxVideoV095MulticonditioningOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideoV095Output = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LumaDreamMachineImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * An image to blend the end of the video with\n   */\n  end_image_url?: string | Blob | File;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Whether the video should loop (end of video is blended with the beginning)\n   */\n  loop?: boolean;\n};\nexport type LumaDreamMachineImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type LumaDreamMachineInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Whether the video should loop (end of video is blended with the beginning)\n   */\n  loop?: boolean;\n};\nexport type LumaDreamMachineOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2FlashImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Initial image to start the video from. Can be used together with end_image_url.\n   */\n  image_url?: string | Blob | File;\n  /**\n   * Final image to end the video with. Can be used together with image_url.\n   */\n  end_image_url?: string | Blob | File;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Whether the video should loop (end of video is blended with the beginning)\n   */\n  loop?: boolean;\n  /**\n   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `\"540p\"`\n   */\n  resolution?: \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video Default value: `\"5s\"`\n   */\n  duration?: \"5s\";\n};\nexport type LumaDreamMachineRay2FlashImageToVideoOutput = {\n  /**\n   * URL of the generated video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2FlashInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Whether the video should loop (end of video is blended with the beginning)\n   */\n  loop?: boolean;\n  /**\n   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `\"540p\"`\n   */\n  resolution?: \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video (9s costs 2x more) Default value: `\"5s\"`\n   */\n  duration?: \"5s\" | \"9s\";\n};\nexport type LumaDreamMachineRay2FlashOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2ImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Initial image to start the video from. Can be used together with end_image_url.\n   */\n  image_url?: string | Blob | File;\n  /**\n   * Final image to end the video with. Can be used together with image_url.\n   */\n  end_image_url?: string | Blob | File;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Whether the video should loop (end of video is blended with the beginning)\n   */\n  loop?: boolean;\n  /**\n   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `\"540p\"`\n   */\n  resolution?: \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video Default value: `\"5s\"`\n   */\n  duration?: \"5s\";\n};\nexport type LumaDreamMachineRay2ImageToVideoOutput = {\n  /**\n   * URL of the generated video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2Input = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Whether the video should loop (end of video is blended with the beginning)\n   */\n  loop?: boolean;\n  /**\n   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `\"540p\"`\n   */\n  resolution?: \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video (9s costs 2x more) Default value: `\"5s\"`\n   */\n  duration?: \"5s\" | \"9s\";\n};\nexport type LumaDreamMachineRay2Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type LumaPhotonFlashInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n};\nexport type LumaPhotonFlashOutput = {\n  /**\n   * The generated image\n   */\n  images: Array<File>;\n};\nexport type LumaPhotonInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n};\nexport type LumaPhotonOutput = {\n  /**\n   * The generated image\n   */\n  images: Array<File>;\n};\nexport type LuminaImageV2Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The system prompt to use. Default value: `\"You are an assistant designed to generate superior images with the superior degree of image-text alignment based on textual prompts or user prompts.\"`\n   */\n  system_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to normalize the CFG. Default value: `true`\n   */\n  cfg_normalization?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type LuminaImageV2Output = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type MandarinOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type MarigoldDepthMapInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Number of denoising steps. Defaults to `10`. The higher the number, the more accurate the result, but the slower the inference. Default value: `10`\n   */\n  num_inference_steps?: number;\n  /**\n   * Number of predictions to average over. Defaults to `10`. The higher the number, the more accurate the result, but the slower the inference. Default value: `10`\n   */\n  ensemble_size?: number;\n  /**\n   * Maximum processing resolution. Defaults `0` which means it uses the size of the input image.\n   */\n  processing_res?: number;\n};\nexport type MarigoldDepthMapOutput = {\n  /**\n   * The depth map.\n   */\n  image: Image;\n};\nexport type MaskInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n};\nexport type MetadataInput = {\n  /**\n   * URL of the media file (video or audio) to analyze\n   */\n  media_url: string | Blob | File;\n  /**\n   * Whether to extract the start and end frames for videos. Note that when true the request will be slower.\n   */\n  extract_frames?: boolean;\n};\nexport type MetadataOutput = {\n  /**\n   * Metadata for the analyzed media file (either Video or Audio)\n   */\n  media: Video | Audio;\n};\nexport type MiDaSInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * A parameter for the MiDaS detector Default value: `6.283185307179586`\n   */\n  a?: number;\n  /**\n   * Background threshold for the MiDaS detector Default value: `0.1`\n   */\n  background_threshold?: number;\n};\nexport type MiDaSOutput = {\n  /**\n   * Image with MiDaS depth map\n   */\n  depth_map: Image;\n  /**\n   * Image with MiDaS normal map\n   */\n  normal_map: Image;\n};\nexport type MiniCpmInput = {\n  /**\n   * List of image URLs to be used for the image description\n   */\n  image_urls: Array<string>;\n  /**\n   * Prompt to be used for the image description\n   */\n  prompt: string;\n};\nexport type MiniCpmOutput = {\n  /**\n   * Response from the model\n   */\n  output: string;\n};\nexport type MiniCPMV26ImageInput = {\n  /**\n   * List of image URLs to be used for the image description\n   */\n  image_urls: Array<string>;\n  /**\n   * Prompt to be used for the image description\n   */\n  prompt: string;\n};\nexport type MiniCPMV26VideoInput = {\n  /**\n   * URL of the video to be analyzed\n   */\n  video_url: string | Blob | File;\n  /**\n   * Prompt to be used for the video description\n   */\n  prompt: string;\n};\nexport type MiniCpmVideoInput = {\n  /**\n   * URL of the video to be analyzed\n   */\n  video_url: string | Blob | File;\n  /**\n   * Prompt to be used for the video description\n   */\n  prompt: string;\n};\nexport type MiniCpmVideoOutput = {\n  /**\n   * Response from the model\n   */\n  output: string;\n};\nexport type MinimaxImageInput = {\n  /**\n   * Text prompt for image generation (max 1500 characters)\n   */\n  prompt: string;\n  /**\n   * Aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"1:1\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"21:9\";\n  /**\n   * Number of images to generate (1-9) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Whether to enable automatic prompt optimization\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxImageOutput = {\n  /**\n   * Generated images\n   */\n  images: Array<File>;\n};\nexport type MinimaxMusicInput = {\n  /**\n   * Lyrics with optional formatting. You can use a newline to separate each line of lyrics. You can use two newlines to add a pause between lines. You can use double hash marks (##) at the beginning and end of the lyrics to add accompaniment. Maximum 600 characters.\n   */\n  prompt: string;\n  /**\n   * Reference song, should contain music and vocals. Must be a .wav or .mp3 file longer than 15 seconds.\n   */\n  reference_audio_url: string | Blob | File;\n};\nexport type MinimaxMusicOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type MinimaxVideo01DirectorImageToVideoInput = {\n  /**\n   * Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01DirectorImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVideo01DirectorInput = {\n  /**\n   * Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645\n   */\n  prompt: string;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01DirectorOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVideo01ImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVideo01Input = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01LiveImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01LiveImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVideo01LiveInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01LiveOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVideo01Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVideo01SubjectReferenceInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * URL of the subject reference image to use for consistent subject appearance\n   */\n  subject_reference_image_url: string | Blob | File;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01SubjectReferenceOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MLSDInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Score threshold for the MLSD detector Default value: `0.1`\n   */\n  score_threshold?: number;\n  /**\n   * Distance threshold for the MLSD detector Default value: `0.1`\n   */\n  distance_threshold?: number;\n};\nexport type MLSDOutput = {\n  /**\n   * Image with lines detected using the MLSD detector\n   */\n  image: Image;\n};\nexport type MmaudioV2Input = {\n  /**\n   * The URL of the video to generate the audio for.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The prompt to generate the audio for.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to generate the audio for. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The number of steps to generate the audio for. Default value: `25`\n   */\n  num_steps?: number;\n  /**\n   * The duration of the audio to generate. Default value: `8`\n   */\n  duration?: number;\n  /**\n   * The strength of Classifier Free Guidance. Default value: `4.5`\n   */\n  cfg_strength?: number;\n  /**\n   * Whether to mask away the clip.\n   */\n  mask_away_clip?: boolean;\n};\nexport type MmaudioV2Output = {\n  /**\n   * The generated video with the lip sync.\n   */\n  video: File;\n};\nexport type MmaudioV2TextToAudioInput = {\n  /**\n   * The prompt to generate the audio for.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to generate the audio for. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The number of steps to generate the audio for. Default value: `25`\n   */\n  num_steps?: number;\n  /**\n   * The duration of the audio to generate. Default value: `8`\n   */\n  duration?: number;\n  /**\n   * The strength of Classifier Free Guidance. Default value: `4.5`\n   */\n  cfg_strength?: number;\n  /**\n   * Whether to mask away the clip.\n   */\n  mask_away_clip?: boolean;\n};\nexport type MmaudioV2TextToAudioOutput = {\n  /**\n   * The generated audio.\n   */\n  audio: File;\n};\nexport type MochiV1Input = {\n  /**\n   * The prompt to generate a video from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt for the video. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * Whether to enable prompt expansion. Default value: `true`\n   */\n  enable_prompt_expansion?: boolean;\n};\nexport type MochiV1Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MoondreamBatchedInput = {\n  /**\n   * Model ID to use for inference Default value: `\"vikhyatk/moondream2\"`\n   */\n  model_id?: \"vikhyatk/moondream2\" | \"fal-ai/moondream2-docci\";\n  /**\n   * List of input prompts and image URLs\n   */\n  inputs: Array<MoondreamInputParam>;\n  /**\n   * Maximum number of new tokens to generate Default value: `64`\n   */\n  max_tokens?: number;\n  /**\n   * Temperature for sampling Default value: `0.2`\n   */\n  temperature?: number;\n  /**\n   * Top P for sampling Default value: `1`\n   */\n  top_p?: number;\n  /**\n   * Repetition penalty for sampling Default value: `1`\n   */\n  repetition_penalty?: number;\n};\nexport type MoondreamBatchedOutput = {\n  /**\n   * List of generated outputs\n   */\n  outputs: Array<string>;\n  /**\n   * Whether the output is partial\n   */\n  partial?: boolean;\n  /**\n   * Timings for different parts of the process\n   */\n  timings: any;\n  /**\n   * Filenames of the images processed\n   */\n  filenames?: Array<string>;\n};\nexport type MoondreamNextBatchInput = {\n  /**\n   * List of image URLs to be processed (maximum 32 images)\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Single prompt to apply to all images\n   */\n  prompt: string;\n  /**\n   * Maximum number of tokens to generate Default value: `64`\n   */\n  max_tokens?: number;\n};\nexport type MoondreamNextBatchOutput = {\n  /**\n   * URL to the generated captions JSON file containing filename-caption pairs.\n   */\n  captions_file: File;\n  /**\n   * List of generated captions\n   */\n  outputs: Array<string>;\n};\nexport type MoondreamNextDetectionInput = {\n  /**\n   * Image URL to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of detection to perform\n   */\n  task_type: \"bbox_detection\" | \"point_detection\" | \"gaze_detection\";\n  /**\n   * Text description of what to detect\n   */\n  detection_prompt: string;\n  /**\n   * Whether to use ensemble for gaze detection\n   */\n  use_ensemble?: boolean;\n};\nexport type MoondreamNextDetectionOutput = {\n  /**\n   * Output image with detection visualization\n   */\n  image: Image;\n  /**\n   * Detection results as text\n   */\n  text_output: string;\n};\nexport type MoondreamNextInput = {\n  /**\n   * Image URL to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of task to perform Default value: `\"caption\"`\n   */\n  task_type?: \"caption\" | \"query\";\n  /**\n   * Prompt for query task\n   */\n  prompt: string;\n  /**\n   * Maximum number of tokens to generate Default value: `64`\n   */\n  max_tokens?: number;\n};\nexport type MoondreamNextOutput = {\n  /**\n   * Response from the model\n   */\n  output: string;\n};\nexport type MoonDreamOutput = {\n  /**\n   * Response from the model\n   */\n  output: string;\n};\nexport type MultiConditioningVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * URL of images to use as conditioning Default value: ``\n   */\n  images?: Array<ImageConditioningInput>;\n  /**\n   * Videos to use as conditioning Default value: ``\n   */\n  videos?: Array<VideoConditioningInput>;\n};\nexport type MulticonditioningVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type MusePoseInput = {\n  /**\n   * URL of the image to animate.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the video to drive the animation\n   */\n  video_url: string | Blob | File;\n  /**\n   * The resolution to use for the pose detection. Default value: `512`\n   */\n  dwpose_detection_resolution?: number;\n  /**\n   * The resolution to use for the image during pose calculation. Default value: `720`\n   */\n  dwpose_image_resolution?: number;\n  /**\n   * The frame to align the pose to.\n   */\n  dwpose_align_frame?: number;\n  /**\n   * The width of the output video. Default value: `748`\n   */\n  width?: number;\n  /**\n   * The height of the output video. Default value: `748`\n   */\n  height?: number;\n  /**\n   * The length of the output video. Default value: `300`\n   */\n  length?: number;\n  /**\n   * The video slice frame number Default value: `48`\n   */\n  slice?: number;\n  /**\n   * The video slice overlap frame number Default value: `4`\n   */\n  overlap?: number;\n  /**\n   * Classifier free guidance Default value: `3.5`\n   */\n  cfg?: number;\n  /**\n   * The seed to use for the random number generator.\n   */\n  seed?: number;\n  /**\n   * DDIM sampling steps Default value: `20`\n   */\n  steps?: number;\n  /**\n   * The frames per second of the output video.\n   */\n  fps?: number;\n  /**\n   * Number of input frames to skip. Skipping 1 effectively reduces the fps in half. Default value: `1`\n   */\n  skip?: number;\n};\nexport type MusePoseOutput = {\n  /**\n   * The generated video with the lip sync.\n   */\n  video: File;\n};\nexport type MusetalkInput = {\n  /**\n   * URL of the source video\n   */\n  source_video_url: string | Blob | File;\n  /**\n   * URL of the audio\n   */\n  audio_url: string | Blob | File;\n};\nexport type MusetalkOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n};\nexport type NafnetDeblurInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n};\nexport type NafnetDeblurOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type NafnetDenoiseInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n};\nexport type NafnetDenoiseOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type NafnetInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n};\nexport type NafnetOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type NSFWImageDetectionInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n};\nexport type NSFWImageDetectionOutput = {\n  /**\n   * The probability of the image being NSFW.\n   */\n  nsfw_probability: number;\n};\nexport type OmnigenV1Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * URL of images to use while generating the image, Use <img><|image_1|></img> for the first image and so on. Default value: ``\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3`\n   */\n  guidance_scale?: number;\n  /**\n   * The Image Guidance scale is a measure of how close you want\n   * the model to stick to your input image when looking for a related image to show you. Default value: `1.6`\n   */\n  img_guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type OmnigenV1Output = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type OmniZeroInput = {\n  /**\n   * Prompt to guide the image generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt to guide the image generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Composition image url.\n   */\n  composition_image_url: string | Blob | File;\n  /**\n   * Style image url.\n   */\n  style_image_url: string | Blob | File;\n  /**\n   * Identity image url.\n   */\n  identity_image_url: string | Blob | File;\n  /**\n   * Image strength. Default value: `0.75`\n   */\n  image_strength?: number;\n  /**\n   * Composition strength. Default value: `1`\n   */\n  composition_strength?: number;\n  /**\n   * Depth strength. Default value: `0.5`\n   */\n  depth_strength?: number;\n  /**\n   * Style strength. Default value: `1`\n   */\n  style_strength?: number;\n  /**\n   * Face strength. Default value: `1`\n   */\n  face_strength?: number;\n  /**\n   * Identity strength. Default value: `1`\n   */\n  identity_strength?: number;\n  /**\n   * Guidance scale. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * Seed. Default value: `42`\n   */\n  seed?: number;\n  /**\n   * Number of images. Default value: `1`\n   */\n  number_of_images?: number;\n};\nexport type OmniZeroOutput = {\n  /**\n   * The generated image.\n   */\n  image: Image;\n};\nexport type Output = {\n  /**\n   * URL of the processed video file\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the video's thumbnail image\n   */\n  thumbnail_url: string | Blob | File;\n};\nexport type PhotoLoraI2IInput = {\n  /**\n   * LoRA Scale of the photo lora model Default value: `0.75`\n   */\n  photo_lora_scale?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type PhotoLoraInpaintInput = {\n  /**\n   * LoRA Scale of the photo lora model Default value: `0.75`\n   */\n  photo_lora_scale?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n  /**\n   * The mask to area to Inpaint in.\n   */\n  mask_url: string | Blob | File;\n};\nexport type PhotomakerInput = {\n  /**\n   * The URL of the image archive containing the images you want to use.\n   */\n  image_archive_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The base pipeline to use for generating the image. Default value: `\"photomaker\"`\n   */\n  base_pipeline?: \"photomaker\" | \"photomaker-style\";\n  /**\n   * Optional initial image for img2img\n   */\n  initial_image_url?: string | Blob | File;\n  /**\n   * How much noise to add to the latent image. O for no noise, 1 for maximum noise. Default value: `0.5`\n   */\n  initial_image_strength?: number;\n  /**\n   *  Default value: `\"Photographic\"`\n   */\n  style?:\n    | \"(No style)\"\n    | \"Cinematic\"\n    | \"Disney Character\"\n    | \"Digital Art\"\n    | \"Photographic\"\n    | \"Fantasy art\"\n    | \"Neonpunk\"\n    | \"Enhance\"\n    | \"Comic book\"\n    | \"Lowpoly\"\n    | \"Line art\";\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   *  Default value: `20`\n   */\n  style_strength?: number;\n  /**\n   * Number of images to generate in one request. Note that the higher the batch size,\n   * the longer it will take to generate the images. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n};\nexport type PhotomakerOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type PiDiInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the safe version of the Pidi detector\n   */\n  safe?: boolean;\n  /**\n   * Whether to use the scribble version of the Pidi detector\n   */\n  scribble?: boolean;\n  /**\n   * Whether to apply the filter to the image.\n   */\n  apply_filter?: boolean;\n};\nexport type PiDiOutput = {\n  /**\n   * Image with Pidi lines detected\n   */\n  image: Image;\n};\nexport type Pika22ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type Pika22TextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikadditionsOutput = {\n  /**\n   * The generated video with added objects/images\n   */\n  video: File;\n};\nexport type PikaffectsOutput = {\n  /**\n   * The generated video with applied effect\n   */\n  video: File;\n};\nexport type PikaswapsOutput = {\n  /**\n   * The generated video with swapped regions\n   */\n  video: File;\n};\nexport type PikaV15PikaffectsInput = {\n  /**\n   * URL of the input image\n   */\n  image_url: string | Blob | File;\n  /**\n   * The Pikaffect to apply\n   */\n  pikaffect:\n    | \"Cake-ify\"\n    | \"Crumble\"\n    | \"Crush\"\n    | \"Decapitate\"\n    | \"Deflate\"\n    | \"Dissolve\"\n    | \"Explode\"\n    | \"Eye-pop\"\n    | \"Inflate\"\n    | \"Levitate\"\n    | \"Melt\"\n    | \"Peel\"\n    | \"Poke\"\n    | \"Squish\"\n    | \"Ta-da\"\n    | \"Tear\";\n  /**\n   * Text prompt to guide the effect\n   */\n  prompt?: string;\n  /**\n   * Negative prompt to guide the model\n   */\n  negative_prompt?: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n};\nexport type PikaV15PikaffectsOutput = {\n  /**\n   * The generated video with applied effect\n   */\n  video: File;\n};\nexport type PikaV21ImageToVideoInput = {\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV21ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV21TextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\" | \"4:5\" | \"5:4\" | \"3:2\" | \"2:3\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV21TextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV22ImageToVideoInput = {\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV22ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV22PikascenesInput = {\n  /**\n   * List of images to use for video generation\n   */\n  images: Array<PikaImage>;\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\" | \"4:5\" | \"5:4\" | \"3:2\" | \"2:3\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n  /**\n   * Mode for integrating multiple images Default value: `\"creative\"`\n   */\n  ingredients_mode?: \"creative\" | \"precise\";\n};\nexport type PikaV22PikascenesOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV22TextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\" | \"4:5\" | \"5:4\" | \"3:2\" | \"2:3\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV22TextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV2TurboImageToVideoInput = {\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV2TurboImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV2TurboTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\" | \"4:5\" | \"5:4\" | \"3:2\" | \"2:3\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV2TurboTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixartSigmaInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style to apply to the image. Default value: `\"(No style)\"`\n   */\n  style?:\n    | \"(No style)\"\n    | \"Cinematic\"\n    | \"Photographic\"\n    | \"Anime\"\n    | \"Manga\"\n    | \"Digital Art\"\n    | \"Pixel art\"\n    | \"Fantasy art\"\n    | \"Neonpunk\"\n    | \"3D Model\";\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `35`\n   */\n  num_inference_steps?: number;\n  /**\n   * The scheduler to use for the model. Default value: `\"DPM-SOLVER\"`\n   */\n  scheduler?: \"DPM-SOLVER\" | \"SA-SOLVER\";\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n};\nexport type PixartSigmaOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   * The timings of the different steps of the generation process.\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type PixverseV35ImageToVideoFastInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n};\nexport type PixverseV35ImageToVideoFastOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV35ImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n};\nexport type PixverseV35ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV35TextToVideoFastInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n};\nexport type PixverseV35TextToVideoFastOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV35TextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n};\nexport type PixverseV35TextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PlayaiTtsDialogInput = {\n  /**\n   * The dialogue text with turn prefixes to distinguish speakers.\n   */\n  input: string;\n  /**\n   * A list of voice definitions for each speaker in the dialogue. Must be between 1 and 2 voices. Default value: `[object Object],[object Object]`\n   */\n  voices?: Array<LDMVoiceInput>;\n  /**\n   * S3 URI of the autoregressive (AR) model.\n   */\n  ar?: string;\n  /**\n   * S3 URI of the AR LoRA model.\n   */\n  lora?: string;\n  /**\n   * S3 URI of the vocoder model.\n   */\n  vocoder?: string;\n  /**\n   * The format of the response. Default value: `\"url\"`\n   */\n  response_format?: \"url\" | \"bytes\";\n  /**\n   * An integer number greater than or equal to 0. If equal to null or not provided, a random seed will be used. Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file.\n   */\n  seed?: number;\n};\nexport type PlayaiTtsDialogOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: AudioFile;\n};\nexport type PlayaiTtsV3Input = {\n  /**\n   * The text to be converted to speech.\n   */\n  input: string;\n  /**\n   * The unique ID of a PlayHT or Cloned Voice, or a name from the available presets.\n   */\n  voice: string;\n  /**\n   * The format of the response. Default value: `\"url\"`\n   */\n  response_format?: \"url\" | \"bytes\";\n  /**\n   * An integer number greater than or equal to 0. If equal to null or not provided, a random seed will be used. Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file.\n   */\n  seed?: number;\n};\nexport type PlayaiTtsV3Output = {\n  /**\n   * The generated audio file.\n   */\n  audio: AudioFile;\n};\nexport type PlaygroundV25ImageToImageInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n};\nexport type PlaygroundV25ImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type PlaygroundV25InpaintingInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n};\nexport type PlaygroundV25InpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type PlaygroundV25Input = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n};\nexport type PlaygroundV25Output = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type PolygonOutput = {\n  /**\n   * List of polygons\n   */\n  polygons: Array<Polygon>;\n};\nexport type PoseTransferInput = {\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same input given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your input when generating the image. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Url for the human image.\n   */\n  pose_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  person_image_url: string | Blob | File;\n};\nexport type PoseTransferOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n  /**\n   * The seed for the inference.\n   */\n  seed: number;\n  /**\n   * Whether the image contains NSFW concepts.\n   */\n  has_nsfw_concepts: boolean;\n};\nexport type PostProcessingInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Enable film grain effect\n   */\n  enable_grain?: boolean;\n  /**\n   * Film grain intensity (when enabled) Default value: `0.4`\n   */\n  grain_intensity?: number;\n  /**\n   * Film grain scale (when enabled) Default value: `10`\n   */\n  grain_scale?: number;\n  /**\n   * Style of film grain to apply Default value: `\"modern\"`\n   */\n  grain_style?:\n    | \"modern\"\n    | \"analog\"\n    | \"kodak\"\n    | \"fuji\"\n    | \"cinematic\"\n    | \"newspaper\";\n  /**\n   * Enable color correction\n   */\n  enable_color_correction?: boolean;\n  /**\n   * Color temperature adjustment\n   */\n  temperature?: number;\n  /**\n   * Brightness adjustment\n   */\n  brightness?: number;\n  /**\n   * Contrast adjustment\n   */\n  contrast?: number;\n  /**\n   * Saturation adjustment\n   */\n  saturation?: number;\n  /**\n   * Gamma adjustment Default value: `1`\n   */\n  gamma?: number;\n  /**\n   * Enable chromatic aberration\n   */\n  enable_chromatic?: boolean;\n  /**\n   * Red channel shift amount\n   */\n  red_shift?: number;\n  /**\n   * Red channel shift direction Default value: `\"horizontal\"`\n   */\n  red_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Green channel shift amount\n   */\n  green_shift?: number;\n  /**\n   * Green channel shift direction Default value: `\"horizontal\"`\n   */\n  green_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Blue channel shift amount\n   */\n  blue_shift?: number;\n  /**\n   * Blue channel shift direction Default value: `\"horizontal\"`\n   */\n  blue_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Enable blur effect\n   */\n  enable_blur?: boolean;\n  /**\n   * Type of blur to apply Default value: `\"gaussian\"`\n   */\n  blur_type?: \"gaussian\" | \"kuwahara\";\n  /**\n   * Blur radius Default value: `3`\n   */\n  blur_radius?: number;\n  /**\n   * Sigma for Gaussian blur Default value: `1`\n   */\n  blur_sigma?: number;\n  /**\n   * Enable vignette effect\n   */\n  enable_vignette?: boolean;\n  /**\n   * Vignette strength (when enabled) Default value: `0.5`\n   */\n  vignette_strength?: number;\n  /**\n   * Enable parabolize effect\n   */\n  enable_parabolize?: boolean;\n  /**\n   * Parabolize coefficient Default value: `1`\n   */\n  parabolize_coeff?: number;\n  /**\n   * Vertex X position Default value: `0.5`\n   */\n  vertex_x?: number;\n  /**\n   * Vertex Y position Default value: `0.5`\n   */\n  vertex_y?: number;\n  /**\n   * Enable color tint effect\n   */\n  enable_tint?: boolean;\n  /**\n   * Tint strength Default value: `1`\n   */\n  tint_strength?: number;\n  /**\n   * Tint color mode Default value: `\"sepia\"`\n   */\n  tint_mode?:\n    | \"sepia\"\n    | \"red\"\n    | \"green\"\n    | \"blue\"\n    | \"cyan\"\n    | \"magenta\"\n    | \"yellow\"\n    | \"purple\"\n    | \"orange\"\n    | \"warm\"\n    | \"cool\"\n    | \"lime\"\n    | \"navy\"\n    | \"vintage\"\n    | \"rose\"\n    | \"teal\"\n    | \"maroon\"\n    | \"peach\"\n    | \"lavender\"\n    | \"olive\";\n  /**\n   * Enable dissolve effect\n   */\n  enable_dissolve?: boolean;\n  /**\n   * URL of second image for dissolve Default value: `\"\"`\n   */\n  dissolve_image_url?: string | Blob | File;\n  /**\n   * Dissolve blend factor Default value: `0.5`\n   */\n  dissolve_factor?: number;\n  /**\n   * Enable dodge and burn effect\n   */\n  enable_dodge_burn?: boolean;\n  /**\n   * Dodge and burn intensity Default value: `0.5`\n   */\n  dodge_burn_intensity?: number;\n  /**\n   * Dodge and burn mode Default value: `\"dodge\"`\n   */\n  dodge_burn_mode?:\n    | \"dodge\"\n    | \"burn\"\n    | \"dodge_and_burn\"\n    | \"burn_and_dodge\"\n    | \"color_dodge\"\n    | \"color_burn\"\n    | \"linear_dodge\"\n    | \"linear_burn\";\n  /**\n   * Enable glow effect\n   */\n  enable_glow?: boolean;\n  /**\n   * Glow intensity Default value: `1`\n   */\n  glow_intensity?: number;\n  /**\n   * Glow blur radius Default value: `5`\n   */\n  glow_radius?: number;\n  /**\n   * Enable sharpen effect\n   */\n  enable_sharpen?: boolean;\n  /**\n   * Type of sharpening to apply Default value: `\"basic\"`\n   */\n  sharpen_mode?: \"basic\" | \"smart\" | \"cas\";\n  /**\n   * Sharpen radius (for basic mode) Default value: `1`\n   */\n  sharpen_radius?: number;\n  /**\n   * Sharpen strength (for basic mode) Default value: `1`\n   */\n  sharpen_alpha?: number;\n  /**\n   * Noise radius for smart sharpen Default value: `7`\n   */\n  noise_radius?: number;\n  /**\n   * Edge preservation factor Default value: `0.75`\n   */\n  preserve_edges?: number;\n  /**\n   * Smart sharpen strength Default value: `5`\n   */\n  smart_sharpen_strength?: number;\n  /**\n   * Smart sharpen blend ratio Default value: `0.5`\n   */\n  smart_sharpen_ratio?: number;\n  /**\n   * CAS sharpening amount Default value: `0.8`\n   */\n  cas_amount?: number;\n  /**\n   * Enable solarize effect\n   */\n  enable_solarize?: boolean;\n  /**\n   * Solarize threshold Default value: `0.5`\n   */\n  solarize_threshold?: number;\n  /**\n   * Enable desaturation effect\n   */\n  enable_desaturate?: boolean;\n  /**\n   * Desaturation factor Default value: `1`\n   */\n  desaturate_factor?: number;\n  /**\n   * Desaturation method Default value: `\"luminance (Rec.709)\"`\n   */\n  desaturate_method?:\n    | \"luminance (Rec.709)\"\n    | \"luminance (Rec.601)\"\n    | \"average\"\n    | \"lightness\";\n};\nexport type PostProcessingOutput = {\n  /**\n   * The processed images\n   */\n  images: Array<Image>;\n};\nexport type ProductShotInput = {\n  /**\n   * The URL of the product shot to be placed in a lifestyle shot. If both image_url and image_file are provided, image_url will be used. Accepted formats are jpeg, jpg, png, webp. Maximum file size 12MB.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text description of the new scene or background for the provided product shot. Bria currently supports prompts in English only, excluding special characters.\n   */\n  scene_description?: string;\n  /**\n   * The URL of the reference image to be used for generating the new scene or background for the product shot. Use \"\" to leave empty.Either ref_image_url or scene_description has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp. Default value: `\"\"`\n   */\n  ref_image_url?: string | Blob | File;\n  /**\n   * Whether to optimize the scene description Default value: `true`\n   */\n  optimize_description?: boolean;\n  /**\n   * The number of lifestyle product shots you would like to generate. You will get num_results x 10 results when placement_type=automatic and according to the number of required placements x num_results if placement_type=manual_placement. Default value: `1`\n   */\n  num_results?: number;\n  /**\n   * Whether to use the fast model Default value: `true`\n   */\n  fast?: boolean;\n  /**\n   * This parameter allows you to control the positioning of the product in the image. Choosing 'original' will preserve the original position of the product in the image. Choosing 'automatic' will generate results with the 10 recommended positions for the product. Choosing 'manual_placement' will allow you to select predefined positions (using the parameter 'manual_placement_selection'). Selecting 'manual_padding' will allow you to control the position and size of the image by defining the desired padding in pixels around the product. Default value: `\"manual_placement\"`\n   */\n  placement_type?:\n    | \"original\"\n    | \"automatic\"\n    | \"manual_placement\"\n    | \"manual_padding\";\n  /**\n   * This flag is only relevant when placement_type=original. If true, the output image retains the original input image's size; otherwise, the image is scaled to 1 megapixel (1MP) while preserving its aspect ratio.\n   */\n  original_quality?: boolean;\n  /**\n   * The desired size of the final product shot. For optimal results, the total number of pixels should be around 1,000,000. This parameter is only relevant when placement_type=automatic or placement_type=manual_placement. Default value: `1000,1000`\n   */\n  shot_size?: Array<number>;\n  /**\n   * If you've selected placement_type=manual_placement, you should use this parameter to specify which placements/positions you would like to use from the list. You can select more than one placement in one request. Default value: `\"bottom_center\"`\n   */\n  manual_placement_selection?:\n    | \"upper_left\"\n    | \"upper_right\"\n    | \"bottom_left\"\n    | \"bottom_right\"\n    | \"right_center\"\n    | \"left_center\"\n    | \"upper_center\"\n    | \"bottom_center\"\n    | \"center_vertical\"\n    | \"center_horizontal\";\n  /**\n   * The desired padding in pixels around the product, when using placement_type=manual_padding. The order of the values is [left, right, top, bottom]. For optimal results, the total number of pixels, including padding, should be around 1,000,000. It is recommended to first use the product cutout API, get the cutout and understand the size of the result, and then define the required padding and use the cutout as an input for this API.\n   */\n  padding_values?: Array<number>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ProductShotOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n};\nexport type PulidInput = {\n  /**\n   * List of reference faces, ideally 4 images.\n   */\n  reference_images: Array<ReferenceFace>;\n  /**\n   * Prompt to generate the face from\n   */\n  prompt: string;\n  /**\n   * Negative prompt to generate the face from Default value: `\"flaws in the eyes, flaws in the face, flaws, lowres, non-HDRi, low quality, worst quality,artifacts noise, text, watermark, glitch, deformed, mutated, ugly, disfigured, hands, low resolution, partially rendered objects,  deformed or partially rendered eyes, deformed, deformed eyeballs, cross-eyed,blurry\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of images to generate Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Guidance scale Default value: `1.2`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of steps to take Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * Random seed for reproducibility\n   */\n  seed?: number;\n  /**\n   * Size of the generated image Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * ID scale Default value: `0.8`\n   */\n  id_scale?: number;\n  /**\n   * Mode of generation Default value: `\"fidelity\"`\n   */\n  mode?: \"fidelity\" | \"extreme style\";\n  /**\n   * if you want to mix two ID image, please turn this on, otherwise, turn this off\n   */\n  id_mix?: boolean;\n};\nexport type PulidOutput = {\n  /**\n   * List of generated images\n   */\n  images: Array<Image>;\n  /**\n   * Random seed used for reproducibility\n   */\n  seed: number;\n};\nexport type QueryInput = {\n  /**\n   * Image URL to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of task to perform Default value: `\"caption\"`\n   */\n  task_type?: \"caption\" | \"query\";\n  /**\n   * Prompt for query task\n   */\n  prompt: string;\n  /**\n   * Maximum number of tokens to generate Default value: `64`\n   */\n  max_tokens?: number;\n};\nexport type Ray2I2VOutput = {\n  /**\n   * URL of the generated video\n   */\n  video: File;\n};\nexport type Ray2T2VOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type RealisticVisionImageToImageInput = {\n  /**\n   * The Realistic Vision model to use.\n   */\n  model_name?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   *  Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n};\nexport type RealisticVisionInpaintingInput = {\n  /**\n   * The Realistic Vision model to use.\n   */\n  model_name?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   *  Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n};\nexport type RealisticVisionInput = {\n  /**\n   * The Realistic Vision model to use.\n   */\n  model_name?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want in the image. Default value: `\"(worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)\"`\n   */\n  negative_prompt?: string;\n  /**\n   *  Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `35`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n};\nexport type RealisticVisionOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Recraft20bInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *  Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The style of the generated images. Vector images cost 2X as much. Default value: `\"realistic_image\"`\n   */\n  style?:\n    | \"any\"\n    | \"realistic_image\"\n    | \"digital_illustration\"\n    | \"vector_illustration\"\n    | \"realistic_image/b_and_w\"\n    | \"realistic_image/enterprise\"\n    | \"realistic_image/hard_flash\"\n    | \"realistic_image/hdr\"\n    | \"realistic_image/motion_blur\"\n    | \"realistic_image/natural_light\"\n    | \"realistic_image/studio_portrait\"\n    | \"digital_illustration/2d_art_poster\"\n    | \"digital_illustration/2d_art_poster_2\"\n    | \"digital_illustration/3d\"\n    | \"digital_illustration/80s\"\n    | \"digital_illustration/engraving_color\"\n    | \"digital_illustration/glow\"\n    | \"digital_illustration/grain\"\n    | \"digital_illustration/hand_drawn\"\n    | \"digital_illustration/hand_drawn_outline\"\n    | \"digital_illustration/handmade_3d\"\n    | \"digital_illustration/infantile_sketch\"\n    | \"digital_illustration/kawaii\"\n    | \"digital_illustration/pixel_art\"\n    | \"digital_illustration/psychedelic\"\n    | \"digital_illustration/seamless\"\n    | \"digital_illustration/voxel\"\n    | \"digital_illustration/watercolor\"\n    | \"vector_illustration/cartoon\"\n    | \"vector_illustration/doodle_line_art\"\n    | \"vector_illustration/engraving\"\n    | \"vector_illustration/flat_2\"\n    | \"vector_illustration/kawaii\"\n    | \"vector_illustration/line_art\"\n    | \"vector_illustration/line_circuit\"\n    | \"vector_illustration/linocut\"\n    | \"vector_illustration/seamless\"\n    | \"icon/broken_line\"\n    | \"icon/colored_outline\"\n    | \"icon/colored_shapes\"\n    | \"icon/colored_shapes_gradient\"\n    | \"icon/doodle_fill\"\n    | \"icon/doodle_offset_fill\"\n    | \"icon/offset_fill\"\n    | \"icon/outline\"\n    | \"icon/outline_gradient\"\n    | \"icon/uneven_fill\";\n  /**\n   * An array of preferable colors Default value: ``\n   */\n  colors?: Array<RGBColor>;\n  /**\n   * The ID of the custom style reference (optional)\n   */\n  style_id?: string;\n};\nexport type Recraft20bOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n};\nexport type RecraftClarityUpscaleInput = {\n  /**\n   * The URL of the image to be upscaled. Must be in PNG format.\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type RecraftClarityUpscaleOutput = {\n  /**\n   * The upscaled image.\n   */\n  image: Image;\n};\nexport type RecraftCreativeUpscaleInput = {\n  /**\n   * The URL of the image to be upscaled. Must be in PNG format.\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type RecraftCreativeUpscaleOutput = {\n  /**\n   * The upscaled image.\n   */\n  image: Image;\n};\nexport type RecraftV3CreateStyleInput = {\n  /**\n   * URL to zip archive with images, use PNG format. Maximum 5 images are allowed.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * The base style of the generated images, this topic is covered above. Default value: `\"digital_illustration\"`\n   */\n  base_style?:\n    | \"any\"\n    | \"realistic_image\"\n    | \"digital_illustration\"\n    | \"vector_illustration\"\n    | \"realistic_image/b_and_w\"\n    | \"realistic_image/hard_flash\"\n    | \"realistic_image/hdr\"\n    | \"realistic_image/natural_light\"\n    | \"realistic_image/studio_portrait\"\n    | \"realistic_image/enterprise\"\n    | \"realistic_image/motion_blur\"\n    | \"realistic_image/evening_light\"\n    | \"realistic_image/faded_nostalgia\"\n    | \"realistic_image/forest_life\"\n    | \"realistic_image/mystic_naturalism\"\n    | \"realistic_image/natural_tones\"\n    | \"realistic_image/organic_calm\"\n    | \"realistic_image/real_life_glow\"\n    | \"realistic_image/retro_realism\"\n    | \"realistic_image/retro_snapshot\"\n    | \"realistic_image/urban_drama\"\n    | \"realistic_image/village_realism\"\n    | \"realistic_image/warm_folk\"\n    | \"digital_illustration/pixel_art\"\n    | \"digital_illustration/hand_drawn\"\n    | \"digital_illustration/grain\"\n    | \"digital_illustration/infantile_sketch\"\n    | \"digital_illustration/2d_art_poster\"\n    | \"digital_illustration/handmade_3d\"\n    | \"digital_illustration/hand_drawn_outline\"\n    | \"digital_illustration/engraving_color\"\n    | \"digital_illustration/2d_art_poster_2\"\n    | \"digital_illustration/antiquarian\"\n    | \"digital_illustration/bold_fantasy\"\n    | \"digital_illustration/child_book\"\n    | \"digital_illustration/child_books\"\n    | \"digital_illustration/cover\"\n    | \"digital_illustration/crosshatch\"\n    | \"digital_illustration/digital_engraving\"\n    | \"digital_illustration/expressionism\"\n    | \"digital_illustration/freehand_details\"\n    | \"digital_illustration/grain_20\"\n    | \"digital_illustration/graphic_intensity\"\n    | \"digital_illustration/hard_comics\"\n    | \"digital_illustration/long_shadow\"\n    | \"digital_illustration/modern_folk\"\n    | \"digital_illustration/multicolor\"\n    | \"digital_illustration/neon_calm\"\n    | \"digital_illustration/noir\"\n    | \"digital_illustration/nostalgic_pastel\"\n    | \"digital_illustration/outline_details\"\n    | \"digital_illustration/pastel_gradient\"\n    | \"digital_illustration/pastel_sketch\"\n    | \"digital_illustration/pop_art\"\n    | \"digital_illustration/pop_renaissance\"\n    | \"digital_illustration/street_art\"\n    | \"digital_illustration/tablet_sketch\"\n    | \"digital_illustration/urban_glow\"\n    | \"digital_illustration/urban_sketching\"\n    | \"digital_illustration/vanilla_dreams\"\n    | \"digital_illustration/young_adult_book\"\n    | \"digital_illustration/young_adult_book_2\"\n    | \"vector_illustration/bold_stroke\"\n    | \"vector_illustration/chemistry\"\n    | \"vector_illustration/colored_stencil\"\n    | \"vector_illustration/contour_pop_art\"\n    | \"vector_illustration/cosmics\"\n    | \"vector_illustration/cutout\"\n    | \"vector_illustration/depressive\"\n    | \"vector_illustration/editorial\"\n    | \"vector_illustration/emotional_flat\"\n    | \"vector_illustration/infographical\"\n    | \"vector_illustration/marker_outline\"\n    | \"vector_illustration/mosaic\"\n    | \"vector_illustration/naivector\"\n    | \"vector_illustration/roundish_flat\"\n    | \"vector_illustration/segmented_colors\"\n    | \"vector_illustration/sharp_contrast\"\n    | \"vector_illustration/thin\"\n    | \"vector_illustration/vector_photo\"\n    | \"vector_illustration/vivid_shapes\"\n    | \"vector_illustration/engraving\"\n    | \"vector_illustration/line_art\"\n    | \"vector_illustration/line_circuit\"\n    | \"vector_illustration/linocut\";\n};\nexport type RecraftV3CreateStyleOutput = {\n  /**\n   * The ID of the created style, this ID can be used to reference the style in the future.\n   */\n  style_id: string;\n};\nexport type RecraftV3Input = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *  Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The style of the generated images. Vector images cost 2X as much. Default value: `\"realistic_image\"`\n   */\n  style?:\n    | \"any\"\n    | \"realistic_image\"\n    | \"digital_illustration\"\n    | \"vector_illustration\"\n    | \"realistic_image/b_and_w\"\n    | \"realistic_image/hard_flash\"\n    | \"realistic_image/hdr\"\n    | \"realistic_image/natural_light\"\n    | \"realistic_image/studio_portrait\"\n    | \"realistic_image/enterprise\"\n    | \"realistic_image/motion_blur\"\n    | \"realistic_image/evening_light\"\n    | \"realistic_image/faded_nostalgia\"\n    | \"realistic_image/forest_life\"\n    | \"realistic_image/mystic_naturalism\"\n    | \"realistic_image/natural_tones\"\n    | \"realistic_image/organic_calm\"\n    | \"realistic_image/real_life_glow\"\n    | \"realistic_image/retro_realism\"\n    | \"realistic_image/retro_snapshot\"\n    | \"realistic_image/urban_drama\"\n    | \"realistic_image/village_realism\"\n    | \"realistic_image/warm_folk\"\n    | \"digital_illustration/pixel_art\"\n    | \"digital_illustration/hand_drawn\"\n    | \"digital_illustration/grain\"\n    | \"digital_illustration/infantile_sketch\"\n    | \"digital_illustration/2d_art_poster\"\n    | \"digital_illustration/handmade_3d\"\n    | \"digital_illustration/hand_drawn_outline\"\n    | \"digital_illustration/engraving_color\"\n    | \"digital_illustration/2d_art_poster_2\"\n    | \"digital_illustration/antiquarian\"\n    | \"digital_illustration/bold_fantasy\"\n    | \"digital_illustration/child_book\"\n    | \"digital_illustration/child_books\"\n    | \"digital_illustration/cover\"\n    | \"digital_illustration/crosshatch\"\n    | \"digital_illustration/digital_engraving\"\n    | \"digital_illustration/expressionism\"\n    | \"digital_illustration/freehand_details\"\n    | \"digital_illustration/grain_20\"\n    | \"digital_illustration/graphic_intensity\"\n    | \"digital_illustration/hard_comics\"\n    | \"digital_illustration/long_shadow\"\n    | \"digital_illustration/modern_folk\"\n    | \"digital_illustration/multicolor\"\n    | \"digital_illustration/neon_calm\"\n    | \"digital_illustration/noir\"\n    | \"digital_illustration/nostalgic_pastel\"\n    | \"digital_illustration/outline_details\"\n    | \"digital_illustration/pastel_gradient\"\n    | \"digital_illustration/pastel_sketch\"\n    | \"digital_illustration/pop_art\"\n    | \"digital_illustration/pop_renaissance\"\n    | \"digital_illustration/street_art\"\n    | \"digital_illustration/tablet_sketch\"\n    | \"digital_illustration/urban_glow\"\n    | \"digital_illustration/urban_sketching\"\n    | \"digital_illustration/vanilla_dreams\"\n    | \"digital_illustration/young_adult_book\"\n    | \"digital_illustration/young_adult_book_2\"\n    | \"vector_illustration/bold_stroke\"\n    | \"vector_illustration/chemistry\"\n    | \"vector_illustration/colored_stencil\"\n    | \"vector_illustration/contour_pop_art\"\n    | \"vector_illustration/cosmics\"\n    | \"vector_illustration/cutout\"\n    | \"vector_illustration/depressive\"\n    | \"vector_illustration/editorial\"\n    | \"vector_illustration/emotional_flat\"\n    | \"vector_illustration/infographical\"\n    | \"vector_illustration/marker_outline\"\n    | \"vector_illustration/mosaic\"\n    | \"vector_illustration/naivector\"\n    | \"vector_illustration/roundish_flat\"\n    | \"vector_illustration/segmented_colors\"\n    | \"vector_illustration/sharp_contrast\"\n    | \"vector_illustration/thin\"\n    | \"vector_illustration/vector_photo\"\n    | \"vector_illustration/vivid_shapes\"\n    | \"vector_illustration/engraving\"\n    | \"vector_illustration/line_art\"\n    | \"vector_illustration/line_circuit\"\n    | \"vector_illustration/linocut\";\n  /**\n   * An array of preferable colors Default value: ``\n   */\n  colors?: Array<RGBColor>;\n  /**\n   * The ID of the custom style reference (optional)\n   */\n  style_id?: string;\n};\nexport type RecraftV3Output = {\n  /**\n   *\n   */\n  images: Array<File>;\n};\nexport type ReferenceToVideoOutput = {\n  /**\n   * The generated video with consistent subjects from reference images\n   */\n  video: File;\n};\nexport type RegexReplaceInput = {\n  /**\n   * Input text\n   */\n  text: string;\n  /**\n   * Pattern to replace\n   */\n  pattern: string;\n  /**\n   * Replacement text\n   */\n  replace: string;\n};\nexport type ReimagineInput = {\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The URL of the structure reference image. Use \"\" to leave empty. Accepted formats are jpeg, jpg, png, webp. Default value: `\"\"`\n   */\n  structure_image_url?: string | Blob | File;\n  /**\n   * The influence of the structure reference on the generated image. Default value: `0.75`\n   */\n  structure_ref_influence?: number;\n  /**\n   * How many images you would like to generate. When using any Guidance Method, Value is set to 1. Default value: `1`\n   */\n  num_results?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Whether to use the fast model Default value: `true`\n   */\n  fast?: boolean;\n  /**\n   * The number of iterations the model goes through to refine the generated image. This parameter is optional. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ReimagineOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type RemeshingInput = {\n  /**\n   * Path for the object file to be remeshed.\n   */\n  object_url: string | Blob | File;\n  /**\n   * Output format for the 3D model. Default value: `\"glb\"`\n   */\n  output_format?: \"glb\" | \"fbx\" | \"obj\" | \"stl\" | \"usdc\";\n  /**\n   * Number of faces for remesh Default value: `5000`\n   */\n  faces?: number;\n  /**\n   * Merge duplicate vertices before exporting Default value: `true`\n   */\n  merge?: boolean;\n  /**\n   * Preserve UVs during remeshing Default value: `true`\n   */\n  preserve_uvs?: boolean;\n};\nexport type RemixImageInput = {\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n};\nexport type RemoveBackgroundInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the resulting image be cropped to a bounding box around the subject\n   */\n  crop_to_bbox?: boolean;\n};\nexport type RemoveBackgroundOutput = {\n  /**\n   * Background removed image.\n   */\n  image: Image;\n};\nexport type ResizeImageInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Width of the resized image\n   */\n  width: number;\n  /**\n   * Height of the resized image\n   */\n  height: number;\n  /**\n   * Resizing mode\n   */\n  mode: \"crop\" | \"pad\" | \"scale\";\n  /**\n   * Resizing strategy. Only used when mode is 'scale', default is nearest Default value: `\"nearest\"`\n   */\n  resampling?: \"nearest\" | \"bilinear\" | \"bicubic\" | \"lanczos\";\n  /**\n   * Proportions of the image. Only used when mode is 'scale', default is fit Default value: `\"fit\"`\n   */\n  scaling_proportions?: \"fit\" | \"fill\" | \"stretch\";\n  /**\n   * Position of cropping. Only used when mode is 'crop', default is center Default value: `\"center\"`\n   */\n  cropping_position?:\n    | \"center\"\n    | \"top_left\"\n    | \"top_right\"\n    | \"bottom_left\"\n    | \"bottom_right\";\n  /**\n   * Color of padding. Only used when mode is 'pad', default is black Default value: `\"black\"`\n   */\n  padding_color?: \"black\" | \"white\" | \"red\" | \"green\" | \"blue\";\n};\nexport type ResizeToPixelsInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Maximum number of pixels in the output image. Default value: `1000000`\n   */\n  max_pixels?: number;\n  /**\n   * If set, the output dimensions will be divisible by this value.\n   */\n  enforce_divisibility?: number;\n};\nexport type RetoucherInput = {\n  /**\n   * The URL of the image to be retouched.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Seed for reproducibility. Different seeds will make slightly different results.\n   */\n  seed?: number;\n};\nexport type RetoucherOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n  /**\n   * The seed used for the generation.\n   */\n  seed: number;\n};\nexport type RFInversionInput = {\n  /**\n   * The prompt to edit the image with\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  control_loras?: Array<ControlLoraWeight>;\n  /**\n   * The controlnets to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment. Default value: ``\n   */\n  controlnet_unions?: Array<ControlNetUnion>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of Image for Reference-Only\n   */\n  reference_image_url?: string | Blob | File;\n  /**\n   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`\n   */\n  reference_strength?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to bestarted.\n   */\n  reference_start?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`\n   */\n  reference_end?: number;\n  /**\n   * Base shift for the scheduled timesteps Default value: `0.5`\n   */\n  base_shift?: number;\n  /**\n   * Max shift for the scheduled timesteps Default value: `1.15`\n   */\n  max_shift?: number;\n  /**\n   * Specifies whether beta sigmas ought to be used.\n   */\n  use_beta_schedule?: boolean;\n  /**\n   * URL of image to be edited\n   */\n  image_url: string | Blob | File;\n  /**\n   * The controller guidance (gamma) used in the creation of structured noise. Default value: `0.6`\n   */\n  controller_guidance_forward?: number;\n  /**\n   * The controller guidance (eta) used in the denoising process.Using values closer to 1 will result in an image closer to input. Default value: `0.75`\n   */\n  controller_guidance_reverse?: number;\n  /**\n   * Timestep to start guidance during reverse process.\n   */\n  reverse_guidance_start?: number;\n  /**\n   * Timestep to stop guidance during reverse process. Default value: `8`\n   */\n  reverse_guidance_end?: number;\n  /**\n   * Scheduler for applying reverse guidance. Default value: `\"constant\"`\n   */\n  reverse_guidance_schedule?:\n    | \"constant\"\n    | \"linear_increase\"\n    | \"linear_decrease\";\n};\nexport type RGBAToRGBImageInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Color to replace the transparent pixels with\n   */\n  transparent_color: Color;\n};\nexport type RundiffusionPhotoFluxInput = {\n  /**\n   * LoRA Scale of the photo lora model Default value: `0.75`\n   */\n  photo_lora_scale?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type RundiffusionPhotoFluxOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Sa2va4bImageInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * Url for the Input image.\n   */\n  image_url: string | Blob | File;\n};\nexport type Sa2va4bImageOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Dictionary of label: mask image\n   */\n  masks: Array<Image>;\n};\nexport type Sa2va4bVideoInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * The URL of the input video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Number of frames to sample from the video. If not provided, all frames are sampled.\n   */\n  num_frames_to_sample?: number;\n};\nexport type Sa2va4bVideoOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Dictionary of label: mask image\n   */\n  masks: Array<Image>;\n};\nexport type Sa2va8bImageInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * Url for the Input image.\n   */\n  image_url: string | Blob | File;\n};\nexport type Sa2va8bImageOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Dictionary of label: mask image\n   */\n  masks: Array<Image>;\n};\nexport type Sa2va8bVideoInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * The URL of the input video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Number of frames to sample from the video. If not provided, all frames are sampled.\n   */\n  num_frames_to_sample?: number;\n};\nexport type Sa2va8bVideoOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Dictionary of label: mask image\n   */\n  masks: Array<Image>;\n};\nexport type SadtalkerInput = {\n  /**\n   * URL of the source image\n   */\n  source_image_url: string | Blob | File;\n  /**\n   * URL of the driven audio\n   */\n  driven_audio_url: string | Blob | File;\n  /**\n   * The style of the pose\n   */\n  pose_style?: number;\n  /**\n   * The resolution of the face model Default value: `\"256\"`\n   */\n  face_model_resolution?: \"256\" | \"512\";\n  /**\n   * The scale of the expression Default value: `1`\n   */\n  expression_scale?: number;\n  /**\n   * The type of face enhancer to use\n   */\n  face_enhancer?: \"gfpgan\";\n  /**\n   * Whether to use still mode. Fewer head motion, works with preprocess `full`.\n   */\n  still_mode?: boolean;\n  /**\n   * The type of preprocessing to use Default value: `\"crop\"`\n   */\n  preprocess?: \"crop\" | \"extcrop\" | \"resize\" | \"full\" | \"extfull\";\n};\nexport type SadTalkerInput = {\n  /**\n   * URL of the source image\n   */\n  source_image_url: string | Blob | File;\n  /**\n   * URL of the driven audio\n   */\n  driven_audio_url: string | Blob | File;\n  /**\n   * The style of the pose\n   */\n  pose_style?: number;\n  /**\n   * The resolution of the face model Default value: `\"256\"`\n   */\n  face_model_resolution?: \"256\" | \"512\";\n  /**\n   * The scale of the expression Default value: `1`\n   */\n  expression_scale?: number;\n  /**\n   * The type of face enhancer to use\n   */\n  face_enhancer?: \"gfpgan\";\n  /**\n   * Whether to use still mode. Fewer head motion, works with preprocess `full`.\n   */\n  still_mode?: boolean;\n  /**\n   * The type of preprocessing to use Default value: `\"crop\"`\n   */\n  preprocess?: \"crop\" | \"extcrop\" | \"resize\" | \"full\" | \"extfull\";\n};\nexport type SadtalkerOutput = {\n  /**\n   * URL of the generated video\n   */\n  video: File;\n};\nexport type SadtalkerReferenceInput = {\n  /**\n   * URL of the source image\n   */\n  source_image_url: string | Blob | File;\n  /**\n   * URL of the driven audio\n   */\n  driven_audio_url: string | Blob | File;\n  /**\n   * URL of the reference video\n   */\n  reference_pose_video_url: string | Blob | File;\n  /**\n   * The style of the pose\n   */\n  pose_style?: number;\n  /**\n   * The resolution of the face model Default value: `\"256\"`\n   */\n  face_model_resolution?: \"256\" | \"512\";\n  /**\n   * The scale of the expression Default value: `1`\n   */\n  expression_scale?: number;\n  /**\n   * The type of face enhancer to use\n   */\n  face_enhancer?: \"gfpgan\";\n  /**\n   * Whether to use still mode. Fewer head motion, works with preprocess `full`.\n   */\n  still_mode?: boolean;\n  /**\n   * The type of preprocessing to use Default value: `\"crop\"`\n   */\n  preprocess?: \"crop\" | \"extcrop\" | \"resize\" | \"full\" | \"extfull\";\n};\nexport type SadtalkerReferenceOutput = {\n  /**\n   * URL of the generated video\n   */\n  video: File;\n};\nexport type SadTalkerRefVideoInput = {\n  /**\n   * URL of the source image\n   */\n  source_image_url: string | Blob | File;\n  /**\n   * URL of the driven audio\n   */\n  driven_audio_url: string | Blob | File;\n  /**\n   * URL of the reference video\n   */\n  reference_pose_video_url: string | Blob | File;\n  /**\n   * The style of the pose\n   */\n  pose_style?: number;\n  /**\n   * The resolution of the face model Default value: `\"256\"`\n   */\n  face_model_resolution?: \"256\" | \"512\";\n  /**\n   * The scale of the expression Default value: `1`\n   */\n  expression_scale?: number;\n  /**\n   * The type of face enhancer to use\n   */\n  face_enhancer?: \"gfpgan\";\n  /**\n   * Whether to use still mode. Fewer head motion, works with preprocess `full`.\n   */\n  still_mode?: boolean;\n  /**\n   * The type of preprocessing to use Default value: `\"crop\"`\n   */\n  preprocess?: \"crop\" | \"extcrop\" | \"resize\" | \"full\" | \"extfull\";\n};\nexport type SAM2AutomaticSegmentationInput = {\n  /**\n   * URL of the image to be automatically segmented\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Number of points to sample along each side of the image. Default value: `32`\n   */\n  points_per_side?: number;\n  /**\n   * Threshold for predicted IOU score. Default value: `0.88`\n   */\n  pred_iou_thresh?: number;\n  /**\n   * Threshold for stability score. Default value: `0.95`\n   */\n  stability_score_thresh?: number;\n  /**\n   * Minimum area of a mask region. Default value: `100`\n   */\n  min_mask_region_area?: number;\n};\nexport type SAM2AutomaticSegmentationOutput = {\n  /**\n   * Combined segmentation mask.\n   */\n  combined_mask: Image;\n  /**\n   * Individual segmentation masks.\n   */\n  individual_masks: Array<Image>;\n};\nexport type Sam2AutoSegmentInput = {\n  /**\n   * URL of the image to be automatically segmented\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Number of points to sample along each side of the image. Default value: `32`\n   */\n  points_per_side?: number;\n  /**\n   * Threshold for predicted IOU score. Default value: `0.88`\n   */\n  pred_iou_thresh?: number;\n  /**\n   * Threshold for stability score. Default value: `0.95`\n   */\n  stability_score_thresh?: number;\n  /**\n   * Minimum area of a mask region. Default value: `100`\n   */\n  min_mask_region_area?: number;\n};\nexport type Sam2AutoSegmentOutput = {\n  /**\n   * Combined segmentation mask.\n   */\n  combined_mask: Image;\n  /**\n   * Individual segmentation masks.\n   */\n  individual_masks: Array<Image>;\n};\nexport type Sam2ImageInput = {\n  /**\n   * URL of the image to be segmented\n   */\n  image_url: string | Blob | File;\n  /**\n   * List of prompts to segment the image Default value: ``\n   */\n  prompts?: Array<PointPrompt>;\n  /**\n   * Coordinates for boxes Default value: ``\n   */\n  box_prompts?: Array<BoxPrompt>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type SAM2ImageInput = {\n  /**\n   * URL of the image to be segmented\n   */\n  image_url: string | Blob | File;\n  /**\n   * List of prompts to segment the image Default value: ``\n   */\n  prompts?: Array<PointPrompt>;\n  /**\n   * Coordinates for boxes Default value: ``\n   */\n  box_prompts?: Array<BoxPrompt>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type Sam2ImageOutput = {\n  /**\n   * Segmented image.\n   */\n  image: Image;\n};\nexport type SAM2ImageOutput = {\n  /**\n   * Segmented image.\n   */\n  image: Image;\n};\nexport type SAM2RLEOutput = {\n  /**\n   * Run Length Encoding of the mask.\n   */\n  rle: string | Array<string>;\n};\nexport type Sam2VideoInput = {\n  /**\n   * The URL of the video to be segmented.\n   */\n  video_url: string | Blob | File;\n  /**\n   * List of prompts to segment the video Default value: ``\n   */\n  prompts?: Array<PointPrompt>;\n  /**\n   * Coordinates for boxes Default value: ``\n   */\n  box_prompts?: Array<BoxPrompt>;\n  /**\n   * Apply the mask on the video.\n   */\n  apply_mask?: boolean;\n};\nexport type Sam2VideoOutput = {\n  /**\n   * The segmented video.\n   */\n  video: File;\n};\nexport type SAM2VideoOutput = {\n  /**\n   * The segmented video.\n   */\n  video: File;\n};\nexport type SAM2VideoRLEInput = {\n  /**\n   * The URL of the video to be segmented.\n   */\n  video_url: string | Blob | File;\n  /**\n   * List of prompts to segment the video Default value: ``\n   */\n  prompts?: Array<PointPrompt>;\n  /**\n   * Coordinates for boxes Default value: ``\n   */\n  box_prompts?: Array<BoxPrompt>;\n  /**\n   * Apply the mask on the video.\n   */\n  apply_mask?: boolean;\n};\nexport type SamInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n};\nexport type SamOutput = {\n  /**\n   * Image with SAM segmentation map\n   */\n  image: Image;\n};\nexport type SanaInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `18`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The style to generate the image in. Default value: `\"(No style)\"`\n   */\n  style_name?:\n    | \"(No style)\"\n    | \"Cinematic\"\n    | \"Photographic\"\n    | \"Anime\"\n    | \"Manga\"\n    | \"Digital Art\"\n    | \"Pixel art\"\n    | \"Fantasy art\"\n    | \"Neonpunk\"\n    | \"3D Model\";\n};\nexport type SanaOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type SchnellReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type SchnellTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type ScribbleInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * The model to use for the Scribble detector Default value: `\"HED\"`\n   */\n  model?: \"HED\" | \"PiDi\";\n  /**\n   * Whether to use the safe version of the Scribble detector\n   */\n  safe?: boolean;\n};\nexport type ScribbleOutput = {\n  /**\n   * Image with lines detected using the Scribble detector\n   */\n  image: Image;\n};\nexport type Sd15DepthControlnetInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The URL of the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `35`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, DeepCache will be enabled. TBD\n   */\n  enable_deep_cache?: boolean;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type Sd15DepthControlnetOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type SdxlControlnetUnionImageToImageInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `35`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n  /**\n   * The URL of the control image.\n   */\n  openpose_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the openpose image. Default value: `true`\n   */\n  openpose_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  depth_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the depth image. Default value: `true`\n   */\n  depth_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  teed_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the teed image. Default value: `true`\n   */\n  teed_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  canny_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the canny image. Default value: `true`\n   */\n  canny_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  normal_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the normal image. Default value: `true`\n   */\n  normal_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  segmentation_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the segmentation image. Default value: `true`\n   */\n  segmentation_preprocess?: boolean;\n};\nexport type SdxlControlnetUnionImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type SdxlControlnetUnionInpaintingInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask to use for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `35`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * determines how much the generated image resembles the initial image Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n  /**\n   * The URL of the control image.\n   */\n  openpose_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the openpose image. Default value: `true`\n   */\n  openpose_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  depth_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the depth image. Default value: `true`\n   */\n  depth_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  teed_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the teed image. Default value: `true`\n   */\n  teed_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  canny_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the canny image. Default value: `true`\n   */\n  canny_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  normal_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the normal image. Default value: `true`\n   */\n  normal_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  segmentation_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the segmentation image. Default value: `true`\n   */\n  segmentation_preprocess?: boolean;\n};\nexport type SdxlControlnetUnionInpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type SdxlControlnetUnionInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `35`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n  /**\n   * The URL of the control image.\n   */\n  openpose_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the openpose image. Default value: `true`\n   */\n  openpose_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  depth_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the depth image. Default value: `true`\n   */\n  depth_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  teed_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the teed image. Default value: `true`\n   */\n  teed_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  canny_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the canny image. Default value: `true`\n   */\n  canny_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  normal_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the normal image. Default value: `true`\n   */\n  normal_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  segmentation_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the segmentation image. Default value: `true`\n   */\n  segmentation_preprocess?: boolean;\n};\nexport type SdxlControlnetUnionOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type ShrinkMaskInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of pixels to shrink the mask. Default value: `5`\n   */\n  pixels?: number;\n  /**\n   * The threshold to convert the image to a mask. 0-255. Default value: `128`\n   */\n  threshold?: number;\n};\nexport type ShrinkMaskOutput = {\n  /**\n   * The mask\n   */\n  image: Image;\n};\nexport type SigmasInput = {\n  /**\n   * The method to use for the sigmas. If set to 'custom', the sigmas will be set based\n   * on the provided sigmas schedule in the `array` field.\n   * Defaults to 'default' which means the scheduler will use the sigmas of the scheduler. Default value: `\"default\"`\n   */\n  method?: \"default\" | \"array\";\n  /**\n   * Sigmas schedule to be used if 'custom' method is selected. Default value: ``\n   */\n  array?: Array<number>;\n};\nexport type SkyreelsI2vInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * URL of the image input.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Random seed for generation. If not provided, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Guidance scale for generation (between 1.0 and 20.0) Default value: `6`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of denoising steps (between 1 and 50). Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Negative prompt to guide generation away from certain attributes.\n   */\n  negative_prompt?: string;\n  /**\n   * Aspect ratio of the output video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n};\nexport type SkyreelsI2vOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generation\n   */\n  seed: number;\n};\nexport type SoteDiffusionInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of steps to run the first stage for. Default value: `25`\n   */\n  first_stage_steps?: number;\n  /**\n   * Number of steps to run the second stage for. Default value: `10`\n   */\n  second_stage_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `8`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2`\n   */\n  second_stage_guidance_scale?: number;\n  /**\n   * The size of the generated image. Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of Stable Cascade\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the image will be returned as base64 encoded string.\n   */\n  sync_mode?: boolean;\n};\nexport type SoundEffectOutput = {\n  /**\n   * The generated sound effect audio file in MP3 format\n   */\n  audio: File;\n};\nexport type SpanishOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type StableAudioInput = {\n  /**\n   * The prompt to generate audio from\n   */\n  prompt: string;\n  /**\n   * The start point of the audio clip to generate\n   */\n  seconds_start?: number;\n  /**\n   * The duration of the audio clip to generate Default value: `30`\n   */\n  seconds_total?: number;\n  /**\n   * The number of steps to denoise the audio for Default value: `100`\n   */\n  steps?: number;\n};\nexport type StableAudioOutput = {\n  /**\n   * The generated audio clip\n   */\n  audio_file: File;\n};\nexport type StableCascadeInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of steps to run the first stage for. Default value: `20`\n   */\n  first_stage_steps?: number;\n  /**\n   * Number of steps to run the second stage for. Default value: `10`\n   */\n  second_stage_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you.\n   */\n  second_stage_guidance_scale?: number;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of Stable Cascade\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the image will be returned as base64 encoded string.\n   */\n  sync_mode?: boolean;\n};\nexport type StableCascadeOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type StableCascadeSoteDiffusionInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of steps to run the first stage for. Default value: `25`\n   */\n  first_stage_steps?: number;\n  /**\n   * Number of steps to run the second stage for. Default value: `10`\n   */\n  second_stage_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `8`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2`\n   */\n  second_stage_guidance_scale?: number;\n  /**\n   * The size of the generated image. Default value: `[object Object]`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of Stable Cascade\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the image will be returned as base64 encoded string.\n   */\n  sync_mode?: boolean;\n};\nexport type StableCascadeSoteDiffusionOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type StableDiffusionV15Input = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type StableDiffusionV15Output = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type StableDiffusionV35LargeInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * ControlNet for inference.\n   */\n  controlnet?: ControlNet;\n  /**\n   * The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * IP-Adapter to use during inference.\n   */\n  ip_adapter?: IPAdapter;\n};\nexport type StableDiffusionV35LargeOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type StableDiffusionV35MediumInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type StableDiffusionV35MediumOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type StableDiffusionV3MediumImageToImageInput = {\n  /**\n   * The image URL to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to generate an image from. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, prompt will be upsampled with more details.\n   */\n  prompt_expansion?: boolean;\n  /**\n   * The size of the generated image. Defaults to the conditioning image's size.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The strength of the image-to-image transformation. Default value: `0.9`\n   */\n  strength?: number;\n};\nexport type StableDiffusionV3MediumImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n  /**\n   * The number of images generated.\n   */\n  num_images: number;\n};\nexport type StableDiffusionV3MediumInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to generate an image from. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, prompt will be upsampled with more details.\n   */\n  prompt_expansion?: boolean;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type StableDiffusionV3MediumOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n  /**\n   * The number of images generated.\n   */\n  num_images: number;\n};\nexport type StableVideoInput = {\n  /**\n   * The URL of the image to use as a starting point for the generation.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The motion bucket id determines the motion of the generated video. The\n   * higher the number, the more motion there will be. Default value: `127`\n   */\n  motion_bucket_id?: number;\n  /**\n   * The conditoning augmentation determines the amount of noise that will be\n   * added to the conditioning frame. The higher the number, the more noise\n   * there will be, and the less the video will look like the initial image.\n   * Increase it for more motion. Default value: `0.02`\n   */\n  cond_aug?: number;\n  /**\n   * The frames per second of the generated video. Default value: `25`\n   */\n  fps?: number;\n};\nexport type StableVideoOutput = {\n  /**\n   * Generated video\n   */\n  video: File;\n  /**\n   * Seed for random number generator\n   */\n  seed: number;\n};\nexport type StartEndToVideoOutput = {\n  /**\n   * The generated transition video between start and end frames\n   */\n  video: File;\n};\nexport type StepfunVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The number of inference steps to run. Lower gets faster results, higher gets better results. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n};\nexport type StepfunVideoOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type StyleReferenceInput = {\n  /**\n   * URL to zip archive with images, use PNG format. Maximum 5 images are allowed.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * The base style of the generated images, this topic is covered above. Default value: `\"digital_illustration\"`\n   */\n  base_style?:\n    | \"any\"\n    | \"realistic_image\"\n    | \"digital_illustration\"\n    | \"vector_illustration\"\n    | \"realistic_image/b_and_w\"\n    | \"realistic_image/hard_flash\"\n    | \"realistic_image/hdr\"\n    | \"realistic_image/natural_light\"\n    | \"realistic_image/studio_portrait\"\n    | \"realistic_image/enterprise\"\n    | \"realistic_image/motion_blur\"\n    | \"realistic_image/evening_light\"\n    | \"realistic_image/faded_nostalgia\"\n    | \"realistic_image/forest_life\"\n    | \"realistic_image/mystic_naturalism\"\n    | \"realistic_image/natural_tones\"\n    | \"realistic_image/organic_calm\"\n    | \"realistic_image/real_life_glow\"\n    | \"realistic_image/retro_realism\"\n    | \"realistic_image/retro_snapshot\"\n    | \"realistic_image/urban_drama\"\n    | \"realistic_image/village_realism\"\n    | \"realistic_image/warm_folk\"\n    | \"digital_illustration/pixel_art\"\n    | \"digital_illustration/hand_drawn\"\n    | \"digital_illustration/grain\"\n    | \"digital_illustration/infantile_sketch\"\n    | \"digital_illustration/2d_art_poster\"\n    | \"digital_illustration/handmade_3d\"\n    | \"digital_illustration/hand_drawn_outline\"\n    | \"digital_illustration/engraving_color\"\n    | \"digital_illustration/2d_art_poster_2\"\n    | \"digital_illustration/antiquarian\"\n    | \"digital_illustration/bold_fantasy\"\n    | \"digital_illustration/child_book\"\n    | \"digital_illustration/child_books\"\n    | \"digital_illustration/cover\"\n    | \"digital_illustration/crosshatch\"\n    | \"digital_illustration/digital_engraving\"\n    | \"digital_illustration/expressionism\"\n    | \"digital_illustration/freehand_details\"\n    | \"digital_illustration/grain_20\"\n    | \"digital_illustration/graphic_intensity\"\n    | \"digital_illustration/hard_comics\"\n    | \"digital_illustration/long_shadow\"\n    | \"digital_illustration/modern_folk\"\n    | \"digital_illustration/multicolor\"\n    | \"digital_illustration/neon_calm\"\n    | \"digital_illustration/noir\"\n    | \"digital_illustration/nostalgic_pastel\"\n    | \"digital_illustration/outline_details\"\n    | \"digital_illustration/pastel_gradient\"\n    | \"digital_illustration/pastel_sketch\"\n    | \"digital_illustration/pop_art\"\n    | \"digital_illustration/pop_renaissance\"\n    | \"digital_illustration/street_art\"\n    | \"digital_illustration/tablet_sketch\"\n    | \"digital_illustration/urban_glow\"\n    | \"digital_illustration/urban_sketching\"\n    | \"digital_illustration/vanilla_dreams\"\n    | \"digital_illustration/young_adult_book\"\n    | \"digital_illustration/young_adult_book_2\"\n    | \"vector_illustration/bold_stroke\"\n    | \"vector_illustration/chemistry\"\n    | \"vector_illustration/colored_stencil\"\n    | \"vector_illustration/contour_pop_art\"\n    | \"vector_illustration/cosmics\"\n    | \"vector_illustration/cutout\"\n    | \"vector_illustration/depressive\"\n    | \"vector_illustration/editorial\"\n    | \"vector_illustration/emotional_flat\"\n    | \"vector_illustration/infographical\"\n    | \"vector_illustration/marker_outline\"\n    | \"vector_illustration/mosaic\"\n    | \"vector_illustration/naivector\"\n    | \"vector_illustration/roundish_flat\"\n    | \"vector_illustration/segmented_colors\"\n    | \"vector_illustration/sharp_contrast\"\n    | \"vector_illustration/thin\"\n    | \"vector_illustration/vector_photo\"\n    | \"vector_illustration/vivid_shapes\"\n    | \"vector_illustration/engraving\"\n    | \"vector_illustration/line_art\"\n    | \"vector_illustration/line_circuit\"\n    | \"vector_illustration/linocut\";\n};\nexport type StyleReferenceOutput = {\n  /**\n   * The ID of the created style, this ID can be used to reference the style in the future.\n   */\n  style_id: string;\n};\nexport type SubjectCustomizeInput = {\n  /**\n   * The text prompt describing what you want to see, using [1] to reference the subject\n   */\n  prompt: string;\n  /**\n   * Type of subject in the reference images\n   */\n  subject_type: \"product\" | \"animal\";\n  /**\n   * 1-4 reference images of the subject to customize\n   */\n  reference_images: Array<ReferenceImage>;\n  /**\n   * Optional description of the subject in the reference images\n   */\n  subject_description?: string;\n  /**\n   * A description of what to discourage in the generated images Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of images to generate (1-4) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type SubjectReferenceOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type Swin2srInput = {\n  /**\n   * URL of image to be used for image enhancement\n   */\n  image_url: string | Blob | File;\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n  /**\n   * Task to perform Default value: `\"classical_sr\"`\n   */\n  task?: \"classical_sr\" | \"compressed_sr\" | \"real_sr\";\n};\nexport type Swin2srOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type Switti512Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of top-k tokens to sample from. Default value: `400`\n   */\n  sampling_top_k?: number;\n  /**\n   * The top-p probability to sample from. Default value: `0.95`\n   */\n  sampling_top_p?: number;\n  /**\n   * Smoothing with Gumbel softmax sampling Default value: `true`\n   */\n  more_smooth?: boolean;\n  /**\n   * More diverse sampling\n   */\n  more_diverse?: boolean;\n  /**\n   * Smoothing starting scale Default value: `2`\n   */\n  smooth_start_si?: number;\n  /**\n   * Disable CFG starting scale Default value: `8`\n   */\n  turn_off_cfg_start_si?: number;\n  /**\n   * Temperature after disabling CFG Default value: `0.1`\n   */\n  last_scale_temp?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `6`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type Switti512Output = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type SwittiInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of top-k tokens to sample from. Default value: `400`\n   */\n  sampling_top_k?: number;\n  /**\n   * The top-p probability to sample from. Default value: `0.95`\n   */\n  sampling_top_p?: number;\n  /**\n   * Smoothing with Gumbel softmax sampling Default value: `true`\n   */\n  more_smooth?: boolean;\n  /**\n   * More diverse sampling\n   */\n  more_diverse?: boolean;\n  /**\n   * Smoothing starting scale Default value: `2`\n   */\n  smooth_start_si?: number;\n  /**\n   * Disable CFG starting scale Default value: `8`\n   */\n  turn_off_cfg_start_si?: number;\n  /**\n   * Temperature after disabling CFG Default value: `0.1`\n   */\n  last_scale_temp?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `6`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type SwittiOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type SyncLipsyncInput = {\n  /**\n   * The model to use for lipsyncing Default value: `\"lipsync-1.9.0-beta\"`\n   */\n  model?: \"lipsync-1.8.0\" | \"lipsync-1.7.1\" | \"lipsync-1.9.0-beta\";\n  /**\n   * URL of the input video\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the input audio\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Lipsync mode when audio and video durations are out of sync. Default value: `\"cut_off\"`\n   */\n  sync_mode?: \"cut_off\" | \"loop\" | \"bounce\";\n};\nexport type SyncLipsyncOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type T2VDirectorOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type T2VLiveOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type T2VOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type T2vTurboInput = {\n  /**\n   * The prompt to generate images from\n   */\n  prompt: string;\n  /**\n   * The seed to use for the random number generator\n   */\n  seed?: number;\n  /**\n   * The number of steps to sample Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The guidance scale Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of frames to generate Default value: `16`\n   */\n  num_frames?: number;\n  /**\n   * The FPS of the exported video Default value: `8`\n   */\n  export_fps?: number;\n};\nexport type T2vTurboOutput = {\n  /**\n   * The URL to the generated video\n   */\n  video: File;\n};\nexport type TeedInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n};\nexport type TeeDInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n};\nexport type TeedOutput = {\n  /**\n   * The edge map.\n   */\n  image: Image;\n};\nexport type TeeDOutput = {\n  /**\n   * Image with TeeD lines detected\n   */\n  image: Image;\n};\nexport type TemplateToVideoOutput = {\n  /**\n   * The generated video using a predefined template\n   */\n  video: File;\n};\nexport type TextInput = {\n  /**\n   * The prompt to use as a starting point for the generation.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use as a starting point for the generation. Default value: `\"unrealistic, saturated, high contrast, big nose, painting, drawing, sketch, cartoon, anime, manga, render, CG, 3d, watermark, signature, label\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated video. Default value: `landscape_16_9`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The motion bucket id determines the motion of the generated video. The\n   * higher the number, the more motion there will be. Default value: `127`\n   */\n  motion_bucket_id?: number;\n  /**\n   * The conditoning augmentation determines the amount of noise that will be\n   * added to the conditioning frame. The higher the number, the more noise\n   * there will be, and the less the video will look like the initial image.\n   * Increase it for more motion. Default value: `0.02`\n   */\n  cond_aug?: number;\n};\nexport type TextOutput = {\n  /**\n   * The output text\n   */\n  text: string;\n};\nexport type TextToImageControlNetInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The URL of the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, DeepCache will be enabled. TBD\n   */\n  enable_deep_cache?: boolean;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type TextToImageControlNetUnionInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The scale of the controlnet conditioning. Default value: `0.5`\n   */\n  controlnet_conditioning_scale?: number;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Leave it none to automatically infer from the control image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `35`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of LoRA weights to use. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n  /**\n   * The URL of the control image.\n   */\n  openpose_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the openpose image. Default value: `true`\n   */\n  openpose_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  depth_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the depth image. Default value: `true`\n   */\n  depth_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  teed_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the teed image. Default value: `true`\n   */\n  teed_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  canny_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the canny image. Default value: `true`\n   */\n  canny_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  normal_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the normal image. Default value: `true`\n   */\n  normal_preprocess?: boolean;\n  /**\n   * The URL of the control image.\n   */\n  segmentation_image_url?: string | Blob | File;\n  /**\n   * Whether to preprocess the segmentation image. Default value: `true`\n   */\n  segmentation_preprocess?: boolean;\n};\nexport type TextToImageFooocusInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `8`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n  /**\n   * If set to true, a smaller model will try to refine the output after it was processed. Default value: `true`\n   */\n  enable_refiner?: boolean;\n};\nexport type TextToImageHyperInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"1\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\";\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type TextToImageInput = {\n  /**\n   * URL or HuggingFace ID of the base model to generate the image.\n   */\n  model_name: string;\n  /**\n   * URL or HuggingFace ID of the custom U-Net model to use for the image generation.\n   */\n  unet_name?: string;\n  /**\n   * The variant of the model to use for huggingface models, e.g. 'fp16'.\n   */\n  variant?: string;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use.Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, the prompt weighting syntax will be used.\n   * Additionally, this will lift the 77 token limit by averaging embeddings.\n   */\n  prompt_weighting?: boolean;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The embeddings to use for the image generation. Only a single embedding is supported at the moment.\n   * The embeddings will be used to map the tokens in the prompt to the embedding weights. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * The control nets to use for the image generation. You can use any number of control nets\n   * and they will be applied to the image at the specified timesteps. Default value: ``\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * If set to true, the controlnet will be applied to only the conditional predictions.\n   */\n  controlnet_guess_mode?: boolean;\n  /**\n   * The IP adapter to use for the image generation. Default value: ``\n   */\n  ip_adapter?: Array<IPAdapter>;\n  /**\n   * The path to the image encoder model to use for the image generation.\n   */\n  image_encoder_path?: string;\n  /**\n   * The subfolder of the image encoder model to use for the image generation.\n   */\n  image_encoder_subfolder?: string;\n  /**\n   * The weight name of the image encoder model to use for the image generation. Default value: `\"pytorch_model.bin\"`\n   */\n  image_encoder_weight_name?: string;\n  /**\n   * The URL of the IC Light model to use for the image generation.\n   */\n  ic_light_model_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model background image to use for the image generation.\n   * Make sure to use a background compatible with the model.\n   */\n  ic_light_model_background_image_url?: string | Blob | File;\n  /**\n   * The URL of the IC Light model image to use for the image generation.\n   */\n  ic_light_image_url?: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The size of the generated image. You can choose between some presets or custom height and width\n   * that **must be multiples of 8**. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Increasing the amount of steps tells Stable Diffusion that it should take more steps\n   * to generate your final result which can increase the amount of detail in your image. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Skips part of the image generation process, leading to slightly different results.\n   * This means the image renders faster, too.\n   */\n  clip_skip?: number;\n  /**\n   * Scheduler / sampler to use for the image denoising process.\n   */\n  scheduler?:\n    | \"DPM++ 2M\"\n    | \"DPM++ 2M Karras\"\n    | \"DPM++ 2M SDE\"\n    | \"DPM++ 2M SDE Karras\"\n    | \"Euler\"\n    | \"Euler A\"\n    | \"Euler (trailing timesteps)\"\n    | \"LCM\"\n    | \"LCM (trailing timesteps)\"\n    | \"DDIM\"\n    | \"TCD\";\n  /**\n   * Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter.\n   * If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set. Default value: `[object Object]`\n   */\n  timesteps?: TimestepsInput;\n  /**\n   * Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method.\n   * Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter.\n   * If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set. Default value: `[object Object]`\n   */\n  sigmas?: SigmasInput;\n  /**\n   * The type of prediction to use for the image generation.\n   * The `epsilon` is the default. Default value: `\"epsilon\"`\n   */\n  prediction_type?: \"v_prediction\" | \"epsilon\";\n  /**\n   * Whether to set the rescale_betas_snr_zero option or not for the sampler\n   */\n  rescale_betas_snr_zero?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  image_format?: \"jpeg\" | \"png\";\n  /**\n   * Number of images to generate in one request. Note that the higher the batch size,\n   * the longer it will take to generate the images. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_width?: number;\n  /**\n   * The size of the tiles to be used for the image generation. Default value: `4096`\n   */\n  tile_height?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_width?: number;\n  /**\n   * The stride of the tiles to be used for the image generation. Default value: `2048`\n   */\n  tile_stride_height?: number;\n  /**\n   * The eta value to be used for the image generation.\n   */\n  eta?: number;\n  /**\n   * If set to true, the latents will be saved for debugging.\n   */\n  debug_latents?: boolean;\n  /**\n   * If set to true, the latents will be saved for debugging per pass.\n   */\n  debug_per_pass_latents?: boolean;\n};\nexport type TextToImageLCMInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/stable-diffusion-xl-base-1.0\"`\n   */\n  model_name?:\n    | \"stabilityai/stable-diffusion-xl-base-1.0\"\n    | \"runwayml/stable-diffusion-v1-5\";\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `6`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n  /**\n   * An id bound to a request, can be used with response to identify the request\n   * itself. Default value: `\"\"`\n   */\n  request_id?: string;\n};\nexport type TextToImageLightningInput = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `\"4\"`\n   */\n  num_inference_steps?: \"1\" | \"2\" | \"4\" | \"8\";\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n};\nexport type TextToImageOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n};\nexport type TextToImagePlaygroundv25Input = {\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `25`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The list of embeddings to use. Default value: ``\n   */\n  embeddings?: Array<Embedding>;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`\n   */\n  safety_checker_version?: \"v1\" | \"v2\";\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  format?: \"jpeg\" | \"png\";\n  /**\n   * The rescale factor for the CFG.\n   */\n  guidance_rescale?: number;\n};\nexport type TextToImageTurboInput = {\n  /**\n   * The name of the model to use. Default value: `\"stabilityai/sdxl-turbo\"`\n   */\n  model_name?: \"stabilityai/sdxl-turbo\" | \"stabilityai/sd-turbo\";\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `square`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `2`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN. Default value: `true`\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the prompt will be expanded with additional prompts.\n   */\n  expand_prompt?: boolean;\n};\nexport type TextToVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n};\nexport type TextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type TimestepsInput = {\n  /**\n   * The method to use for the timesteps. If set to 'array', the timesteps will be set based\n   * on the provided timesteps schedule in the `array` field.\n   * Defaults to 'default' which means the scheduler will use the `num_inference_steps` parameter. Default value: `\"default\"`\n   */\n  method?: \"default\" | \"array\";\n  /**\n   * Timesteps schedule to be used if 'custom' method is selected. Default value: ``\n   */\n  array?: Array<number>;\n};\nexport type TopazUpscaleVideoInput = {\n  /**\n   * URL of the video to upscale\n   */\n  video_url: string | Blob | File;\n  /**\n   * Factor to upscale the video by (e.g. 2.0 doubles width and height) Default value: `2`\n   */\n  upscale_factor?: number;\n  /**\n   * Target FPS for frame interpolation. If set, frame interpolation will be enabled.\n   */\n  target_fps?: number;\n};\nexport type TopazUpscaleVideoOutput = {\n  /**\n   * The upscaled video file\n   */\n  video: File;\n};\nexport type TrainingInput = {\n  /**\n   * The name of the training job (required, max 255 characters).\n   */\n  name: string;\n  /**\n   * A list of audio URLs used for training (must be between 1 and 5 URLs).\n   */\n  training_data: Array<AudioInput>;\n};\nexport type TranscriptionOutput = {\n  /**\n   * The full transcribed text\n   */\n  text: string;\n  /**\n   * Detected or specified language code\n   */\n  language_code: string;\n  /**\n   * Confidence in language detection\n   */\n  language_probability: number;\n  /**\n   * Word-level transcription details\n   */\n  words: Array<TranscriptionWord>;\n};\nexport type TransparentImageToMaskInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The threshold to convert the image to a mask. Default value: `128`\n   */\n  threshold?: number;\n};\nexport type TransparentImageToMaskOutput = {\n  /**\n   * The mask\n   */\n  image: Image;\n};\nexport type TranspixarInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to generate video from Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. Default value: `24`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * The target FPS of the video Default value: `8`\n   */\n  export_fps?: number;\n};\nexport type TranspixarOutput = {\n  /**\n   * The URL to the generated video\n   */\n  videos: Array<File>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated video. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * The prompt used for generating the video.\n   */\n  prompt: string;\n};\nexport type TrellisInput = {\n  /**\n   * URL of the input image to convert to 3D\n   */\n  image_url: string | Blob | File;\n  /**\n   * Random seed for reproducibility\n   */\n  seed?: number;\n  /**\n   * Guidance strength for sparse structure generation Default value: `7.5`\n   */\n  ss_guidance_strength?: number;\n  /**\n   * Sampling steps for sparse structure generation Default value: `12`\n   */\n  ss_sampling_steps?: number;\n  /**\n   * Guidance strength for structured latent generation Default value: `3`\n   */\n  slat_guidance_strength?: number;\n  /**\n   * Sampling steps for structured latent generation Default value: `12`\n   */\n  slat_sampling_steps?: number;\n  /**\n   * Mesh simplification factor Default value: `0.95`\n   */\n  mesh_simplify?: number;\n  /**\n   * Texture resolution Default value: `\"1024\"`\n   */\n  texture_size?: \"512\" | \"1024\" | \"1536\" | \"2048\";\n};\nexport type TrellisOutput = {\n  /**\n   * Generated 3D mesh file\n   */\n  model_mesh: File;\n  /**\n   * Processing timings\n   */\n  timings: any;\n};\nexport type TriposrInput = {\n  /**\n   * Path for the image file to be processed.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Output format for the 3D model. Default value: `\"glb\"`\n   */\n  output_format?: \"glb\" | \"obj\";\n  /**\n   * Whether to remove the background from the input image. Default value: `true`\n   */\n  do_remove_background?: boolean;\n  /**\n   * Ratio of the foreground image to the original image. Default value: `0.9`\n   */\n  foreground_ratio?: number;\n  /**\n   * Resolution of the marching cubes. Above 512 is not recommended. Default value: `256`\n   */\n  mc_resolution?: number;\n};\nexport type TriposrOutput = {\n  /**\n   * Generated 3D object file.\n   */\n  model_mesh: File;\n  /**\n   * Inference timings.\n   */\n  timings: any;\n  /**\n   * Directory containing textures for the remeshed model.\n   */\n  remeshing_dir?: File;\n};\nexport type TryonInput = {\n  /**\n   * URL or base64 of the model image\n   */\n  model_image: string;\n  /**\n   * URL or base64 of the garment image\n   */\n  garment_image: string;\n  /**\n   * Category of the garment to try-on.\n   */\n  category: \"tops\" | \"bottoms\" | \"one-pieces\";\n  /**\n   * Specifies the type of garment photo to optimize internal parameters for better performance. 'model' is for photos of garments on a model, 'flat-lay' is for flat-lay or ghost mannequin images, and 'auto' attempts to automatically detect the photo type. Default value: `\"auto\"`\n   */\n  garment_photo_type?: \"auto\" | \"model\" | \"flat-lay\";\n  /**\n   * Runs NSFW content filter on inputs. Default value: `true`\n   */\n  nsfw_filter?: boolean;\n  /**\n   * Allows long garments to cover the feet/shoes or change their appearance.\n   */\n  cover_feet?: boolean;\n  /**\n   * Allow to change the appearance of the model’s hands. Example use-cases: Remove gloves, get hands out of pockets, long sleeves that should cover hands.\n   */\n  adjust_hands?: boolean;\n  /**\n   * Apply additional steps to preserve the original background. Runtime will be slower. Not needed for simple backgrounds.\n   */\n  restore_background?: boolean;\n  /**\n   * Apply additional steps to preserve the appearance of clothes that weren’t swapped (e.g. keep pants if trying-on top).\n   */\n  restore_clothes?: boolean;\n  /**\n   * Adjusts internal parameters for better performance on long tops such as: Longline shirts, tunics, coats, etc.\n   */\n  long_top?: boolean;\n  /**\n   * Higher guidance scales can help with preserving garment detail, but risks oversaturated colors. Default value: `2`\n   */\n  guidance_scale?: number;\n  /**\n   * Determines how many steps the diffusion model will take to generate the image. For simple try-ons, steps can be reduced for faster runtime. Default value: `50`\n   */\n  timesteps?: number;\n  /**\n   * Sets random operations to a fixed state. Use the same seed to reproduce results with the same inputs, or different seed to force different results. Default value: `42`\n   */\n  seed?: number;\n  /**\n   * Number of images to generate in a single run. Image generation has a random element in it, so trying multiple images at once increases the chances of getting a good result. Default value: `1`\n   */\n  num_samples?: number;\n};\nexport type TryonOutput = {\n  /**\n   * URLs of the generated images\n   */\n  images: Array<Image>;\n};\nexport type TTSOutput = {\n  /**\n   * The generated audio file\n   */\n  audio: File;\n};\nexport type UpscaleImageInput = {\n  /**\n   * The image URL to upscale\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to upscale the image with Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The resemblance of the upscaled image to the original image Default value: `50`\n   */\n  resemblance?: number;\n  /**\n   * The detail of the upscaled image Default value: `50`\n   */\n  detail?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n};\nexport type UpscaleInput = {\n  /**\n   * Url to input image\n   */\n  image_url: string | Blob | File;\n  /**\n   * Rescaling factor Default value: `2`\n   */\n  scale?: number;\n  /**\n   * Tile size. Default is 0, that is no tile. When encountering the out-of-GPU-memory issue, please specify it, e.g., 400 or 200\n   */\n  tile?: number;\n  /**\n   * Upscaling a face\n   */\n  face?: boolean;\n  /**\n   * Model to use for upscaling Default value: `\"RealESRGAN_x4plus\"`\n   */\n  model?:\n    | \"RealESRGAN_x4plus\"\n    | \"RealESRGAN_x2plus\"\n    | \"RealESRGAN_x4plus_anime_6B\"\n    | \"RealESRGAN_x4_v3\"\n    | \"RealESRGAN_x4_wdn_v3\"\n    | \"RealESRGAN_x4_anime_v3\";\n  /**\n   * Output image format (png or jpeg) Default value: `\"png\"`\n   */\n  output_format?: \"png\" | \"jpeg\";\n};\nexport type UpscaleOutput = {\n  /**\n   * Upscaled image\n   */\n  image: Image;\n};\nexport type V3TTSInput = {\n  /**\n   * The text to be converted to speech.\n   */\n  input: string;\n  /**\n   * The unique ID of a PlayHT or Cloned Voice, or a name from the available presets.\n   */\n  voice: string;\n  /**\n   * The format of the response. Default value: `\"url\"`\n   */\n  response_format?: \"url\" | \"bytes\";\n  /**\n   * An integer number greater than or equal to 0. If equal to null or not provided, a random seed will be used. Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file.\n   */\n  seed?: number;\n};\nexport type V3TTSOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: AudioFile;\n};\nexport type Veo2ImageToVideoInput = {\n  /**\n   * The text prompt describing how the image should be animated\n   */\n  prompt: string;\n  /**\n   * URL of the input image to animate. Should be 720p or higher resolution.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the generated video Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5s\"`\n   */\n  duration?: \"5s\" | \"6s\" | \"7s\" | \"8s\";\n};\nexport type Veo2ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type Veo2Input = {\n  /**\n   * The text prompt describing the video you want to generate\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5s\"`\n   */\n  duration?: \"5s\" | \"6s\" | \"7s\" | \"8s\";\n};\nexport type Veo2Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type VideoConditioningInput = {\n  /**\n   * URL of video to be extended\n   */\n  video_url: string | Blob | File;\n  /**\n   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.\n   */\n  start_frame_num: number;\n};\nexport type VideoEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type VideoInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * The URL of the input video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Number of frames to sample from the video. If not provided, all frames are sampled.\n   */\n  num_frames_to_sample?: number;\n};\nexport type VideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type VideoPromptGeneratorInput = {\n  /**\n   * Core concept or thematic input for the video prompt\n   */\n  input_concept: string;\n  /**\n   * Style of the video prompt Default value: `\"Simple\"`\n   */\n  style?:\n    | \"Minimalist\"\n    | \"Simple\"\n    | \"Detailed\"\n    | \"Descriptive\"\n    | \"Dynamic\"\n    | \"Cinematic\"\n    | \"Documentary\"\n    | \"Animation\"\n    | \"Action\"\n    | \"Experimental\";\n  /**\n   * Camera movement style Default value: `\"None\"`\n   */\n  camera_style?:\n    | \"None\"\n    | \"Steadicam flow\"\n    | \"Drone aerials\"\n    | \"Handheld urgency\"\n    | \"Crane elegance\"\n    | \"Dolly precision\"\n    | \"VR 360\"\n    | \"Multi-angle rig\"\n    | \"Static tripod\"\n    | \"Gimbal smoothness\"\n    | \"Slider motion\"\n    | \"Jib sweep\"\n    | \"POV immersion\"\n    | \"Time-slice array\"\n    | \"Macro extreme\"\n    | \"Tilt-shift miniature\"\n    | \"Snorricam character\"\n    | \"Whip pan dynamics\"\n    | \"Dutch angle tension\"\n    | \"Underwater housing\"\n    | \"Periscope lens\";\n  /**\n   * Camera direction Default value: `\"None\"`\n   */\n  camera_direction?:\n    | \"None\"\n    | \"Zoom in\"\n    | \"Zoom out\"\n    | \"Pan left\"\n    | \"Pan right\"\n    | \"Tilt up\"\n    | \"Tilt down\"\n    | \"Orbital rotation\"\n    | \"Push in\"\n    | \"Pull out\"\n    | \"Track forward\"\n    | \"Track backward\"\n    | \"Spiral in\"\n    | \"Spiral out\"\n    | \"Arc movement\"\n    | \"Diagonal traverse\"\n    | \"Vertical rise\"\n    | \"Vertical descent\";\n  /**\n   * Pacing rhythm Default value: `\"None\"`\n   */\n  pacing?:\n    | \"None\"\n    | \"Slow burn\"\n    | \"Rhythmic pulse\"\n    | \"Frantic energy\"\n    | \"Ebb and flow\"\n    | \"Hypnotic drift\"\n    | \"Time-lapse rush\"\n    | \"Stop-motion staccato\"\n    | \"Gradual build\"\n    | \"Quick cut rhythm\"\n    | \"Long take meditation\"\n    | \"Jump cut energy\"\n    | \"Match cut flow\"\n    | \"Cross-dissolve dreamscape\"\n    | \"Parallel action\"\n    | \"Slow motion impact\"\n    | \"Ramping dynamics\"\n    | \"Montage tempo\"\n    | \"Continuous flow\"\n    | \"Episodic breaks\";\n  /**\n   * Special effects approach Default value: `\"None\"`\n   */\n  special_effects?:\n    | \"None\"\n    | \"Practical effects\"\n    | \"CGI enhancement\"\n    | \"Analog glitches\"\n    | \"Light painting\"\n    | \"Projection mapping\"\n    | \"Nanosecond exposures\"\n    | \"Double exposure\"\n    | \"Smoke diffusion\"\n    | \"Lens flare artistry\"\n    | \"Particle systems\"\n    | \"Holographic overlay\"\n    | \"Chromatic aberration\"\n    | \"Digital distortion\"\n    | \"Wire removal\"\n    | \"Motion capture\"\n    | \"Miniature integration\"\n    | \"Weather simulation\"\n    | \"Color grading\"\n    | \"Mixed media composite\"\n    | \"Neural style transfer\";\n  /**\n   * Custom technical elements (optional) Default value: `\"\"`\n   */\n  custom_elements?: string;\n  /**\n   * URL of an image to analyze and incorporate into the video prompt (optional)\n   */\n  image_url?: string | Blob | File;\n  /**\n   * Model to use Default value: `\"google/gemini-flash-1.5\"`\n   */\n  model?:\n    | \"anthropic/claude-3.5-sonnet\"\n    | \"anthropic/claude-3-5-haiku\"\n    | \"anthropic/claude-3-haiku\"\n    | \"google/gemini-pro-1.5\"\n    | \"google/gemini-flash-1.5\"\n    | \"google/gemini-flash-1.5-8b\"\n    | \"meta-llama/llama-3.2-1b-instruct\"\n    | \"meta-llama/llama-3.2-3b-instruct\"\n    | \"meta-llama/llama-3.1-8b-instruct\"\n    | \"meta-llama/llama-3.1-70b-instruct\"\n    | \"openai/gpt-4o-mini\"\n    | \"openai/gpt-4o\"\n    | \"deepseek/deepseek-r1\";\n  /**\n   * Length of the prompt Default value: `\"Medium\"`\n   */\n  prompt_length?: \"Short\" | \"Medium\" | \"Long\";\n};\nexport type VideoPromptGeneratorOutput = {\n  /**\n   * Generated video prompt\n   */\n  prompt: string;\n};\nexport type VideoToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated video. Default value: `[object Object]`\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to generate video from Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRAs to use for the image generation. We currently support one lora. Default value: ``\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * Use RIFE for video interpolation Default value: `true`\n   */\n  use_rife?: boolean;\n  /**\n   * The target FPS of the video Default value: `16`\n   */\n  export_fps?: number;\n  /**\n   * The video to generate the video from.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The strength to use for Video to Video.  1.0 completely remakes the video while 0.0 preserves the original. Default value: `0.8`\n   */\n  strength?: number;\n};\nexport type VideoUpscalerInput = {\n  /**\n   * The URL of the video to upscale\n   */\n  video_url: string | Blob | File;\n  /**\n   * The scale factor Default value: `2`\n   */\n  scale?: number;\n};\nexport type VideoUpscalerOutput = {\n  /**\n   * The stitched video\n   */\n  video: File;\n};\nexport type ViduImageToVideoInput = {\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The movement amplitude of objects in the frame Default value: `\"auto\"`\n   */\n  movement_amplitude?: \"auto\" | \"small\" | \"medium\" | \"large\";\n};\nexport type ViduImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ViduReferenceToVideoInput = {\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * URLs of the reference images to use for consistent subject appearance\n   */\n  reference_image_urls: Array<string>;\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the output video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * The movement amplitude of objects in the frame Default value: `\"auto\"`\n   */\n  movement_amplitude?: \"auto\" | \"small\" | \"medium\" | \"large\";\n};\nexport type ViduReferenceToVideoOutput = {\n  /**\n   * The generated video with consistent subjects from reference images\n   */\n  video: File;\n};\nexport type ViduStartEndToVideoInput = {\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  start_image_url: string | Blob | File;\n  /**\n   * URL of the image to use as the last frame\n   */\n  end_image_url: string | Blob | File;\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The movement amplitude of objects in the frame Default value: `\"auto\"`\n   */\n  movement_amplitude?: \"auto\" | \"small\" | \"medium\" | \"large\";\n};\nexport type ViduStartEndToVideoOutput = {\n  /**\n   * The generated transition video between start and end frames\n   */\n  video: File;\n};\nexport type ViduTemplateToVideoInput = {\n  /**\n   * AI video template to use. Pricing varies by template: Standard templates (hug, kiss, love_pose, etc.) cost 4 credits ($0.20), Premium templates (lunar_newyear, dynasty_dress, dreamy_wedding, etc.) cost 6 credits ($0.30), and Advanced templates (live_photo) cost 10 credits ($0.50). Default value: `\"hug\"`\n   */\n  template?:\n    | \"dreamy_wedding\"\n    | \"romantic_lift\"\n    | \"sweet_proposal\"\n    | \"couple_arrival\"\n    | \"cupid_arrow\"\n    | \"pet_lovers\"\n    | \"lunar_newyear\"\n    | \"hug\"\n    | \"kiss\"\n    | \"dynasty_dress\"\n    | \"wish_sender\"\n    | \"love_pose\"\n    | \"hair_swap\"\n    | \"youth_rewind\"\n    | \"morphlab\"\n    | \"live_photo\"\n    | \"emotionlab\"\n    | \"live_memory\"\n    | \"interaction\"\n    | \"christmas\";\n  /**\n   * URLs of the images to use with the template. Number of images required varies by template: 'dynasty_dress' and 'shop_frame' accept 1-2 images, 'wish_sender' requires exactly 3 images, all other templates accept only 1 image.\n   */\n  input_image_urls: Array<string>;\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the output video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n};\nexport type ViduTemplateToVideoOutput = {\n  /**\n   * The generated video using a predefined template\n   */\n  video: File;\n};\nexport type VisionInput = {\n  /**\n   * Name of the model to use. Premium models are charged at 3x the rate of standard models, they include: meta-llama/llama-3.2-90b-vision-instruct, openai/gpt-4o, anthropic/claude-3-5-haiku, google/gemini-pro-1.5, anthropic/claude-3.5-sonnet. Default value: `\"google/gemini-flash-1.5\"`\n   */\n  model?:\n    | \"anthropic/claude-3.5-sonnet\"\n    | \"anthropic/claude-3-haiku\"\n    | \"google/gemini-pro-1.5\"\n    | \"google/gemini-flash-1.5\"\n    | \"google/gemini-flash-1.5-8b\"\n    | \"openai/gpt-4o\"\n    | \"meta-llama/llama-3.2-90b-vision-instruct\";\n  /**\n   * Prompt to be used for the image\n   */\n  prompt: string;\n  /**\n   * System prompt to provide context or instructions to the model\n   */\n  system_prompt?: string;\n  /**\n   * Should reasoning be the part of the final answer.\n   */\n  reasoning?: boolean;\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n};\nexport type VTONInput = {\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same input given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your input when generating the image. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Url for the human image.\n   */\n  human_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  garment_image_url: string | Blob | File;\n};\nexport type VTONOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n  /**\n   * The seed for the inference.\n   */\n  seed: number;\n  /**\n   * Whether the image contains NSFW concepts.\n   */\n  has_nsfw_concepts: boolean;\n};\nexport type WanEffectsInput = {\n  /**\n   * The subject to insert into the predefined prompt template for the selected effect.\n   */\n  subject: string;\n  /**\n   * URL of the input image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The type of effect to apply to the video. Default value: `\"cakeify\"`\n   */\n  effect_type?:\n    | \"squish\"\n    | \"muscle\"\n    | \"inflate\"\n    | \"crush\"\n    | \"rotate\"\n    | \"gun-shooting\"\n    | \"deflate\"\n    | \"cakeify\"\n    | \"hulk\"\n    | \"baby\"\n    | \"bride\"\n    | \"classy\"\n    | \"puppy\"\n    | \"snow-white\"\n    | \"disney-princess\"\n    | \"mona-lisa\"\n    | \"painting\"\n    | \"pirate-captain\"\n    | \"princess\"\n    | \"jungle\"\n    | \"samurai\"\n    | \"vip\"\n    | \"warrior\"\n    | \"zen\"\n    | \"assassin\"\n    | \"timelapse\";\n  /**\n   * Number of frames to generate. Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Aspect ratio of the output video. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The scale of the LoRA weight. Used to adjust effect intensity. Default value: `1`\n   */\n  lora_scale?: number;\n};\nexport type WanEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type WanI2vInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * URL of the input image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Number of frames to generate. Must be between 81 to 100 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n};\nexport type WanI2vOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanProImageToVideoInput = {\n  /**\n   * The prompt to generate the video\n   */\n  prompt: string;\n  /**\n   * Whether to enable the safety checker Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The URL of the image to generate the video from\n   */\n  image_url: string | Blob | File;\n};\nexport type WanProImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type WanProTextToVideoInput = {\n  /**\n   * The prompt to generate the video\n   */\n  prompt: string;\n  /**\n   * Whether to enable the safety checker Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type WanProTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type WanT2vInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 100 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p,580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n};\nexport type WanT2vOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV2113bTextToVideoInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small\n   * details (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Classifier-free guidance scale. Controls prompt adherence vs. creativity Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * Noise schedule shift parameter. Affects temporal dynamics. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * The sampler to use for generation. Default value: `\"unipc\"`\n   */\n  sampler?: \"unipc\" | \"dpm++\";\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n};\nexport type WanV2113bTextToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WaveformInput = {\n  /**\n   * URL of the audio file to analyze\n   */\n  media_url: string | Blob | File;\n  /**\n   * Controls how many points are sampled per second of audio. Lower values (e.g. 1-2) create a coarser waveform, higher values (e.g. 4-10) create a more detailed one. Default value: `4`\n   */\n  points_per_second?: number;\n  /**\n   * Number of decimal places for the waveform values. Higher values provide more precision but increase payload size. Default value: `2`\n   */\n  precision?: number;\n  /**\n   * Size of the smoothing window. Higher values create a smoother waveform. Must be an odd number. Default value: `3`\n   */\n  smoothing_window?: number;\n};\nexport type WaveformOutput = {\n  /**\n   * Normalized waveform data as an array of values between -1 and 1. The number of points is determined by audio duration × points_per_second.\n   */\n  waveform: Array<number>;\n  /**\n   * Duration of the audio in seconds\n   */\n  duration: number;\n  /**\n   * Number of points in the waveform data\n   */\n  points: number;\n  /**\n   * Number of decimal places used in the waveform values\n   */\n  precision: number;\n};\nexport type WhisperInput = {\n  /**\n   * URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Task to perform on the audio file. Either transcribe or translate. Default value: `\"transcribe\"`\n   */\n  task?: \"transcribe\" | \"translate\";\n  /**\n   * Language of the audio file. If set to null, the language will be\n   * automatically detected. Defaults to null.\n   *\n   * If translate is selected as the task, the audio will be translated to\n   * English, regardless of the language selected.\n   */\n  language?:\n    | \"af\"\n    | \"am\"\n    | \"ar\"\n    | \"as\"\n    | \"az\"\n    | \"ba\"\n    | \"be\"\n    | \"bg\"\n    | \"bn\"\n    | \"bo\"\n    | \"br\"\n    | \"bs\"\n    | \"ca\"\n    | \"cs\"\n    | \"cy\"\n    | \"da\"\n    | \"de\"\n    | \"el\"\n    | \"en\"\n    | \"es\"\n    | \"et\"\n    | \"eu\"\n    | \"fa\"\n    | \"fi\"\n    | \"fo\"\n    | \"fr\"\n    | \"gl\"\n    | \"gu\"\n    | \"ha\"\n    | \"haw\"\n    | \"he\"\n    | \"hi\"\n    | \"hr\"\n    | \"ht\"\n    | \"hu\"\n    | \"hy\"\n    | \"id\"\n    | \"is\"\n    | \"it\"\n    | \"ja\"\n    | \"jw\"\n    | \"ka\"\n    | \"kk\"\n    | \"km\"\n    | \"kn\"\n    | \"ko\"\n    | \"la\"\n    | \"lb\"\n    | \"ln\"\n    | \"lo\"\n    | \"lt\"\n    | \"lv\"\n    | \"mg\"\n    | \"mi\"\n    | \"mk\"\n    | \"ml\"\n    | \"mn\"\n    | \"mr\"\n    | \"ms\"\n    | \"mt\"\n    | \"my\"\n    | \"ne\"\n    | \"nl\"\n    | \"nn\"\n    | \"no\"\n    | \"oc\"\n    | \"pa\"\n    | \"pl\"\n    | \"ps\"\n    | \"pt\"\n    | \"ro\"\n    | \"ru\"\n    | \"sa\"\n    | \"sd\"\n    | \"si\"\n    | \"sk\"\n    | \"sl\"\n    | \"sn\"\n    | \"so\"\n    | \"sq\"\n    | \"sr\"\n    | \"su\"\n    | \"sv\"\n    | \"sw\"\n    | \"ta\"\n    | \"te\"\n    | \"tg\"\n    | \"th\"\n    | \"tk\"\n    | \"tl\"\n    | \"tr\"\n    | \"tt\"\n    | \"uk\"\n    | \"ur\"\n    | \"uz\"\n    | \"vi\"\n    | \"yi\"\n    | \"yo\"\n    | \"yue\"\n    | \"zh\";\n  /**\n   * Whether to diarize the audio file. Defaults to false.\n   */\n  diarize?: boolean;\n  /**\n   * Level of the chunks to return. Either segment or word. Default value: `\"segment\"`\n   */\n  chunk_level?: \"segment\" | \"word\";\n  /**\n   * Version of the model to use. All of the models are the Whisper large variant. Default value: `\"3\"`\n   */\n  version?: \"3\";\n  /**\n   *  Default value: `64`\n   */\n  batch_size?: number;\n  /**\n   * Prompt to use for generation. Defaults to an empty string. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * Number of speakers in the audio file. Defaults to null.\n   * If not provided, the number of speakers will be automatically\n   * detected.\n   */\n  num_speakers?: number;\n};\nexport type WhisperOutput = {\n  /**\n   * Transcription of the audio file\n   */\n  text: string;\n  /**\n   * Timestamp chunks of the audio file\n   */\n  chunks?: Array<WhisperChunk>;\n  /**\n   * List of languages that the audio file is inferred to be. Defaults to null.\n   */\n  inferred_languages: Array<\n    | \"af\"\n    | \"am\"\n    | \"ar\"\n    | \"as\"\n    | \"az\"\n    | \"ba\"\n    | \"be\"\n    | \"bg\"\n    | \"bn\"\n    | \"bo\"\n    | \"br\"\n    | \"bs\"\n    | \"ca\"\n    | \"cs\"\n    | \"cy\"\n    | \"da\"\n    | \"de\"\n    | \"el\"\n    | \"en\"\n    | \"es\"\n    | \"et\"\n    | \"eu\"\n    | \"fa\"\n    | \"fi\"\n    | \"fo\"\n    | \"fr\"\n    | \"gl\"\n    | \"gu\"\n    | \"ha\"\n    | \"haw\"\n    | \"he\"\n    | \"hi\"\n    | \"hr\"\n    | \"ht\"\n    | \"hu\"\n    | \"hy\"\n    | \"id\"\n    | \"is\"\n    | \"it\"\n    | \"ja\"\n    | \"jw\"\n    | \"ka\"\n    | \"kk\"\n    | \"km\"\n    | \"kn\"\n    | \"ko\"\n    | \"la\"\n    | \"lb\"\n    | \"ln\"\n    | \"lo\"\n    | \"lt\"\n    | \"lv\"\n    | \"mg\"\n    | \"mi\"\n    | \"mk\"\n    | \"ml\"\n    | \"mn\"\n    | \"mr\"\n    | \"ms\"\n    | \"mt\"\n    | \"my\"\n    | \"ne\"\n    | \"nl\"\n    | \"nn\"\n    | \"no\"\n    | \"oc\"\n    | \"pa\"\n    | \"pl\"\n    | \"ps\"\n    | \"pt\"\n    | \"ro\"\n    | \"ru\"\n    | \"sa\"\n    | \"sd\"\n    | \"si\"\n    | \"sk\"\n    | \"sl\"\n    | \"sn\"\n    | \"so\"\n    | \"sq\"\n    | \"sr\"\n    | \"su\"\n    | \"sv\"\n    | \"sw\"\n    | \"ta\"\n    | \"te\"\n    | \"tg\"\n    | \"th\"\n    | \"tk\"\n    | \"tl\"\n    | \"tr\"\n    | \"tt\"\n    | \"uk\"\n    | \"ur\"\n    | \"uz\"\n    | \"vi\"\n    | \"yi\"\n    | \"yo\"\n    | \"yue\"\n    | \"zh\"\n  >;\n  /**\n   * Speaker diarization segments of the audio file. Only present if diarization is enabled.\n   */\n  diarization_segments: Array<DiarizationSegment>;\n};\nexport type WizperInput = {\n  /**\n   * URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Task to perform on the audio file. Either transcribe or translate. Default value: `\"transcribe\"`\n   */\n  task?: \"transcribe\" | \"translate\";\n  /**\n   * Language of the audio file.\n   * If translate is selected as the task, the audio will be translated to\n   * English, regardless of the language selected. Default value: `\"en\"`\n   */\n  language?:\n    | \"af\"\n    | \"am\"\n    | \"ar\"\n    | \"as\"\n    | \"az\"\n    | \"ba\"\n    | \"be\"\n    | \"bg\"\n    | \"bn\"\n    | \"bo\"\n    | \"br\"\n    | \"bs\"\n    | \"ca\"\n    | \"cs\"\n    | \"cy\"\n    | \"da\"\n    | \"de\"\n    | \"el\"\n    | \"en\"\n    | \"es\"\n    | \"et\"\n    | \"eu\"\n    | \"fa\"\n    | \"fi\"\n    | \"fo\"\n    | \"fr\"\n    | \"gl\"\n    | \"gu\"\n    | \"ha\"\n    | \"haw\"\n    | \"he\"\n    | \"hi\"\n    | \"hr\"\n    | \"ht\"\n    | \"hu\"\n    | \"hy\"\n    | \"id\"\n    | \"is\"\n    | \"it\"\n    | \"ja\"\n    | \"jw\"\n    | \"ka\"\n    | \"kk\"\n    | \"km\"\n    | \"kn\"\n    | \"ko\"\n    | \"la\"\n    | \"lb\"\n    | \"ln\"\n    | \"lo\"\n    | \"lt\"\n    | \"lv\"\n    | \"mg\"\n    | \"mi\"\n    | \"mk\"\n    | \"ml\"\n    | \"mn\"\n    | \"mr\"\n    | \"ms\"\n    | \"mt\"\n    | \"my\"\n    | \"ne\"\n    | \"nl\"\n    | \"nn\"\n    | \"no\"\n    | \"oc\"\n    | \"pa\"\n    | \"pl\"\n    | \"ps\"\n    | \"pt\"\n    | \"ro\"\n    | \"ru\"\n    | \"sa\"\n    | \"sd\"\n    | \"si\"\n    | \"sk\"\n    | \"sl\"\n    | \"sn\"\n    | \"so\"\n    | \"sq\"\n    | \"sr\"\n    | \"su\"\n    | \"sv\"\n    | \"sw\"\n    | \"ta\"\n    | \"te\"\n    | \"tg\"\n    | \"th\"\n    | \"tk\"\n    | \"tl\"\n    | \"tr\"\n    | \"tt\"\n    | \"uk\"\n    | \"ur\"\n    | \"uz\"\n    | \"vi\"\n    | \"yi\"\n    | \"yo\"\n    | \"yue\"\n    | \"zh\";\n  /**\n   * Level of the chunks to return. Default value: `\"segment\"`\n   */\n  chunk_level?: \"segment\";\n  /**\n   * Version of the model to use. All of the models are the Whisper large variant. Default value: `\"3\"`\n   */\n  version?: \"3\";\n};\nexport type WizperOutput = {\n  /**\n   * Transcription of the audio file\n   */\n  text: string;\n  /**\n   * Timestamp chunks of the audio file\n   */\n  chunks: Array<WhisperChunk>;\n};\nexport type WorkflowutilsCannyInput = {\n  /**\n   * Input image url.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Low threshold for the hysteresis procedure Default value: `100`\n   */\n  low_threshold?: number;\n  /**\n   * High threshold for the hysteresis procedure Default value: `200`\n   */\n  high_threshold?: number;\n};\nexport type WorkflowutilsCannyOutput = {\n  /**\n   * The output image\n   */\n  image: Image;\n};\nexport type YueInput = {\n  /**\n   * The prompt to generate an image from. Must have two sections. Sections start with either [chorus] or a [verse].\n   */\n  lyrics: string;\n  /**\n   * The genres (separated by a space ' ') to guide the music generation.\n   */\n  genres: string;\n};\nexport type YueOutput = {\n  /**\n   * Generated music file.\n   */\n  audio: File;\n};\nexport type ZoeInput = {\n  /**\n   * URL of the image to process\n   */\n  image_url: string | Blob | File;\n};\nexport type ZoeOutput = {\n  /**\n   * Image with depth map\n   */\n  image: Image;\n};\nexport type ZonosInput = {\n  /**\n   * The reference audio.\n   */\n  reference_audio_url: string | Blob | File;\n  /**\n   * The content generated using cloned voice.\n   */\n  prompt: string;\n};\nexport type ZonosOutput = {\n  /**\n   * The generated audio\n   */\n  audio: File;\n};\nexport type EndpointTypeMap = {\n  \"fal-ai/sync-lipsync\": {\n    input: SyncLipsyncInput;\n    output: SyncLipsyncOutput;\n  };\n  \"fal-ai/veo2/image-to-video\": {\n    input: Veo2ImageToVideoInput;\n    output: Veo2ImageToVideoOutput;\n  };\n  \"fal-ai/veo2\": {\n    input: Veo2Input;\n    output: Veo2Output;\n  };\n  \"fal-ai/minimax-music\": {\n    input: MinimaxMusicInput;\n    output: MinimaxMusicOutput;\n  };\n  \"fal-ai/diffrhythm\": {\n    input: DiffrhythmInput;\n    output: DiffrhythmOutput;\n  };\n  \"fal-ai/yue\": {\n    input: YueInput;\n    output: YueOutput;\n  };\n  \"fal-ai/flux-pro/v1.1-ultra\": {\n    input: FluxProV11UltraInput;\n    output: FluxProV11UltraOutput;\n  };\n  \"fal-ai/flux-pro/v1.1-ultra-finetuned\": {\n    input: FluxProV11UltraFinetunedInput;\n    output: FluxProV11UltraFinetunedOutput;\n  };\n  \"fal-ai/ideogram/v2\": {\n    input: IdeogramV2Input;\n    output: IdeogramV2Output;\n  };\n  \"fal-ai/hunyuan-video-lora-training\": {\n    input: HunyuanVideoLoraTrainingInput;\n    output: HunyuanVideoLoraTrainingOutput;\n  };\n  \"fal-ai/flux-lora-fast-training\": {\n    input: FluxLoraFastTrainingInput;\n    output: FluxLoraFastTrainingOutput;\n  };\n  \"fal-ai/flux-lora-portrait-trainer\": {\n    input: FluxLoraPortraitTrainerInput;\n    output: FluxLoraPortraitTrainerOutput;\n  };\n  \"fal-ai/flux-pro-trainer\": {\n    input: FluxProTrainerInput;\n    output: FluxProTrainerOutput;\n  };\n  \"fal-ai/recraft-v3\": {\n    input: RecraftV3Input;\n    output: RecraftV3Output;\n  };\n  \"fal-ai/minimax/video-01-live\": {\n    input: MinimaxVideo01LiveInput;\n    output: MinimaxVideo01LiveOutput;\n  };\n  \"fal-ai/minimax/video-01-live/image-to-video\": {\n    input: MinimaxVideo01LiveImageToVideoInput;\n    output: MinimaxVideo01LiveImageToVideoOutput;\n  };\n  \"fal-ai/minimax/video-01-subject-reference\": {\n    input: MinimaxVideo01SubjectReferenceInput;\n    output: MinimaxVideo01SubjectReferenceOutput;\n  };\n  \"fal-ai/minimax/video-01-director\": {\n    input: MinimaxVideo01DirectorInput;\n    output: MinimaxVideo01DirectorOutput;\n  };\n  \"fal-ai/minimax/video-01-director/image-to-video\": {\n    input: MinimaxVideo01DirectorImageToVideoInput;\n    output: MinimaxVideo01DirectorImageToVideoOutput;\n  };\n  \"fal-ai/minimax-image\": {\n    input: MinimaxImageInput;\n    output: MinimaxImageOutput;\n  };\n  \"fal-ai/hyper3d/rodin\": {\n    input: Hyper3dRodinInput;\n    output: Hyper3dRodinOutput;\n  };\n  \"fal-ai/aura-flow\": {\n    input: AuraFlowInput;\n    output: AuraFlowOutput;\n  };\n  \"fal-ai/flux/dev/image-to-image\": {\n    input: FluxDevImageToImageInput;\n    output: FluxDevImageToImageOutput;\n  };\n  \"fal-ai/flux/dev\": {\n    input: FluxDevInput;\n    output: FluxDevOutput;\n  };\n  \"fal-ai/flux-lora\": {\n    input: FluxLoraInput;\n    output: FluxLoraOutput;\n  };\n  \"fal-ai/flux-lora/inpainting\": {\n    input: FluxLoraInpaintingInput;\n    output: FluxLoraInpaintingOutput;\n  };\n  \"fal-ai/flux/schnell\": {\n    input: FluxSchnellInput;\n    output: FluxSchnellOutput;\n  };\n  \"fal-ai/flux-subject\": {\n    input: FluxSubjectInput;\n    output: FluxSubjectOutput;\n  };\n  \"fal-ai/flux/schnell/redux\": {\n    input: FluxSchnellReduxInput;\n    output: FluxSchnellReduxOutput;\n  };\n  \"fal-ai/flux/dev/redux\": {\n    input: FluxDevReduxInput;\n    output: FluxDevReduxOutput;\n  };\n  \"fal-ai/flux-pro/v1/redux\": {\n    input: FluxProV1ReduxInput;\n    output: FluxProV1ReduxOutput;\n  };\n  \"fal-ai/flux-pro/v1.1/redux\": {\n    input: FluxProV11ReduxInput;\n    output: FluxProV11ReduxOutput;\n  };\n  \"fal-ai/flux-pro/v1.1-ultra/redux\": {\n    input: FluxProV11UltraReduxInput;\n    output: FluxProV11UltraReduxOutput;\n  };\n  \"fal-ai/flux-pro/v1/fill\": {\n    input: FluxProV1FillInput;\n    output: FluxProV1FillOutput;\n  };\n  \"fal-ai/flux-pro/v1/fill-finetuned\": {\n    input: FluxProV1FillFinetunedInput;\n    output: FluxProV1FillFinetunedOutput;\n  };\n  \"fal-ai/flux-pro/v1/canny\": {\n    input: FluxProV1CannyInput;\n    output: FluxProV1CannyOutput;\n  };\n  \"fal-ai/flux-pro/v1/canny-finetuned\": {\n    input: FluxProV1CannyFinetunedInput;\n    output: FluxProV1CannyFinetunedOutput;\n  };\n  \"fal-ai/flux-pro/v1/depth\": {\n    input: FluxProV1DepthInput;\n    output: FluxProV1DepthOutput;\n  };\n  \"fal-ai/flux-pro/v1/depth-finetuned\": {\n    input: FluxProV1DepthFinetunedInput;\n    output: FluxProV1DepthFinetunedOutput;\n  };\n  \"fal-ai/flux-lora-canny\": {\n    input: FluxLoraCannyInput;\n    output: FluxLoraCannyOutput;\n  };\n  \"fal-ai/flux-lora-depth\": {\n    input: FluxLoraDepthInput;\n    output: FluxLoraDepthOutput;\n  };\n  \"fal-ai/flux-pro/v1.1\": {\n    input: FluxProV11Input;\n    output: FluxProV11Output;\n  };\n  \"fal-ai/flux-pro/new\": {\n    input: FluxProNewInput;\n    output: FluxProNewOutput;\n  };\n  \"fal-ai/sana\": {\n    input: SanaInput;\n    output: SanaOutput;\n  };\n  \"fal-ai/omnigen-v1\": {\n    input: OmnigenV1Input;\n    output: OmnigenV1Output;\n  };\n  \"fal-ai/lumina-image/v2\": {\n    input: LuminaImageV2Input;\n    output: LuminaImageV2Output;\n  };\n  \"fal-ai/stable-diffusion-v35-large\": {\n    input: StableDiffusionV35LargeInput;\n    output: StableDiffusionV35LargeOutput;\n  };\n  \"fal-ai/stable-diffusion-v35-medium\": {\n    input: StableDiffusionV35MediumInput;\n    output: StableDiffusionV35MediumOutput;\n  };\n  \"fal-ai/switti\": {\n    input: SwittiInput;\n    output: SwittiOutput;\n  };\n  \"fal-ai/switti/512\": {\n    input: Switti512Input;\n    output: Switti512Output;\n  };\n  \"fal-ai/recraft-v3/create-style\": {\n    input: RecraftV3CreateStyleInput;\n    output: RecraftV3CreateStyleOutput;\n  };\n  \"fal-ai/minimax/video-01/image-to-video\": {\n    input: MinimaxVideo01ImageToVideoInput;\n    output: MinimaxVideo01ImageToVideoOutput;\n  };\n  \"fal-ai/recraft-20b\": {\n    input: Recraft20bInput;\n    output: Recraft20bOutput;\n  };\n  \"fal-ai/ideogram/v2/edit\": {\n    input: IdeogramV2EditInput;\n    output: IdeogramV2EditOutput;\n  };\n  \"fal-ai/ideogram/v2/remix\": {\n    input: IdeogramV2RemixInput;\n    output: IdeogramV2RemixOutput;\n  };\n  \"fal-ai/ideogram/v2/turbo\": {\n    input: IdeogramV2TurboInput;\n    output: IdeogramV2TurboOutput;\n  };\n  \"fal-ai/ideogram/v2/turbo/edit\": {\n    input: IdeogramV2TurboEditInput;\n    output: IdeogramV2TurboEditOutput;\n  };\n  \"fal-ai/ideogram/v2/turbo/remix\": {\n    input: IdeogramV2TurboRemixInput;\n    output: IdeogramV2TurboRemixOutput;\n  };\n  \"fal-ai/ideogram/upscale\": {\n    input: IdeogramUpscaleInput;\n    output: IdeogramUpscaleOutput;\n  };\n  \"fal-ai/ideogram/v2a\": {\n    input: IdeogramV2aInput;\n    output: IdeogramV2aOutput;\n  };\n  \"fal-ai/ideogram/v2a/remix\": {\n    input: IdeogramV2aRemixInput;\n    output: IdeogramV2aRemixOutput;\n  };\n  \"fal-ai/ideogram/v2a/turbo\": {\n    input: IdeogramV2aTurboInput;\n    output: IdeogramV2aTurboOutput;\n  };\n  \"fal-ai/ideogram/v2a/turbo/remix\": {\n    input: IdeogramV2aTurboRemixInput;\n    output: IdeogramV2aTurboRemixOutput;\n  };\n  \"fal-ai/bria/text-to-image/base\": {\n    input: BriaTextToImageBaseInput;\n    output: BriaTextToImageBaseOutput;\n  };\n  \"fal-ai/bria/text-to-image/fast\": {\n    input: BriaTextToImageFastInput;\n    output: BriaTextToImageFastOutput;\n  };\n  \"fal-ai/bria/text-to-image/hd\": {\n    input: BriaTextToImageHdInput;\n    output: BriaTextToImageHdOutput;\n  };\n  \"fal-ai/bria/eraser\": {\n    input: BriaEraserInput;\n    output: BriaEraserOutput;\n  };\n  \"fal-ai/bria/product-shot\": {\n    input: BriaProductShotInput;\n    output: BriaProductShotOutput;\n  };\n  \"fal-ai/bria/background/replace\": {\n    input: BriaBackgroundReplaceInput;\n    output: BriaBackgroundReplaceOutput;\n  };\n  \"fal-ai/bria/genfill\": {\n    input: BriaGenfillInput;\n    output: BriaGenfillOutput;\n  };\n  \"fal-ai/bria/expand\": {\n    input: BriaExpandInput;\n    output: BriaExpandOutput;\n  };\n  \"fal-ai/bria/background/remove\": {\n    input: BriaBackgroundRemoveInput;\n    output: BriaBackgroundRemoveOutput;\n  };\n  \"fal-ai/flux-lora-fill\": {\n    input: FluxLoraFillInput;\n    output: FluxLoraFillOutput;\n  };\n  \"fal-ai/flux-lora/image-to-image\": {\n    input: FluxLoraImageToImageInput;\n    output: FluxLoraImageToImageOutput;\n  };\n  \"fal-ai/flux-control-lora-canny\": {\n    input: FluxControlLoraCannyInput;\n    output: FluxControlLoraCannyOutput;\n  };\n  \"fal-ai/flux-control-lora-canny/image-to-image\": {\n    input: FluxControlLoraCannyImageToImageInput;\n    output: FluxControlLoraCannyImageToImageOutput;\n  };\n  \"fal-ai/flux-control-lora-depth\": {\n    input: FluxControlLoraDepthInput;\n    output: FluxControlLoraDepthOutput;\n  };\n  \"fal-ai/flux-control-lora-depth/image-to-image\": {\n    input: FluxControlLoraDepthImageToImageInput;\n    output: FluxControlLoraDepthImageToImageOutput;\n  };\n  \"fal-ai/flux-general\": {\n    input: FluxGeneralInput;\n    output: FluxGeneralOutput;\n  };\n  \"fal-ai/flux-general/inpainting\": {\n    input: FluxGeneralInpaintingInput;\n    output: FluxGeneralInpaintingOutput;\n  };\n  \"fal-ai/flux-general/image-to-image\": {\n    input: FluxGeneralImageToImageInput;\n    output: FluxGeneralImageToImageOutput;\n  };\n  \"fal-ai/flux-general/differential-diffusion\": {\n    input: FluxGeneralDifferentialDiffusionInput;\n    output: FluxGeneralDifferentialDiffusionOutput;\n  };\n  \"fal-ai/flux-general/rf-inversion\": {\n    input: FluxGeneralRfInversionInput;\n    output: FluxGeneralRfInversionOutput;\n  };\n  \"fal-ai/flux-pulid\": {\n    input: FluxPulidInput;\n    output: FluxPulidOutput;\n  };\n  \"fal-ai/iclight-v2\": {\n    input: IclightV2Input;\n    output: IclightV2Output;\n  };\n  \"fal-ai/flux-differential-diffusion\": {\n    input: FluxDifferentialDiffusionInput;\n    output: FluxDifferentialDiffusionOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux/base\": {\n    input: JuggernautFluxBaseInput;\n    output: JuggernautFluxBaseOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux/lightning\": {\n    input: JuggernautFluxLightningInput;\n    output: JuggernautFluxLightningOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux/pro\": {\n    input: JuggernautFluxProInput;\n    output: JuggernautFluxProOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux/base/image-to-image\": {\n    input: JuggernautFluxBaseImageToImageInput;\n    output: JuggernautFluxBaseImageToImageOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux/pro/image-to-image\": {\n    input: JuggernautFluxProImageToImageInput;\n    output: JuggernautFluxProImageToImageOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux-lora\": {\n    input: JuggernautFluxLoraInput;\n    output: JuggernautFluxLoraOutput;\n  };\n  \"rundiffusion-fal/rundiffusion-photo-flux\": {\n    input: RundiffusionPhotoFluxInput;\n    output: RundiffusionPhotoFluxOutput;\n  };\n  \"fal-ai/stable-diffusion-v3-medium\": {\n    input: StableDiffusionV3MediumInput;\n    output: StableDiffusionV3MediumOutput;\n  };\n  \"fal-ai/stable-diffusion-v3-medium/image-to-image\": {\n    input: StableDiffusionV3MediumImageToImageInput;\n    output: StableDiffusionV3MediumImageToImageOutput;\n  };\n  \"fal-ai/fast-sdxl\": {\n    input: FastSdxlInput;\n    output: FastSdxlOutput;\n  };\n  \"fal-ai/lora\": {\n    input: LoraInput;\n    output: LoraOutput;\n  };\n  \"fal-ai/aura-sr\": {\n    input: AuraSrInput;\n    output: AuraSrOutput;\n  };\n  \"fal-ai/codeformer\": {\n    input: CodeformerInput;\n    output: CodeformerOutput;\n  };\n  \"fal-ai/ddcolor\": {\n    input: DdcolorInput;\n    output: DdcolorOutput;\n  };\n  \"fal-ai/swin2sr\": {\n    input: Swin2srInput;\n    output: Swin2srOutput;\n  };\n  \"fal-ai/docres\": {\n    input: DocresInput;\n    output: DocresOutput;\n  };\n  \"fal-ai/docres/dewarp\": {\n    input: DocresDewarpInput;\n    output: DocresDewarpOutput;\n  };\n  \"fal-ai/nafnet/deblur\": {\n    input: NafnetDeblurInput;\n    output: NafnetDeblurOutput;\n  };\n  \"fal-ai/nafnet/denoise\": {\n    input: NafnetDenoiseInput;\n    output: NafnetDenoiseOutput;\n  };\n  \"fal-ai/drct-super-resolution\": {\n    input: DrctSuperResolutionInput;\n    output: DrctSuperResolutionOutput;\n  };\n  \"fal-ai/ben/v2/image\": {\n    input: BenV2ImageInput;\n    output: BenV2ImageOutput;\n  };\n  \"fal-ai/ben/v2/video\": {\n    input: BenV2VideoInput;\n    output: BenV2VideoOutput;\n  };\n  \"fal-ai/flowedit\": {\n    input: FloweditInput;\n    output: FloweditOutput;\n  };\n  \"fal-ai/zonos\": {\n    input: ZonosInput;\n    output: ZonosOutput;\n  };\n  \"fal-ai/stable-cascade\": {\n    input: StableCascadeInput;\n    output: StableCascadeOutput;\n  };\n  \"fal-ai/minimax/video-01\": {\n    input: MinimaxVideo01Input;\n    output: MinimaxVideo01Output;\n  };\n  \"fal-ai/mochi-v1\": {\n    input: MochiV1Input;\n    output: MochiV1Output;\n  };\n  \"fal-ai/stepfun-video\": {\n    input: StepfunVideoInput;\n    output: StepfunVideoOutput;\n  };\n  \"fal-ai/hunyuan-video\": {\n    input: HunyuanVideoInput;\n    output: HunyuanVideoOutput;\n  };\n  \"fal-ai/hunyuan-video-image-to-video\": {\n    input: HunyuanVideoImageToVideoInput;\n    output: HunyuanVideoImageToVideoOutput;\n  };\n  \"fal-ai/hunyuan-video-img2vid-lora\": {\n    input: HunyuanVideoImg2vidLoraInput;\n    output: HunyuanVideoImg2vidLoraOutput;\n  };\n  \"fal-ai/hunyuan-video/video-to-video\": {\n    input: HunyuanVideoVideoToVideoInput;\n    output: HunyuanVideoVideoToVideoOutput;\n  };\n  \"fal-ai/hunyuan-video-lora\": {\n    input: HunyuanVideoLoraInput;\n    output: HunyuanVideoLoraOutput;\n  };\n  \"fal-ai/hunyuan-video-lora/video-to-video\": {\n    input: HunyuanVideoLoraVideoToVideoInput;\n    output: HunyuanVideoLoraVideoToVideoOutput;\n  };\n  \"fal-ai/wan/v2.1/1.3b/text-to-video\": {\n    input: WanV2113bTextToVideoInput;\n    output: WanV2113bTextToVideoOutput;\n  };\n  \"fal-ai/wan-t2v\": {\n    input: WanT2vInput;\n    output: WanT2vOutput;\n  };\n  \"fal-ai/wan-i2v\": {\n    input: WanI2vInput;\n    output: WanI2vOutput;\n  };\n  \"fal-ai/wan-pro/image-to-video\": {\n    input: WanProImageToVideoInput;\n    output: WanProImageToVideoOutput;\n  };\n  \"fal-ai/wan-pro/text-to-video\": {\n    input: WanProTextToVideoInput;\n    output: WanProTextToVideoOutput;\n  };\n  \"fal-ai/wan-effects\": {\n    input: WanEffectsInput;\n    output: WanEffectsOutput;\n  };\n  \"fal-ai/skyreels-i2v\": {\n    input: SkyreelsI2vInput;\n    output: SkyreelsI2vOutput;\n  };\n  \"fal-ai/vidu/image-to-video\": {\n    input: ViduImageToVideoInput;\n    output: ViduImageToVideoOutput;\n  };\n  \"fal-ai/vidu/reference-to-video\": {\n    input: ViduReferenceToVideoInput;\n    output: ViduReferenceToVideoOutput;\n  };\n  \"fal-ai/vidu/start-end-to-video\": {\n    input: ViduStartEndToVideoInput;\n    output: ViduStartEndToVideoOutput;\n  };\n  \"fal-ai/vidu/template-to-video\": {\n    input: ViduTemplateToVideoInput;\n    output: ViduTemplateToVideoOutput;\n  };\n  \"fal-ai/video-upscaler\": {\n    input: VideoUpscalerInput;\n    output: VideoUpscalerOutput;\n  };\n  \"fal-ai/auto-caption\": {\n    input: AutoCaptionInput;\n    output: AutoCaptionOutput;\n  };\n  \"fal-ai/mmaudio-v2\": {\n    input: MmaudioV2Input;\n    output: MmaudioV2Output;\n  };\n  \"fal-ai/mmaudio-v2/text-to-audio\": {\n    input: MmaudioV2TextToAudioInput;\n    output: MmaudioV2TextToAudioOutput;\n  };\n  \"fal-ai/luma-dream-machine/ray-2-flash\": {\n    input: LumaDreamMachineRay2FlashInput;\n    output: LumaDreamMachineRay2FlashOutput;\n  };\n  \"fal-ai/luma-dream-machine/ray-2-flash/image-to-video\": {\n    input: LumaDreamMachineRay2FlashImageToVideoInput;\n    output: LumaDreamMachineRay2FlashImageToVideoOutput;\n  };\n  \"fal-ai/luma-dream-machine/ray-2\": {\n    input: LumaDreamMachineRay2Input;\n    output: LumaDreamMachineRay2Output;\n  };\n  \"fal-ai/luma-dream-machine/ray-2/image-to-video\": {\n    input: LumaDreamMachineRay2ImageToVideoInput;\n    output: LumaDreamMachineRay2ImageToVideoOutput;\n  };\n  \"fal-ai/luma-dream-machine\": {\n    input: LumaDreamMachineInput;\n    output: LumaDreamMachineOutput;\n  };\n  \"fal-ai/luma-dream-machine/image-to-video\": {\n    input: LumaDreamMachineImageToVideoInput;\n    output: LumaDreamMachineImageToVideoOutput;\n  };\n  \"fal-ai/luma-photon\": {\n    input: LumaPhotonInput;\n    output: LumaPhotonOutput;\n  };\n  \"fal-ai/luma-photon/flash\": {\n    input: LumaPhotonFlashInput;\n    output: LumaPhotonFlashOutput;\n  };\n  \"fal-ai/kling/v1-5/kolors-virtual-try-on\": {\n    input: KlingV15KolorsVirtualTryOnInput;\n    output: KlingV15KolorsVirtualTryOnOutput;\n  };\n  \"fal-ai/kling-video/v1/standard/text-to-video\": {\n    input: KlingVideoV1StandardTextToVideoInput;\n    output: KlingVideoV1StandardTextToVideoOutput;\n  };\n  \"fal-ai/kling-video/v1/standard/effects\": {\n    input: KlingVideoV1StandardEffectsInput;\n    output: KlingVideoV1StandardEffectsOutput;\n  };\n  \"fal-ai/kling-video/v1/standard/image-to-video\": {\n    input: KlingVideoV1StandardImageToVideoInput;\n    output: KlingVideoV1StandardImageToVideoOutput;\n  };\n  \"fal-ai/kling-video/v1/pro/text-to-video\": {\n    input: KlingVideoV1ProTextToVideoInput;\n    output: KlingVideoV1ProTextToVideoOutput;\n  };\n  \"fal-ai/kling-video/v1/pro/image-to-video\": {\n    input: KlingVideoV1ProImageToVideoInput;\n    output: KlingVideoV1ProImageToVideoOutput;\n  };\n  \"fal-ai/kling-video/v1.5/pro/image-to-video\": {\n    input: KlingVideoV15ProImageToVideoInput;\n    output: KlingVideoV15ProImageToVideoOutput;\n  };\n  \"fal-ai/kling-video/v1.5/pro/text-to-video\": {\n    input: KlingVideoV15ProTextToVideoInput;\n    output: KlingVideoV15ProTextToVideoOutput;\n  };\n  \"fal-ai/kling-video/v1.5/pro/effects\": {\n    input: KlingVideoV15ProEffectsInput;\n    output: KlingVideoV15ProEffectsOutput;\n  };\n  \"fal-ai/kling-video/v1.6/standard/image-to-video\": {\n    input: KlingVideoV16StandardImageToVideoInput;\n    output: KlingVideoV16StandardImageToVideoOutput;\n  };\n  \"fal-ai/kling-video/v1.6/standard/text-to-video\": {\n    input: KlingVideoV16StandardTextToVideoInput;\n    output: KlingVideoV16StandardTextToVideoOutput;\n  };\n  \"fal-ai/kling-video/v1.6/standard/effects\": {\n    input: KlingVideoV16StandardEffectsInput;\n    output: KlingVideoV16StandardEffectsOutput;\n  };\n  \"fal-ai/kling-video/v1.6/pro/image-to-video\": {\n    input: KlingVideoV16ProImageToVideoInput;\n    output: KlingVideoV16ProImageToVideoOutput;\n  };\n  \"fal-ai/kling-video/v1.6/pro/effects\": {\n    input: KlingVideoV16ProEffectsInput;\n    output: KlingVideoV16ProEffectsOutput;\n  };\n  \"fal-ai/kling-video/v1.6/pro/text-to-video\": {\n    input: KlingVideoV16ProTextToVideoInput;\n    output: KlingVideoV16ProTextToVideoOutput;\n  };\n  \"fal-ai/pixverse/v3.5/text-to-video\": {\n    input: PixverseV35TextToVideoInput;\n    output: PixverseV35TextToVideoOutput;\n  };\n  \"fal-ai/pixverse/v3.5/image-to-video\": {\n    input: PixverseV35ImageToVideoInput;\n    output: PixverseV35ImageToVideoOutput;\n  };\n  \"fal-ai/pixverse/v3.5/text-to-video/fast\": {\n    input: PixverseV35TextToVideoFastInput;\n    output: PixverseV35TextToVideoFastOutput;\n  };\n  \"fal-ai/pixverse/v3.5/image-to-video/fast\": {\n    input: PixverseV35ImageToVideoFastInput;\n    output: PixverseV35ImageToVideoFastOutput;\n  };\n  \"fal-ai/transpixar\": {\n    input: TranspixarInput;\n    output: TranspixarOutput;\n  };\n  \"fal-ai/cogvideox-5b\": {\n    input: Cogvideox5bInput;\n    output: Cogvideox5bOutput;\n  };\n  \"fal-ai/cogvideox-5b/video-to-video\": {\n    input: Cogvideox5bVideoToVideoInput;\n    output: Cogvideox5bVideoToVideoOutput;\n  };\n  \"fal-ai/cogvideox-5b/image-to-video\": {\n    input: Cogvideox5bImageToVideoInput;\n    output: Cogvideox5bImageToVideoOutput;\n  };\n  \"fal-ai/cogview4\": {\n    input: Cogview4Input;\n    output: Cogview4Output;\n  };\n  \"fal-ai/ltx-video\": {\n    input: LtxVideoInput;\n    output: LtxVideoOutput;\n  };\n  \"fal-ai/ltx-video/image-to-video\": {\n    input: LtxVideoImageToVideoInput;\n    output: LtxVideoImageToVideoOutput;\n  };\n  \"fal-ai/ltx-video-v095\": {\n    input: LtxVideoV095Input;\n    output: LtxVideoV095Output;\n  };\n  \"fal-ai/ltx-video-v095/image-to-video\": {\n    input: LtxVideoV095ImageToVideoInput;\n    output: LtxVideoV095ImageToVideoOutput;\n  };\n  \"fal-ai/ltx-video-v095/extend\": {\n    input: LtxVideoV095ExtendInput;\n    output: LtxVideoV095ExtendOutput;\n  };\n  \"fal-ai/ltx-video-v095/multiconditioning\": {\n    input: LtxVideoV095MulticonditioningInput;\n    output: LtxVideoV095MulticonditioningOutput;\n  };\n  \"fal-ai/stable-video\": {\n    input: StableVideoInput;\n    output: StableVideoOutput;\n  };\n  \"fal-ai/fast-svd/text-to-video\": {\n    input: FastSvdTextToVideoInput;\n    output: FastSvdTextToVideoOutput;\n  };\n  \"fal-ai/fast-svd-lcm\": {\n    input: FastSvdLcmInput;\n    output: FastSvdLcmOutput;\n  };\n  \"fal-ai/birefnet\": {\n    input: BirefnetInput;\n    output: BirefnetOutput;\n  };\n  \"fal-ai/birefnet/v2\": {\n    input: BirefnetV2Input;\n    output: BirefnetV2Output;\n  };\n  \"fal-ai/got-ocr/v2\": {\n    input: GotOcrV2Input;\n    output: GotOcrV2Output;\n  };\n  \"fal-ai/fast-svd-lcm/text-to-video\": {\n    input: FastSvdLcmTextToVideoInput;\n    output: FastSvdLcmTextToVideoOutput;\n  };\n  \"fal-ai/creative-upscaler\": {\n    input: CreativeUpscalerInput;\n    output: CreativeUpscalerOutput;\n  };\n  \"fal-ai/ffmpeg-api/compose\": {\n    input: FfmpegApiComposeInput;\n    output: FfmpegApiComposeOutput;\n  };\n  \"fal-ai/ffmpeg-api/metadata\": {\n    input: FfmpegApiMetadataInput;\n    output: FfmpegApiMetadataOutput;\n  };\n  \"fal-ai/ffmpeg-api/waveform\": {\n    input: FfmpegApiWaveformInput;\n    output: FfmpegApiWaveformOutput;\n  };\n  \"fal-ai/clarity-upscaler\": {\n    input: ClarityUpscalerInput;\n    output: ClarityUpscalerOutput;\n  };\n  \"fal-ai/ccsr\": {\n    input: CcsrInput;\n    output: CcsrOutput;\n  };\n  \"fal-ai/fast-turbo-diffusion\": {\n    input: FastTurboDiffusionInput;\n    output: FastTurboDiffusionOutput;\n  };\n  \"fal-ai/fast-turbo-diffusion/image-to-image\": {\n    input: FastTurboDiffusionImageToImageInput;\n    output: FastTurboDiffusionImageToImageOutput;\n  };\n  \"fal-ai/fast-turbo-diffusion/inpainting\": {\n    input: FastTurboDiffusionInpaintingInput;\n    output: FastTurboDiffusionInpaintingOutput;\n  };\n  \"fal-ai/fast-lcm-diffusion\": {\n    input: FastLcmDiffusionInput;\n    output: FastLcmDiffusionOutput;\n  };\n  \"fal-ai/fast-lcm-diffusion/image-to-image\": {\n    input: FastLcmDiffusionImageToImageInput;\n    output: FastLcmDiffusionImageToImageOutput;\n  };\n  \"fal-ai/fast-lcm-diffusion/inpainting\": {\n    input: FastLcmDiffusionInpaintingInput;\n    output: FastLcmDiffusionInpaintingOutput;\n  };\n  \"fal-ai/whisper\": {\n    input: WhisperInput;\n    output: WhisperOutput;\n  };\n  \"fal-ai/wizper\": {\n    input: WizperInput;\n    output: WizperOutput;\n  };\n  \"fal-ai/fast-lightning-sdxl\": {\n    input: FastLightningSdxlInput;\n    output: FastLightningSdxlOutput;\n  };\n  \"fal-ai/fast-lightning-sdxl/image-to-image\": {\n    input: FastLightningSdxlImageToImageInput;\n    output: FastLightningSdxlImageToImageOutput;\n  };\n  \"fal-ai/fast-lightning-sdxl/inpainting\": {\n    input: FastLightningSdxlInpaintingInput;\n    output: FastLightningSdxlInpaintingOutput;\n  };\n  \"fal-ai/hyper-sdxl\": {\n    input: HyperSdxlInput;\n    output: HyperSdxlOutput;\n  };\n  \"fal-ai/hyper-sdxl/image-to-image\": {\n    input: HyperSdxlImageToImageInput;\n    output: HyperSdxlImageToImageOutput;\n  };\n  \"fal-ai/hyper-sdxl/inpainting\": {\n    input: HyperSdxlInpaintingInput;\n    output: HyperSdxlInpaintingOutput;\n  };\n  \"fal-ai/playground-v25\": {\n    input: PlaygroundV25Input;\n    output: PlaygroundV25Output;\n  };\n  \"fal-ai/playground-v25/image-to-image\": {\n    input: PlaygroundV25ImageToImageInput;\n    output: PlaygroundV25ImageToImageOutput;\n  };\n  \"fal-ai/playground-v25/inpainting\": {\n    input: PlaygroundV25InpaintingInput;\n    output: PlaygroundV25InpaintingOutput;\n  };\n  \"fal-ai/amt-interpolation\": {\n    input: AmtInterpolationInput;\n    output: AmtInterpolationOutput;\n  };\n  \"fal-ai/amt-interpolation/frame-interpolation\": {\n    input: AmtInterpolationFrameInterpolationInput;\n    output: AmtInterpolationFrameInterpolationOutput;\n  };\n  \"fal-ai/t2v-turbo\": {\n    input: T2vTurboInput;\n    output: T2vTurboOutput;\n  };\n  \"fal-ai/sd15-depth-controlnet\": {\n    input: Sd15DepthControlnetInput;\n    output: Sd15DepthControlnetOutput;\n  };\n  \"fal-ai/photomaker\": {\n    input: PhotomakerInput;\n    output: PhotomakerOutput;\n  };\n  \"fal-ai/lcm\": {\n    input: LcmInput;\n    output: LcmOutput;\n  };\n  \"fal-ai/lcm-sd15-i2i\": {\n    input: LcmSd15I2iInput;\n    output: LcmSd15I2iOutput;\n  };\n  \"fal-ai/fooocus\": {\n    input: FooocusInput;\n    output: FooocusOutput;\n  };\n  \"fal-ai/animatediff-v2v\": {\n    input: AnimatediffV2vInput;\n    output: AnimatediffV2vOutput;\n  };\n  \"fal-ai/animatediff-v2v/turbo\": {\n    input: AnimatediffV2vTurboInput;\n    output: AnimatediffV2vTurboOutput;\n  };\n  \"fal-ai/fast-animatediff/text-to-video\": {\n    input: FastAnimatediffTextToVideoInput;\n    output: FastAnimatediffTextToVideoOutput;\n  };\n  \"fal-ai/fast-animatediff/video-to-video\": {\n    input: FastAnimatediffVideoToVideoInput;\n    output: FastAnimatediffVideoToVideoOutput;\n  };\n  \"fal-ai/fast-animatediff/turbo/text-to-video\": {\n    input: FastAnimatediffTurboTextToVideoInput;\n    output: FastAnimatediffTurboTextToVideoOutput;\n  };\n  \"fal-ai/fast-animatediff/turbo/video-to-video\": {\n    input: FastAnimatediffTurboVideoToVideoInput;\n    output: FastAnimatediffTurboVideoToVideoOutput;\n  };\n  \"fal-ai/illusion-diffusion\": {\n    input: IllusionDiffusionInput;\n    output: IllusionDiffusionOutput;\n  };\n  \"fal-ai/imageutils/depth\": {\n    input: ImageutilsDepthInput;\n    output: ImageutilsDepthOutput;\n  };\n  \"fal-ai/imageutils/rembg\": {\n    input: ImageutilsRembgInput;\n    output: ImageutilsRembgOutput;\n  };\n  \"fal-ai/esrgan\": {\n    input: EsrganInput;\n    output: EsrganOutput;\n  };\n  \"fal-ai/controlnetsdxl\": {\n    input: ControlnetsdxlInput;\n    output: ControlnetsdxlOutput;\n  };\n  \"fal-ai/fast-sdxl-controlnet-canny\": {\n    input: FastSdxlControlnetCannyInput;\n    output: FastSdxlControlnetCannyOutput;\n  };\n  \"fal-ai/fast-sdxl-controlnet-canny/image-to-image\": {\n    input: FastSdxlControlnetCannyImageToImageInput;\n    output: FastSdxlControlnetCannyImageToImageOutput;\n  };\n  \"fal-ai/fast-sdxl-controlnet-canny/inpainting\": {\n    input: FastSdxlControlnetCannyInpaintingInput;\n    output: FastSdxlControlnetCannyInpaintingOutput;\n  };\n  \"fal-ai/inpaint\": {\n    input: InpaintInput;\n    output: InpaintOutput;\n  };\n  \"fal-ai/animatediff-sparsectrl-lcm\": {\n    input: AnimatediffSparsectrlLcmInput;\n    output: AnimatediffSparsectrlLcmOutput;\n  };\n  \"fal-ai/pulid\": {\n    input: PulidInput;\n    output: PulidOutput;\n  };\n  \"fal-ai/ip-adapter-face-id\": {\n    input: IpAdapterFaceIdInput;\n    output: IpAdapterFaceIdOutput;\n  };\n  \"fal-ai/imageutils/marigold-depth\": {\n    input: ImageutilsMarigoldDepthInput;\n    output: ImageutilsMarigoldDepthOutput;\n  };\n  \"fal-ai/stable-audio\": {\n    input: StableAudioInput;\n    output: StableAudioOutput;\n  };\n  \"fal-ai/diffusion-edge\": {\n    input: DiffusionEdgeInput;\n    output: DiffusionEdgeOutput;\n  };\n  \"fal-ai/triposr\": {\n    input: TriposrInput;\n    output: TriposrOutput;\n  };\n  \"fal-ai/fooocus/upscale-or-vary\": {\n    input: FooocusUpscaleOrVaryInput;\n    output: FooocusUpscaleOrVaryOutput;\n  };\n  \"fal-ai/fooocus/image-prompt\": {\n    input: FooocusImagePromptInput;\n    output: FooocusImagePromptOutput;\n  };\n  \"fal-ai/fooocus/inpaint\": {\n    input: FooocusInpaintInput;\n    output: FooocusInpaintOutput;\n  };\n  \"fal-ai/retoucher\": {\n    input: RetoucherInput;\n    output: RetoucherOutput;\n  };\n  \"fal-ai/any-llm\": {\n    input: AnyLlmInput;\n    output: AnyLlmOutput;\n  };\n  \"fal-ai/any-llm/vision\": {\n    input: AnyLlmVisionInput;\n    output: AnyLlmVisionOutput;\n  };\n  \"fal-ai/llavav15-13b\": {\n    input: Llavav1513bInput;\n    output: Llavav1513bOutput;\n  };\n  \"fal-ai/llava-next\": {\n    input: LlavaNextInput;\n    output: LlavaNextOutput;\n  };\n  \"fal-ai/imageutils/nsfw\": {\n    input: ImageutilsNsfwInput;\n    output: ImageutilsNsfwOutput;\n  };\n  \"fal-ai/fast-fooocus-sdxl\": {\n    input: FastFooocusSdxlInput;\n    output: FastFooocusSdxlOutput;\n  };\n  \"fal-ai/fast-fooocus-sdxl/image-to-image\": {\n    input: FastFooocusSdxlImageToImageInput;\n    output: FastFooocusSdxlImageToImageOutput;\n  };\n  \"fal-ai/face-to-sticker\": {\n    input: FaceToStickerInput;\n    output: FaceToStickerOutput;\n  };\n  \"fal-ai/moondream/batched\": {\n    input: MoondreamBatchedInput;\n    output: MoondreamBatchedOutput;\n  };\n  \"fal-ai/sadtalker\": {\n    input: SadtalkerInput;\n    output: SadtalkerOutput;\n  };\n  \"fal-ai/musetalk\": {\n    input: MusetalkInput;\n    output: MusetalkOutput;\n  };\n  \"fal-ai/dubbing\": {\n    input: DubbingInput;\n    output: DubbingOutput;\n  };\n  \"fal-ai/sadtalker/reference\": {\n    input: SadtalkerReferenceInput;\n    output: SadtalkerReferenceOutput;\n  };\n  \"fal-ai/layer-diffusion\": {\n    input: LayerDiffusionInput;\n    output: LayerDiffusionOutput;\n  };\n  \"fal-ai/stable-diffusion-v15\": {\n    input: StableDiffusionV15Input;\n    output: StableDiffusionV15Output;\n  };\n  \"fal-ai/lora/image-to-image\": {\n    input: LoraImageToImageInput;\n    output: LoraImageToImageOutput;\n  };\n  \"fal-ai/fast-sdxl/image-to-image\": {\n    input: FastSdxlImageToImageInput;\n    output: FastSdxlImageToImageOutput;\n  };\n  \"fal-ai/fast-sdxl/inpainting\": {\n    input: FastSdxlInpaintingInput;\n    output: FastSdxlInpaintingOutput;\n  };\n  \"fal-ai/lora/inpaint\": {\n    input: LoraInpaintInput;\n    output: LoraInpaintOutput;\n  };\n  \"fal-ai/pixart-sigma\": {\n    input: PixartSigmaInput;\n    output: PixartSigmaOutput;\n  };\n  \"fal-ai/dreamshaper\": {\n    input: DreamshaperInput;\n    output: DreamshaperOutput;\n  };\n  \"fal-ai/realistic-vision\": {\n    input: RealisticVisionInput;\n    output: RealisticVisionOutput;\n  };\n  \"fal-ai/lightning-models\": {\n    input: LightningModelsInput;\n    output: LightningModelsOutput;\n  };\n  \"fal-ai/omni-zero\": {\n    input: OmniZeroInput;\n    output: OmniZeroOutput;\n  };\n  \"fal-ai/leffa/virtual-tryon\": {\n    input: LeffaVirtualTryonInput;\n    output: LeffaVirtualTryonOutput;\n  };\n  \"fal-ai/leffa/pose-transfer\": {\n    input: LeffaPoseTransferInput;\n    output: LeffaPoseTransferOutput;\n  };\n  \"fal-ai/cat-vton\": {\n    input: CatVtonInput;\n    output: CatVtonOutput;\n  };\n  \"fal-ai/dwpose\": {\n    input: DwposeInput;\n    output: DwposeOutput;\n  };\n  \"fal-ai/stable-cascade/sote-diffusion\": {\n    input: StableCascadeSoteDiffusionInput;\n    output: StableCascadeSoteDiffusionOutput;\n  };\n  \"fal-ai/florence-2-large/caption\": {\n    input: Florence2LargeCaptionInput;\n    output: Florence2LargeCaptionOutput;\n  };\n  \"fal-ai/florence-2-large/detailed-caption\": {\n    input: Florence2LargeDetailedCaptionInput;\n    output: Florence2LargeDetailedCaptionOutput;\n  };\n  \"fal-ai/florence-2-large/more-detailed-caption\": {\n    input: Florence2LargeMoreDetailedCaptionInput;\n    output: Florence2LargeMoreDetailedCaptionOutput;\n  };\n  \"fal-ai/florence-2-large/object-detection\": {\n    input: Florence2LargeObjectDetectionInput;\n    output: Florence2LargeObjectDetectionOutput;\n  };\n  \"fal-ai/florence-2-large/dense-region-caption\": {\n    input: Florence2LargeDenseRegionCaptionInput;\n    output: Florence2LargeDenseRegionCaptionOutput;\n  };\n  \"fal-ai/florence-2-large/region-proposal\": {\n    input: Florence2LargeRegionProposalInput;\n    output: Florence2LargeRegionProposalOutput;\n  };\n  \"fal-ai/florence-2-large/caption-to-phrase-grounding\": {\n    input: Florence2LargeCaptionToPhraseGroundingInput;\n    output: Florence2LargeCaptionToPhraseGroundingOutput;\n  };\n  \"fal-ai/florence-2-large/referring-expression-segmentation\": {\n    input: Florence2LargeReferringExpressionSegmentationInput;\n    output: Florence2LargeReferringExpressionSegmentationOutput;\n  };\n  \"fal-ai/florence-2-large/region-to-segmentation\": {\n    input: Florence2LargeRegionToSegmentationInput;\n    output: Florence2LargeRegionToSegmentationOutput;\n  };\n  \"fal-ai/florence-2-large/open-vocabulary-detection\": {\n    input: Florence2LargeOpenVocabularyDetectionInput;\n    output: Florence2LargeOpenVocabularyDetectionOutput;\n  };\n  \"fal-ai/florence-2-large/region-to-category\": {\n    input: Florence2LargeRegionToCategoryInput;\n    output: Florence2LargeRegionToCategoryOutput;\n  };\n  \"fal-ai/florence-2-large/region-to-description\": {\n    input: Florence2LargeRegionToDescriptionInput;\n    output: Florence2LargeRegionToDescriptionOutput;\n  };\n  \"fal-ai/florence-2-large/ocr\": {\n    input: Florence2LargeOcrInput;\n    output: Florence2LargeOcrOutput;\n  };\n  \"fal-ai/florence-2-large/ocr-with-region\": {\n    input: Florence2LargeOcrWithRegionInput;\n    output: Florence2LargeOcrWithRegionOutput;\n  };\n  \"fal-ai/era-3d\": {\n    input: Era3dInput;\n    output: Era3dOutput;\n  };\n  \"fal-ai/live-portrait\": {\n    input: LivePortraitInput;\n    output: LivePortraitOutput;\n  };\n  \"fal-ai/live-portrait/image\": {\n    input: LivePortraitImageInput;\n    output: LivePortraitImageOutput;\n  };\n  \"fal-ai/muse-pose\": {\n    input: MusePoseInput;\n    output: MusePoseOutput;\n  };\n  \"fal-ai/kolors\": {\n    input: KolorsInput;\n    output: KolorsOutput;\n  };\n  \"fal-ai/kolors/image-to-image\": {\n    input: KolorsImageToImageInput;\n    output: KolorsImageToImageOutput;\n  };\n  \"fal-ai/sdxl-controlnet-union\": {\n    input: SdxlControlnetUnionInput;\n    output: SdxlControlnetUnionOutput;\n  };\n  \"fal-ai/sdxl-controlnet-union/image-to-image\": {\n    input: SdxlControlnetUnionImageToImageInput;\n    output: SdxlControlnetUnionImageToImageOutput;\n  };\n  \"fal-ai/sdxl-controlnet-union/inpainting\": {\n    input: SdxlControlnetUnionInpaintingInput;\n    output: SdxlControlnetUnionInpaintingOutput;\n  };\n  \"fal-ai/sam2/image\": {\n    input: Sam2ImageInput;\n    output: Sam2ImageOutput;\n  };\n  \"fal-ai/sam2/video\": {\n    input: Sam2VideoInput;\n    output: Sam2VideoOutput;\n  };\n  \"fal-ai/sam2/auto-segment\": {\n    input: Sam2AutoSegmentInput;\n    output: Sam2AutoSegmentOutput;\n  };\n  \"fal-ai/evf-sam\": {\n    input: EvfSamInput;\n    output: EvfSamOutput;\n  };\n  \"fal-ai/imageutils/sam\": {\n    input: ImageutilsSamInput;\n    output: ImageutilsSamOutput;\n  };\n  \"fal-ai/sa2va/8b/image\": {\n    input: Sa2va8bImageInput;\n    output: Sa2va8bImageOutput;\n  };\n  \"fal-ai/sa2va/8b/video\": {\n    input: Sa2va8bVideoInput;\n    output: Sa2va8bVideoOutput;\n  };\n  \"fal-ai/sa2va/4b/image\": {\n    input: Sa2va4bImageInput;\n    output: Sa2va4bImageOutput;\n  };\n  \"fal-ai/sa2va/4b/video\": {\n    input: Sa2va4bVideoInput;\n    output: Sa2va4bVideoOutput;\n  };\n  \"fal-ai/mini-cpm\": {\n    input: MiniCpmInput;\n    output: MiniCpmOutput;\n  };\n  \"fal-ai/mini-cpm/video\": {\n    input: MiniCpmVideoInput;\n    output: MiniCpmVideoOutput;\n  };\n  \"fal-ai/controlnext\": {\n    input: ControlnextInput;\n    output: ControlnextOutput;\n  };\n  \"fal-ai/workflowutils/canny\": {\n    input: WorkflowutilsCannyInput;\n    output: WorkflowutilsCannyOutput;\n  };\n  \"fal-ai/image-preprocessors/depth-anything/v2\": {\n    input: ImagePreprocessorsDepthAnythingV2Input;\n    output: ImagePreprocessorsDepthAnythingV2Output;\n  };\n  \"fal-ai/image-preprocessors/hed\": {\n    input: ImagePreprocessorsHedInput;\n    output: ImagePreprocessorsHedOutput;\n  };\n  \"fal-ai/image-preprocessors/lineart\": {\n    input: ImagePreprocessorsLineartInput;\n    output: ImagePreprocessorsLineartOutput;\n  };\n  \"fal-ai/image-preprocessors/midas\": {\n    input: ImagePreprocessorsMidasInput;\n    output: ImagePreprocessorsMidasOutput;\n  };\n  \"fal-ai/image-preprocessors/mlsd\": {\n    input: ImagePreprocessorsMlsdInput;\n    output: ImagePreprocessorsMlsdOutput;\n  };\n  \"fal-ai/image-preprocessors/pidi\": {\n    input: ImagePreprocessorsPidiInput;\n    output: ImagePreprocessorsPidiOutput;\n  };\n  \"fal-ai/image-preprocessors/sam\": {\n    input: ImagePreprocessorsSamInput;\n    output: ImagePreprocessorsSamOutput;\n  };\n  \"fal-ai/image-preprocessors/scribble\": {\n    input: ImagePreprocessorsScribbleInput;\n    output: ImagePreprocessorsScribbleOutput;\n  };\n  \"fal-ai/image-preprocessors/teed\": {\n    input: ImagePreprocessorsTeedInput;\n    output: ImagePreprocessorsTeedOutput;\n  };\n  \"fal-ai/image-preprocessors/zoe\": {\n    input: ImagePreprocessorsZoeInput;\n    output: ImagePreprocessorsZoeOutput;\n  };\n  \"fal-ai/f5-tts\": {\n    input: F5TtsInput;\n    output: F5TtsOutput;\n  };\n  \"fal-ai/kokoro/american-english\": {\n    input: KokoroAmericanEnglishInput;\n    output: KokoroAmericanEnglishOutput;\n  };\n  \"fal-ai/kokoro/british-english\": {\n    input: KokoroBritishEnglishInput;\n    output: KokoroBritishEnglishOutput;\n  };\n  \"fal-ai/kokoro/japanese\": {\n    input: KokoroJapaneseInput;\n    output: KokoroJapaneseOutput;\n  };\n  \"fal-ai/kokoro/mandarin-chinese\": {\n    input: KokoroMandarinChineseInput;\n    output: KokoroMandarinChineseOutput;\n  };\n  \"fal-ai/kokoro/spanish\": {\n    input: KokoroSpanishInput;\n    output: KokoroSpanishOutput;\n  };\n  \"fal-ai/kokoro/french\": {\n    input: KokoroFrenchInput;\n    output: KokoroFrenchOutput;\n  };\n  \"fal-ai/kokoro/hindi\": {\n    input: KokoroHindiInput;\n    output: KokoroHindiOutput;\n  };\n  \"fal-ai/kokoro/italian\": {\n    input: KokoroItalianInput;\n    output: KokoroItalianOutput;\n  };\n  \"fal-ai/kokoro/brazilian-portuguese\": {\n    input: KokoroBrazilianPortugueseInput;\n    output: KokoroBrazilianPortugueseOutput;\n  };\n  \"fashn/tryon\": {\n    input: TryonInput;\n    output: TryonOutput;\n  };\n  \"fal-ai/trellis\": {\n    input: TrellisInput;\n    output: TrellisOutput;\n  };\n  \"fal-ai/playai/tts/v3\": {\n    input: PlayaiTtsV3Input;\n    output: PlayaiTtsV3Output;\n  };\n  \"fal-ai/playai/tts/dialog\": {\n    input: PlayaiTtsDialogInput;\n    output: PlayaiTtsDialogOutput;\n  };\n  \"fal-ai/latentsync\": {\n    input: LatentsyncInput;\n    output: LatentsyncOutput;\n  };\n  \"fal-ai/moondream-next\": {\n    input: MoondreamNextInput;\n    output: MoondreamNextOutput;\n  };\n  \"fal-ai/moondream-next/detection\": {\n    input: MoondreamNextDetectionInput;\n    output: MoondreamNextDetectionOutput;\n  };\n  \"fal-ai/moondream-next/batch\": {\n    input: MoondreamNextBatchInput;\n    output: MoondreamNextBatchOutput;\n  };\n  \"fal-ai/recraft-clarity-upscale\": {\n    input: RecraftClarityUpscaleInput;\n    output: RecraftClarityUpscaleOutput;\n  };\n  \"fal-ai/recraft-creative-upscale\": {\n    input: RecraftCreativeUpscaleInput;\n    output: RecraftCreativeUpscaleOutput;\n  };\n  \"fal-ai/elevenlabs/audio-isolation\": {\n    input: ElevenlabsAudioIsolationInput;\n    output: ElevenlabsAudioIsolationOutput;\n  };\n  \"fal-ai/elevenlabs/tts/multilingual-v2\": {\n    input: ElevenlabsTtsMultilingualV2Input;\n    output: ElevenlabsTtsMultilingualV2Output;\n  };\n  \"fal-ai/elevenlabs/sound-effects\": {\n    input: ElevenlabsSoundEffectsInput;\n    output: ElevenlabsSoundEffectsOutput;\n  };\n  \"fal-ai/elevenlabs/tts/turbo-v2.5\": {\n    input: ElevenlabsTtsTurboV25Input;\n    output: ElevenlabsTtsTurboV25Output;\n  };\n  \"fal-ai/elevenlabs/speech-to-text\": {\n    input: ElevenlabsSpeechToTextInput;\n    output: ElevenlabsSpeechToTextOutput;\n  };\n  \"fal-ai/janus\": {\n    input: JanusInput;\n    output: JanusOutput;\n  };\n  \"fal-ai/imagen3\": {\n    input: Imagen3Input;\n    output: Imagen3Output;\n  };\n  \"fal-ai/imagen3/fast\": {\n    input: Imagen3FastInput;\n    output: Imagen3FastOutput;\n  };\n  \"fal-ai/gemini-flash-edit\": {\n    input: GeminiFlashEditInput;\n    output: GeminiFlashEditOutput;\n  };\n  \"fal-ai/post-processing\": {\n    input: PostProcessingInput;\n    output: PostProcessingOutput;\n  };\n  \"fal-ai/video-prompt-generator\": {\n    input: VideoPromptGeneratorInput;\n    output: VideoPromptGeneratorOutput;\n  };\n  \"fal-ai/eye-correct\": {\n    input: EyeCorrectInput;\n    output: EyeCorrectOutput;\n  };\n  \"fal-ai/topaz/upscale/video\": {\n    input: TopazUpscaleVideoInput;\n    output: TopazUpscaleVideoOutput;\n  };\n  \"easel-ai/advanced-face-swap\": {\n    input: AdvancedFaceSwapInput;\n    output: AdvancedFaceSwapOutput;\n  };\n  \"fal-ai/csm-1b\": {\n    input: Csm1bInput;\n    output: Csm1bOutput;\n  };\n  \"fal-ai/pika/v1.5/pikaffects\": {\n    input: PikaV15PikaffectsInput;\n    output: PikaV15PikaffectsOutput;\n  };\n  \"fal-ai/pika/v2/turbo/image-to-video\": {\n    input: PikaV2TurboImageToVideoInput;\n    output: PikaV2TurboImageToVideoOutput;\n  };\n  \"fal-ai/pika/v2/turbo/text-to-video\": {\n    input: PikaV2TurboTextToVideoInput;\n    output: PikaV2TurboTextToVideoOutput;\n  };\n  \"fal-ai/pika/v2.1/image-to-video\": {\n    input: PikaV21ImageToVideoInput;\n    output: PikaV21ImageToVideoOutput;\n  };\n  \"fal-ai/pika/v2.1/text-to-video\": {\n    input: PikaV21TextToVideoInput;\n    output: PikaV21TextToVideoOutput;\n  };\n  \"fal-ai/pika/v2.2/image-to-video\": {\n    input: PikaV22ImageToVideoInput;\n    output: PikaV22ImageToVideoOutput;\n  };\n  \"fal-ai/pika/v2.2/text-to-video\": {\n    input: PikaV22TextToVideoInput;\n    output: PikaV22TextToVideoOutput;\n  };\n  \"fal-ai/pika/v2.2/pikascenes\": {\n    input: PikaV22PikascenesInput;\n    output: PikaV22PikascenesOutput;\n  };\n  \"fal-ai/invisible-watermark\": {\n    input: InvisibleWatermarkInput;\n    output: InvisibleWatermarkOutput;\n  };\n};\n"]}